Global information systems provide the opportunity for decision makers to provide timely space information about the Earth system. These information will come from a variety of sources, including on-site monitoring, remote perception images, and environmental models. Three of them have the greatest potential to provide regional and global environmental systems behavioral information, which may be crucial to the development of multi-government policies and decisions, which are crucial to the quality of life. However, environmental models have limited prootocol quality control and standardization. They tend to have a weak or incorrect sequence, so their production is often difficult to explain in a very limited range of applications, they are designed.
Many business databases systems use some form of statistics, usually histograms, summarizing the content of the relationship, and allow effective estimation of the required quantity. While there is a lot of work in identifying good histograms of the data distribution of a query result estimation requires an important application background, we rarely notice the importance of estimating the data distribution results, which is in query optimization. In this article, we prove that the best histograms of the estimated results of the size of a combined operator is the best estimation of its data distribution as well. We also studied the effectiveness of these best histograms in the context of an important application, we need to estimate the data distribution of a query result: We will make a table of the cost of the data distribution and optimize the data in query.
This chapter describes an effective way to maintain the materialized view with non-distributed integrated function, even in the presence of superintegrated. Enhanced view maintenance is a very important aspect of the modern database management system. It allows for rapid execution of complex queries, without sacrificing the data's freshness. However, maintenance view defined non-distributed integrated function has not been fully explored. Enhanced updates have been thoroughly studied, only for a subgroup of integrated function. Materialized view, or automatic integrated table (ASTs), is increasingly used to analyze a large amount of data collected in the relationship database. using AST can significantly reduce the execution time, often defined by non-distributed integrated function of view maintenance has been updated in depth research, only as a functional integrated component of the image
While the need for objective-oriented database system to solve a lot of problems similar to the problem solved by the relationship system, there are also significant problems that are unique. In particular, the object query can contain a route expression, through a series of related collections. The track expression provided by the collection channel order may not be the most effective processing query. This generates a key problem, the object query optimizer chooses an algorithm to process query based on direct navigation or a combination of different combinations. This paper studies different algorithms to deal with the route expression forecasts, including the depth of the first navigation, forward and reversal combination using a cost model, it will be compared in different circumstances, according to the memory size, from the predictive selection, from the collection, from the collection, from the collection, from
Space fusion operations are used by the well-known variables of space data structure, such as R tree, R tree, R + tree, and PMR four tree focuses on space fusion with space output, as the result of space fusion is often as input to subsequent space operations (i.e., a concrete space fusion will be common space distribution table).
Finding 3D capacity objects similar to specific 3D search objects is an important problem that occurs in the number of database applications - for example, in medicine and CAD. In this paper, we introduce a new geological-based solution to find similar 3D capacity objects problems. This problem is made by a practical application in the medical field, where capacity similarity is used as a basis for surgical decision. Our solution for effective similarity search in a large database of 3D capacity objects is based on a new geological index structure. Our new method's basic idea is to use the 3D sequence of objects closer to the concept to accelerate the search process.
Hopefully its publications are considered to be reviewed in computing linguistics, the authors and publishers should send a copy to the book review editor, Graeme Hirst, the Faculty of Computing Sciences, the University of Toronto, Toronto, Canada M5S 3G4. all relevant books will be included but not all reviewed. technical reports (except the paper) will not be included or reviewed. the authors should note that some publishers will not send a book review (even if they are instructed to do so); wish to ask whether their books are received by the author can contact the book review editor.
The explosion of complex multimedia content makes the database system able to effectively support these data is crucial. This article points out that the "black box" ADT used in the existing object relationship system blocks its performance, thereby limiting its use in emerging applications. Instead, the next generation of object relationship database system should be based on enhanced abstract data type (E-ADT) technology. A (E-ADT) can show the methods of its system, thereby allowing advanced query optimization.
The DART '96 with Information and Knowledge Management Conference (CIKM) took place on November 15 in Baltimore. Its objective is to provide a forum for researchers and practitioners involved in integrating concepts and technologies, from active and real-time databases to discuss the state of art and a chart to an action process. For this purpose, nine speakers from the academic community, industry and research laboratories are invited to provide theoretical and practical basis for an active real-time database perspective.
This dynamic environment requires fast, cost-efficient, time to the market for new or modified business processes, services and products. To support this important business demand, the next generation of DBMS must: 1. take advantage of the enormous investment in existing relationship technologies, both in data and skills groups; 2. use the flexibility, productivity and performance benefits of the OO model; and 3. integrate the powerful DBMS service quality production system. This article aims to provide a brief overview of the business SQL object database system.
In this work, we design and evaluate control strategies that combine two potentially powerful bubble management techniques on the basis of objects: (1) bubble divide with the divide specific replacement standards and (2) double bubble composed from the page copy objects to the object bubble.
Stream is the continuous data transmission produced by sources such as sensors, satellites and warehouses, monitoring applications tracking data from multiple streams, filtering them as signs of abnormal activity, and processing them for filtering.
Active databases and real-time databases are an important field of research in recent past. have been recognized that many benefits can be obtained by integrating active and real-time databases technology. However, there is not too much work in the field of transaction processing in active real-time databases. This paper deals with an important aspect of transaction processing in active real-time databases, that is, the problem of priorities of transactions. In these systems, time-limited transactions trigger other transactions during their execution period. We introduce three policy priorities of parents, instant and transfer transactions performed in a multi-processor system, and then through the simulation policy. using different quantities of transaction information priorities of transactions. In these systems, time-limited transactions trigger transactions during their execu
Many applications require space data management. Integrated large space database is an important problem, trying to find an intense population area in functional space for data mining, knowledge discovery, or effective information collection. A good integrated method should be effective and detects the integrator’s voluntary shape. It must not be sensitive to the output (luck) and input data in order. We recommend WaveCluster, a new integrated method based on wavelet conversion, which meets all of the above requirements. Using the multi-resolution characteristics of wavelet conversion, we can effectively identify the voluntary shape integrator in different accuracy. We also prove that WaveCluster is very effective term in time complexity (luck) and input data in order We recommend WaveCluster, a new method based on wavelet conversion.
In this article, we introduce a mechanism of approximately translation of Brian Question restrictions in various sources of information. achieving the best translation is challenging because the source supports different restrictions, developing queries, and often these restrictions cannot be translated accurately. for example, a query [pointing > 8] may be "perfect" translated to [pointing > 0.8] on one site, but can only be approximately as [pointing = A] on another site.
Classification is an important data mining problem. Although classification is a carefully studied issue, the current classification algorithm requires that the whole set of data be kept in memory. This restricts the suitability of their mining on a large database. We introduce a new decision tree based on a certain rate of classification, called SPRINT, eliminating all memory limits, and is fast and extensible. The algorithm is also designed to be easily parallel, allowing many processors to work together to build a consistent model.
Text files often contain valuable structural data, hidden in conventional English sentences. These data are best used if available as a relationship table, we can use to answer accurate queries or perform data mining tasks. We explore a technology to extract these tables from the file collection, only requires some training examples from the user. These examples are used to produce the extract pattern, which in turn leads to the extract of new modules from the file collection. We build this idea and introduce our Snowball system. Snowball introduces a new strategy to produce modules and extract modules from the tablet text file.
Decision support applications involve complex queries in a very large database. Because the response time should be small, queries optimization is critical. Users usually see the data as a multi-dimensional database. Each database of cells is a view that is accumulated by an interest, such as total sales. Many of these cells value depends on the value of other cells in the database.. A common and powerful queries optimization technology is to take these cells some or all of the time, rather than calculate them from the original data every time. Business systems are mainly different from their methods to the database. In this article, we study the cells (view) view when it is too expensive to all points of view. A framework expression case depends on the value of other cells in the database and a strong queries.
Abstract - Commercial applications usually rely on pre-prepared parameters with database interactions. Unfortunately, the execution of a program with different parameters group of parameters time can be voluntarily low optimization. Parameters query optimization (PQO) tries to solve this problem by thoroughly determining the best plan in each point of the parameters space in the preparation time. However, PQO may not be cost-efficient if query is frequently performed or if it is performed value only within a subgroup of parameters space. In this article, we advise not to gradually explore the parameters space and to build a parameters system during the execution of the same query several times.
Data storage systems will integrate information from operating data sources to a central storage library to analyze and mining integrated information. In the integration process, source data usually experiences a series of transformations, these transformations may be from simple algorithm operations or integrated to complex "data clean" programs. In the storage environment, data linear problems are from storage data components tracking to the original source components, from where they are derived. We formally define linear tracking problems in the total data storage transformation existence, we introduce linear tracking algorithms in this environment. Our tracking programs use known structures or properties transformations, when existing, but also lack such information. Our data linear problems can be used as a linear tracking tool from the basis where they are derived.
Using data storage and online analytics processing (OLAP) for decision-making support applications new security issues arise.This paper aims to introduce an OLAP security design method that highlights the need for further research.We introduce possible access control requirements according to its complexity classification.
We introduce a random walking as a precise and accurate method to approach the integrated query about the web pages. Our method uses a new random walking to produce a almost uniformly distributed web sample. Walking through a dynamic-built non-oriented chart. Ask us using this method, including the search engine coverage age, belonging to the t.com and other domains of the page ratio, as well as the average size of the web page.
In StruQL, one way to write this is: Enter DataGraph in which root (x); x! y; y! l! z; l in f “paper”, “technical report”, “titles”, “abstract”, “author” g create the author(); page(y); page(z) link page(y)! l! page(z) there x! y1; y1! “author”! z1 link author()! “author”! page(z1) out SiteGraph 2 to collect information from multiple sources, allow us to enter multiple tables.
A number of formal models of database access control have been proposed. However, a small amount of attention is obtained on temporary issues such as limited valid license or by temporary limitation reference reasons. We introduce a access control model in which the license includes a regular temporary validity period. the license is automatically granted within the time intervals specified and cancelled at the end of these intervals. the period and the order limitation reference provisional rules are provided to produce new license based on the presence or absence of other licenses for a specific time period.
Data mining is expensive on the computer. Because the benefits of data mining results are unpredictable, the organization may not be willing to buy new hardware for this purpose. We will introduce a system that allows data mining applications to operate parallelly on the network of the workstations in a way of error tolerance. We will describe the parallelness of our combination patterns to find algorithms and class trees algorithms. We will show the effectiveness of our system through two actual applications: to find active motivations in the protein sequence, and to predict the movement of the exchange rate.
One very prospective idea of quick search in traditional and multimedia databases is to put an object map to the point in the k-d space, using the k-function extract feature provided by the domain expert [Jag91]. therefore, we can then use very sophisticated space access methods (SAMs) to answer multiple types of queries, including "queries as an example" type (it translates as range queries); "all objects" queries (it translates as space connection (BKSS94)); the closest neighbor or best matching queries, etc.
Database systems now play an increasingly important role in a knowledge-based society, computers have penetrated all the fields of activity, the internet tends to develop around the world. In the current computer science context, the development of applications and databases is the work of experts. using databases, from various applications to databases, as well as some related concepts, has become accessible to all categories of IT users. This paper aims to summarize the curriculum fields on basic databases systems issues, these issues are necessary to train economical computers higher education professionals. Databases systems integrate and interfere with multiple computer technologies, so it is more difficult to understand and use. Therefore, students should already know a group of compulsory concepts and their implementation: computer systems, technical systems, data structure, data structure, data structure, data structure, data structure, data structure
Web caching proxy servers are a necessity for improving web performance and scalability, and recent research focuses on proxy caching work for web sites supported by the database. In this article, we explore a new proxy caching framework, using HTML tables for queries. We identify two common types of web sites supported by the real world database based on tables queries, i.e. based on keyword queries and functional embedded queries. Using these queries typical examples, we study two representative queries within our framework: (i) traditional passive queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries queries
Structural data stored in les can be from the standard database technology. in particular, we here show how these data can be quoted and updated using the statement database language. we introduce the concept of the structural program that includes a letter of the database program. on the basis of a structural program, le can be considered as the database structure, quoted and updated as such. for queries, weshow almost standard database optimization technology can be used to answer queries, without building the entire database. for updates, we study in depth to le the updated specifications editing on the database view this le. the problem is impossible, we introduce the results are negative results positive results are the technology that allows to be updated in some cases, in some cases to be analyzed.
In the DBGlobe project, we view the data and services transfer group as a superdatabase, our goal is to develop a data management system to simulate, index and query data hosted by these large-scale, independent and possibly mobile competitors, we take service-oriented methods in which data is embedded in the service, direct query data is also supported by XML-based query language. in this article, we introduce our research results to the following topics: (a) infrastructure support, including mobile competitors and creating a community-dependent background, (b) data management of services and services, including location-dependent data, (c) efficiently query data in the service.
Due to the Internet, we believe that there will be a long-term alternative supplier for any application. data suppliers will bring more and more data and more different types of data to the network. Similarly, features suppliers will develop new methods to process and process data; for example, features suppliers may develop new algorithms to compress data or make small images and try to sell them on the Internet.
The behavior of scientific and engineering research is becoming critical to the effective management of scientific and engineering data and technical information. The rapid progress of scientific tools, computers and communication technologies enables scientists to collect, generate, process and share unprecedented amounts of data. For example, the mission of Earth Observation Systems Data and Information Systems (EOSDIS) is to manage the data from NASA's Earth Scientific Research satellite and field measurement programmes, as well as other necessary data to explain these measurements support global change research. In addition to being able to process a traffic of 1 trapart of data daily to 2000, EOSDIS also needs to provide transparent access to multiple data stored in the files of several U.S. government agencies and national organizations.
Recently there is an increasing interest in collective operations that support the multi-dimensional index structure. Collective load refers to the process of creating a initial index structure, that is, a fairly large set of data. In this article, we introduce a general collective load algorithm that applies to a wide range of index structure categories. Our method is completely different from the previous reasons. First, according to scheduled global orders completely avoid classification of multi-dimensional data. On the contrary, our method is based on the classification and combination of the standard habits of the page, which has been fully implemented in the corresponding index structure. Second, on the contrary, the input record one-to-one, our method is based on the multi-dimensional input record idea.
In order to accelerate the classification algorithms, the data summary method has been proposed, first to summarize the data settings for the calculation of the appropriate representative objects. Then, the classification algorithm applies only to these representatives, and a classification structure of the whole data set is derived, based on the result representatives. However, most of the previous methods are limited application fields. They are generally based on sufficient statistics, such as linear summary of a set of points, assuming that the data is from a vector space. On the other hand, in many important applications, the data is from a non-vector space, and only the distance between objects can be used to build a effective data summary. In this new file, we develop a data-based method rather than the application distance information can only be based on sufficient statistics
From information sources to detect and extract changes are a component of data storage. for unshared sources, in practice, it is often necessary to determine changes by regularly comparing the source data screens. Although this problem is closely related to traditional merger and external merger, there are significant differences that lead to simple new algorithms. in particular, we introduce the algorithm to perform (may lose) record compression. We also introduce a window algorithm if the screens are not "very different."
The paper describes a model that combines the execution of the starter with the assessment of the declaration restrictions in the SQL database system. The model achieves complete compatibility with the 1992 International SQL Standard (SQL92) it retains a sequence of declaration restrictions assessment while enabling the execution of a powerful program starter. It is implemented in DB2 for common servers and has recently been accepted as a model of the emerging SQL Standard (SQW).
Business Relationship Database Management System (RDBMS) usually provides query capabilities for text properties, which include the most advanced information access (IR) relevance ranking strategy, but this query feature requires query to determine the exact column or column that the keyword list should match.
The real world entity is the internal space and time reference, the database application is increasingly using the database recording the past, now, and predicting the location of the graphic entity, for example, the residence of ofcuEERRx7 is obtained by geological time coded address. the indicator is effective suient the quiet space time range ofsuE entity is required. however, the past indicator studies have progressed in a great extent separate time space and time flow assessment. add time size to space indicator, because time is the space size, and there is no suther7 and does not use the special characteristics of time. on the other hand, the time indicator is generally not applicable to space size. the indicator is based on the space first part and the first part of the space efficiency; but the previous indicator has progressed significantly in the
This paper introduces a new temporary isolation strategy. temporary isolation combines the equivalent value of the temporary range of uranium. the temporary range is usually isolated and stored as the isolation is a costly operation. but with the current temporary range of uranium, the time in different particles, or incomplete time cannot be determined until query assessment. this paper introduces a strategy, partially isolating the temporary range, by determining the potential coverage of the area. the coverage area can be used to evaluate the temporary predictions and builders in the isolation range. our strategy uses the standard of relative database technology.
The paper introduces the relative effectiveness of the traditional lists and the USPS address lists as a sample framework for the domestic country probability sample.NORC and ISR jointly compared the two national regional probability sample framework for domestic surveys.We conducted this comparison in the ongoing survey operation, combining the current HRS wave with the first wave of NSHAP.Since 2000, the survey sample explored the potential of the USPS address list as a sample framework for the total population probability sample.We the relative coverage characteristics of the two frameworks, as well as the US PS framework’s coverage and performance predictor.
Because the fact is that the relationship database management system is not so good, they should do what to help us get answers from our data unless we ignore a good relationship doctrine and use a new method to the data model when we build a decision support database. The relatively correct data model, everyone in school is taught, is only useful to high performance in online trading processing. The result model will divide the data into many tables of relatively equal width and depth to speed of trading. But using this model with the real world decision support system almost guarantees failure. Taking a medium-size sustainable goods manufacturer, one of the horizontal smoking boxes. A year later, the IS staff design database, with hundreds of negative on the online trading processing results will divide the data into many tables of relatively equal width and depth to the speed of
With the latest advances in storage and networking technology, it is now possible to provide film in demand (MOD) service, eliminating the inability within today's broadcast cable system. a MOD server is a computer system that stores film in compressed digital form and provides different parts of compressed film data that can be accessed and transferred simultaneously. in this article, we introduce a low-cost storage architecture, a MOD server primarily depends on the disk.
The database management system (DBMS) stores and manages a lot of shared data, while the application performs data processing tasks, for example, running the company’s business. often, these programs are written in different programming languages (PLs) different types of systems. therefore, the DBMS should be “multi-language” to serve the application requests. This is usually achieved by providing a DBMS and its database language (DBL), such as SQL2, with its own type of system. to access the database (DB), it takes a DBL/PL group called the database API (DB-API or API short).
Classification is the process of making a set of objects similar to objects. While the definition of similarity differs from one classification model to another, in most of these models, the concept of similarity is based on a distance, for example, the Oakland distance or synchronic distance. In other words, similar objects need a close value for at least one set of sizes. In this article, we can explore a more common type of similarity. In pCluster model we propose that two objects are similar if they show a consistent model in one classification model. For example, in the DNA micron analysis, the expression level of two genes can rise and fall synchronically in response to a setting of environmental stimulation despite their expression level of intensity, these models cannot express a more common type of similarity.
Introduction to MOCHA1 is a new database intermediate software system designed to connect data sources, distributed in a wide regional network. MOCHA is built in a concept, intermediate software, a large-scale distributed environment should be self-sensitive. This means that new applications specific data types and query operators need query processing being deployed to a remote site automatically by the intermediate software system itself. In MOCHA, it is by sending Java operators to the customer assessment operating these types or operators to their remote site where they can be used to manipulate the data interests. All these Java classes are first stored in one or more code storage, from MOCHA later recovered and deployed in a "new conversion" on the basis of this main data processing idea can be achieved.
Although the query is published in the keyword, the return results contain the correct information that the user is query, which may not be clearly described in the input query. the required information is often not included in the web page, its URL is returned by the search engine.
The regular readers of this column will be familiar with the database language SQL - in fact, the majority of readers are familiar with it. We also discussed the fact that the SQL standard was published in several parts and even discussed one of them in some details. Another standard, based on the SQL and its structured user definition type, was developed and published by the International Standardization Organization (ISO). This standard, such as SQL, is divided into several parts (in fact, more independent than the part of SQL).
Adapt/X Harness is an integrated information system, platform and toolkit that provides integrated and smooth access to unusual and distributed information in the network environment (LAN, WAN, Intranets, and the global Internet). it allows cost-efficient access, keywords and properties queries, navigation and linking and operating these information resources using popular Internet browsers, without the need for any translation, transfer, or re-save of original information resources. information resources, such as text and multimedia files, various types of files, software applications, relationship databases, e-mails and references can be "registered" with Adapt/X Harness and organized to the collection.
XML data is likely to be widely used as data exchange formats, but users also need to store and query XML data.
Data storage and online analytical processing (OLAP) are key elements of decision support, which is increasingly becoming the focus of the database industry. Many business products and services are now available, all major database management systems suppliers are now offered in these fields. decision support put some quite different database technology requirements, with traditional online transaction processing applications. This paper provides a overview of data storage and OLAP technology, focusing on their new requirements. We describe the background tools for extracting, cleaning and charging data to the database; Multi-Data model typical OLAP; client tools for query and data analysis; server extensions for effective query processing; tools for the database management.
Multimedia information systems have appeared in many applications, from library information systems to entertainment technology. However, the majority of these systems are unable to support the continuous display of multimedia objects, and often suffer from the so-called interference and delay. This is due to the low I/O bandwidth of current disk technology, the high bandwidth requirements of multimedia objects, and the large size of these objects, almost always requires them to be discs residents.
Sensor network technology has evolved very quickly over the past decade. In many cases, high-quality multimedia stream may need to provide detailed information about hotpoints in a large-scale sensor network. With a limited capacity of sensor nodes and sensor network, it is very difficult to support multimedia stream in the current sensor network structure. In this article, we recommend by deploying a limited number of mobile "sensors". Sensor nodes have a higher capacity than the sensor nodes in hardware function and network capacity. Mobile sensors can point to hotpoints in the sensor network to provide detailed information in the target area. Through mobile sensor nodes, high-quality multimedia stream can support the sensor network's size, no much more cost.
Some issues related to zero-value processing in SQL are being discussed, suggesting a definition of the exact answer to SQL queries, which concerns the meaning of zero-value in SQL, an algorithm is submitted to modify SQL queries, so the answer will not change a database without zero-value, but the exact answer is obtained by a standard SQL series database.
Materialization is a useful abstract pattern that can be identified in many applications settings.Intuitively, materialization is the relationship between categories (e.g., car models) and more specific objects (e.g., individual cars).This paper provides the definition of abstract in common algorithms and algorithms, as well as the match of a class / metal class.New and powerful hereditary mechanisms are related to materialization.The materialization examples, properties and expansion are also presented.
The dynamic writing technology allows the site to try to adjust the content to each user according to different working time parameters (e.g., based on the table parameters).Web developers have a variety of options in the dynamic writing languages, such as Java Server Pages (JSP) and Sun's servers;Microsoft's Active Server Pages (ASP).
In large projects and corporate data models, the cost of creating a data model is equal to a large part of the total cost. On the other hand, there is a general pressure to reduce the cost of the data model of the application system to use the cost of the data processing in one corner.
In the past few years, some works have proposed a Troll model for access to XML data, which only takes into account the right to read and access to non-recurring DTD. Some works have studied the right to access to updates. In this article, we presented a general model to determine the existence of Troll data access to updates on the W3CXQuery Update Facility. Our method to perform such updates is based on the concept of query re-writing. A major issue is that query re-writing DTD remains an open issue. We show that this limit can only be avoided with the right to express XML and propose a linear algorithm to re-writing each operation defined by Lotto (D3CQuery Update Facility is our method to re-writing query).
The Information Management Group of the University of Dublin has research subjects such as digital multimedia, interactive systems and database engineering.In the field of digital multimedia, in collaboration with our Faculty of Electronic Engineering, a digital video processing center, the University designated a research center, the aim of which is to research, develop and evaluate the operation of digital video information based on content.To this goal, the expertise of the center covers from image analysis and functional extraction to video search engine technology and interface to video browsing.The Interactive Systems Group is interested in the research of the Federal database and interactiveness, object model and database engineering.
RasDaMan is a universal - that is, domain-independent - DBMS series, used for volunteer sizes and structures of multi-dimensional series. a declared, SQL-based series query language provides flexible access and operation. server-based effective query assessment is enabled by intelligent optimizer and flexible series-based folding and compressed flexible storage architecture.
The popularity of mobile and transparent computing devices brings energy limitations, as well as performance considerations.The design of energy consciousness is important at all levels of the system architecture, and the software plays a key role in the battery energy storage on these devices.As the increasing popularity of space database applications, and their expected deployation on mobile devices (such as road athletes and GPS-based applications), the energy impact of space data storage and access data sets is crucial to review.
Microsoft SQL Server over the years of successful transaction processing and decision-making super-work load, no merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger
Therefore, it is important that a database system used for implementing such a database can effectively process operations on such a program. In this article, we will describe one of the operations, combined operations --- the most important operations --- implemented in IBM Informix Extended Parallel Server (XPS).
Smart cards are the most secure mobile computing devices today. They have been successfully used for applications involving funding, as well as ownership and personal data (such as banking, healthcare, insurance, etc.) As smart cards become stronger (with 32-bit CPUs and more than 1MB of stable memory in the next version) and become multiple applications, the demand for database management arises. However, smart cards have severe hardware restrictions (very slow writing, very little RAM, limited stable memory, no autonomy, etc.) making traditional database technology unrelated.
As the importance of XML, LDAP catalogues and text-based information sources on the Internet increases, there is increasing need to evaluate (sub)string matching queries. in many cases, the match must be on multiple properties/size, with the interrelationship between multiple sizes. Effective queries optimization in this context requires good selective estimates. In this article, we use pruned count-suffix trees (PSTs) as a basic data structure to evaluate selectiveness.
The main characteristic of the language is its descriptivity, its capacity maps between the graphs written in relative, object-oriented, ER, or EXPRESS data model, its facilities to determine the user’s definition of updating operations, will spread to the data source.
There are also problems with the use of the values obtained from the study of the population to the economic model, as well as the difficulties in avoiding divisions in predicting the health status values. Review recommends a set of health status values as part of the "reference case" used in the economic model. Due to the lack of quality estimates in the field, further recommendations were made on the design of the future research to collect the economic model related to the HSV.
First, what we mean is RM? there are several lines of RM, each line has its own evolution. the original line was developed by E.F.Codd in the 1970s, he later called it RM/VI. In 1979, he proposed a new model, RM/T, which means a huge change from the original RM method.
In history, there have been a few transitions between the databases and the network research community; they work at very different levels and focus on very different issues. although this strict separation concern has lasted for many years, in this speech, I will say that the gap has recently decreased to two fields now have a lot to say about each other.
Our design allows self-start distributed queries, jumping directly to the least common ancestors of queries results, significantly reducing the queries response time. We introduce a new queries assessment collection technology (using XSLT) detection (1) which data in the local database section is part of the queries results, and (2) how to collect the missing parts. We define the division and storage unchanged, ensuring that even some of the queries data are used and ensure that the right answers are returned, despite our dynamic queries-oriented queries.
Question processing is one of the most critical issues in objective-oriented DBMS. Extensible options to optimize, search strategy requires a cost model to choose the most effective implementation plan. In this paper, we propose and partially verify the objective-oriented DBMS general cost model. Storage model and its access methods support collection, flow and flow of assemblies, links and route indicators. Question may involve complex predictions and qualified route expressions.
The database is used to collect and analyze data from remote sources. the data collected is often derived from transaction information, which can become very big. This paper introduces a framework to gradually remove storage data (there is no need to fully re-calculate two options. one is storage data, in this case the result is if the data has never existed. the second is storage data, in this case defined the data view is not necessarily affected. the framework, a user or administrator can specify what data storage or storage, what support data storage to promote increased view maintenance, what kind of updates are expected from an external source, and how the system should compensate when the data storage or other parameters are now stored and stored and stored and stored.
Three thematic areas related to the database community are identified, the first is the user-centered information analysis environment, based on interrelationship and manipulation of multimedia and complex information resources, visualized complex and abstract information space, based on value search, access and processing multimedia and complex files. The second is extensive, secure and interactive information storage, supporting a wide range of information resources and services. Problems to be solved include: information resources and services registration and security, access control and rights management, automatic category and federal, as well as distributed service quality assurance facilities.
In this article, we introduce and evaluate alternative technologies to use the location independent detector in the distributed database system. Location independent detector is important to take full advantage of the migration and copying, as they allow access to the object that creates the object. We will show how to use the distributed index structure, we will introduce a simple but effective copying strategy for the index nodes, we will introduce alternative strategies that can be through the index to the direction of the detector (i.e., to find a copy of the object to its detector).
We capture these queries in our defined preference queries, using the weight function over the relationship properties to obtain the score of each score. Database systems cannot effectively produce the top results of the preference queries because they need to evaluate the weight function on all the ratio.
In this column, I will introduce myself, commentary can be published in this section of the type of software reviews, and encourage other tasks in professional consideration to review the statistical software package.
We have developed a XML storage management system called Rainbow designed to use relation database technology to manage XML based on flexible map strategies.As shown in Figure 1, Rainbow system consists of three subsystems, for example, a load administrator, a map administrator and a XML query engine built on the top of a relationship database.
One major challenge that designers and executives of database programming languages (DBPLs) still face is query optimization. in the paper, we first provide our file DBPL synthesis and briefly discuss its grammar. then, we define a small but powerful operator of the data type algorithm, providing some critical equations for these operators and listing the conversion principles to optimize the expression.
This article provides a method of static analysis of the active database rules to determine whether these rules (1) guarantee the termination, (2) guarantee the production of a unique final database state, (3) guarantee the production of a unique observable action flow.
ORES TDBMS will support effective and user-friendly representation and manipulation of temporary knowledge, and will be an extended development of the INGRES relationship database management system. ORES project will lead to the general purpose of TDBMS, its development is based on practical but theoretically correct approach. More specifically, the overall goal of ORES project is: i) to develop temporary representation and reasonable formal basis, ii) to develop temporary query language that will be consistent with SQL2, iii) to develop models, technologies and tools for user-friendly definition, manipulation and verification of temporary database applications, iv) to evaluate the ORES environment through case studies.
We that the "Triple Magic" condition (Double Magic Frequency Intensity Optical Hole along with Magic Magical Field) of the microwave optical capture of the alkali metal atoms (DLS) is adopted by the degradation of the dual-light process to compensate for the DLS related to the single-light process.
Multimedia applications require specific support for the database management system due to the characteristics of multimedia data and their interactive use. This includes integrated support for high-capacity and time-dependent (continuous) data types, such as audio and video. A key issue is to provide the processing of the data flow, including the bubble management required for the multimedia presentation. The bubble management strategy must take into account specific requirements, such as providing the continuity of the presentation, immediately continuing to demonstrate the frequent user interaction, through the appropriate bubble resource consumption.
Hardware development provides incredible reliability and essentially unlimited storage, network, memory and processing power.The cost decreases significantly.Computer is becoming incredible.DBMS software’s functionality and scalability have advanced to most business systems that can meet almost all OLTP and DSS requirements.
In this article, we introduce the second improvement: a single operator, allowing the analysts to observe a drop or increase the summary of the causes, which eliminates the need to manually mined these causes, we developed a form of information theory of expression of the causes, it is delicate, easy to explain, we designed a dynamic programming algorithm that only takes one channel of data to significantly improve our initial heat algorithm, which requires multiple channels.
This paper studies the work file disk management as the same combination of ina multiprocessor database systems. in particular, we review the work file disk allocation and data division impact on the same reaction time. the same combination in multiprocessor systems can create serious I/O interference, in which a large number of serial writing requests are continuously sent to the same work file disk and prevent other reading requests for a period of time.
The current paper lists some important changes facing the database community and introduces some agenda on how to address these challenges.
With increased global exposure, today’s have to respond quickly to change, rapidly develop new services and products, while improving productivity and quality, reducing costs. business process reengineering and workflow automation activities throughout the enterprise are recognized as important emerging technologies to support these requirements. Rosie estimates that more than a billion workflow software market has led to important business activities in the field, almost a hundred products now claim to support workflow automation although many help automating documents and image-driven office applications.
In the past few years, Internet services have not only entered the main flow of society, but also moved to a clear best effort service model is no longer suitable area. This phenomenon is made by two major impulse areas: e-commerce, where poor performance or availability can be very expensive, streaming services (including voice over IP) where the quality of the service is a basic requirement. These areas bring a range of performance, availability and construction problems that must be effectively solved to prevent a wide range of customer dissatisfaction, which may have a negative impact on the long-term growth of online services.
Using views to answer queries is to find an effective way to answer a query, using a set of previous databases views rather than access to databases relationships. The problem has received significant attention as it relates to a variety of data management issues, such as data integration, query optimization, andining physical data independence. So far, the performance of the proposed algorithms has received little attention, especially, their scale in a large number of views of existence is unknown. We first analyze two previous algorithms, the cookie algorithms and the opposite rules, and show their disadvantages. Then we described the MiniCon, a new algorithm to find the most containing re-read a connection algorithm, using a setup algorithm, we use the current algorithm algorithm.
When Alex Labrinis asked me to write this paper, I was silent at first, and I was glad to talk about scientists around the world, even just SIGMOD scientists, but later I realized that I could speak from personal experience, so these random papers were necessarily completely subjective, very personalized and non-representative... The characteristic that a scientist usually tries to strongly avoid in his writing I’m not a “typical” scientist (I don’t know such a animal), but I can talk about some authority about what motivates me.
Setup Value Properties are a short and natural way of modeling complex data sets. The modern object relationship system supports setup value properties and allows a variety of query capabilities. In this article, we launched the formal study of setup value properties based on similarities of indicator technologies, the proper definition of similarities between groups. These technologies are necessary in modern applications, such as through collaborative merger and automatic advertising recommendations. Our technology is essentially likely and approximate. As a design principle, we create the structure of the known and widely used data structure technology as a means of integration with existing infrastructure.
Support vector machines (SVMs) have shown excellent text classification tasks. they are accurate, solid, rapidly applied to test examples. their only potential disadvantages are their training time and memory requirements. for n training examples keep in memory, the most famous SVM implementation takes time ratio, where it is usually 1.8 and 2.1.
In the past decade, a lot of work has been done in the development of integration and automation of business processes. Today, with the growth of e-commerce and the confusion of corporate boundaries, there are new interests in business processes coordination, on cross-organic processes. This paper provides a historical view of technology, internal and cross-corporate business processes, reviewing the state of art and reveals some open research issues. We include a process-based coordination and event-based coordination discussion, as well as the corresponding products and standard activities. We provide a fairly broad work over the advanced trading model of business processes, as well as the process information field.
In this paper, I will explore a field of science where this situation is miserable - the field of biodiversity science. Crane's model works best in physics, where no assumption of the information collected in the early nineteenth century is still interested in the current generation of theorists. There is a assumption (for example) that the new theory will rearrange the field of knowledge in an effective and effective way; since Cohen(5) will accept, in the "gravity" understanding, a major paradigm change, saying that making the previous work on the sliding aircraft is literally unmeasurable - not to mention the technical improvement, making the old work too inaccurate.
Many aspects of time-based media - complex data coding, compressed, "quality factors", timing - problems arise from the point of view of data simulation. This paper proposes time flow as a simulation of time-based media's basic abstraction. Some media independent structuring mechanisms were introduced and proposed a data model rather than allowing multimedia data to interpret applications, solving complex organizations and relationships that exist.
In the bottom of the logical program assessment and re-defined the view of the database, all the facts generated are usually considered to be stored at the end of the assessment. However, the exclusion of facts during the assessment can significantly improve the efficiency of the assessment: the required space to assess the process, I/O costs, the cost ofining and accessing indicators, and the cost of eliminating the copy can be reduced. Considering that a assessment method is clear, complete, and not repeated derivative steps, we consider how facts can be eliminated during the assessment, without damaging these features. We show that each space optimization method has some components, first is to ensure clarity and integrity, second is to avoid the copy (i.e.e.e.e.e.e.e.e.e.e.e.
Many social applications, for example, in the fields of healthcare, land use, catastrophic management and environmental monitoring, are increasingly dependent on geographical information for their decisions. With the appearance of the World WideWeb, this information is usually located in multiple, distributed, diversified, independent maintenance systems. Therefore, in these social applications, strategic decisions depend on the ability to enrich with geographical information related grammar to support a variety of tasks, including data integration, interactivity, knowledge recycling, knowledge acquisition, knowledge management, space rationalization, etc.
This article will highlight the political issues in this prohibition by analyzing the process of legislative reforms in Hong Kong, especially the gray field of public debate. attempt to break the dialogue on "for" or "opposition", which is often typical of the debate on these symbolic mammals' extinction. In this news comment, I took a detailed analysis of the local newspaper articles, essentially the article of the English newspaper. From the 41 articles reviewed, I chose 21 articles, based on the relevance of the legislative reforms in Hong Kong and the diversity of its content.
This paper proposes and evaluates predictions B+3 (pB+3) using predictions to accelerate two important operations in B+ Tree Index: search and range scanning. to accelerate search, pB+3 uses predictions to effectively create wider nodes than the size of natural data transmission: for example, eight pair of a cache line or disk page. These wider nodes reduce the height of B+ Tree, thereby reducing the number of expensive errors from parents to children, without significantly increasing the predictive cost of a nodes. Our results show that this technology accelerates the search and updating time factor is 1.21-1.5 main B+T memory.
We will share with the readers some good news about the NSF and the Defense Budget, as well as some interesting new projects on the DARPA and the NSF.
A major challenge that designers and executives of the database programming languages (DBPLs) still face is query optimization.We study the DBPLs algorithm query optimization technology, in the background of a pure declaration functional language, supporting settings as the first class objects.As the language is calculating the complete problem, such as not terminating the expression and building the unlimited data structure can be investigated, while its declaration nature allows the problem to be avoided and develop a richer grade.
We introduce an optimized method and algorithm designed for three objectives: physical data independence, academic optimization, and general chart minimization. The method depends on the general form of tracking and “backpack” with restrictions (depending on details). By using the dictionary (terminal function) in the physical program, we can catch the lines of useful access structures, such as indicators, materialized view, source capacity, access support relationships, gmaps, etc. Search space query program is defined and listed in a new way: query phase rewrite the original query to a “universal” program, which integrates all access structures and alternative routes, allowing through the cable restrictions.
Data integrated query processing occurs on a network-connected, independent data source, which requires expansion to the traditional optimization and implementation of technology for three reasons: lack of data quality statistics, data transmission rates are unpredictable and destructive, while slow or unavailable data sources can often be replaced by excess or mirror source. This paper introduces the Tukwila data integration system, designed to support adaptability in its core using a double method. indirect planning and implementation with partial optimization enables Tukwila to quickly recover based on inaccurate estimates of decisions.
The European Commission in its seventh IST calls for a proposed action line for Semantic Web Technologies. It is based on thought, has been created for many years, but has gained their greatest motivation when the World Wide Web Alliance establishes an interest group on this topic. The Semantic Web is designed to make the content machine understandable to automatize a wide new task in the background of different and distributed systems.
In this context, taking into account the characteristics of the document data set, we propose 2Step-SSJ to solve the document similarity in the CUDA environment. 2Step-SSJ performs the similarity similarity calculation in two steps, i.e., the similarity calculation in the conversion list and the similarity calculation in the previous list, which damages memory access and dot- product calculation.
In this article, we first described the XFilter and YFilter methods, and introduced the detailed performance comparison of the structures that match these algorithms as well as the result of a mixed method. Results show that the route sharing used by YFilter can provide the benefits of order size performance. Then we proposed two alternative technologies to expand the YFilter shared structure match, support value-based predictions, and compared the performance of these two technologies.
In this article, we presented the first space efficiency algorithm solution to estimate the frequency of the full set of expressions in the general update stream. Our estimated algorithm is the probability in essence and depends on a new, based on the hash symphony data structure, called the "2 level hash slide. We showed how our 2 level hash slide symphony can be used to provide low errors, high confidence estimates to set the frequency of the expressions (including operators such as setting the alliance, cross, and differences) in the continuous update stream, using only small space and small processing time for each update.
Many database applications are widely using BitMap Indexing Systems. In this article, we study how to improve the efficiency of these indexing systems by providing new compression systems for BitMap. Most compression systems are mainly designed to good compression.
In order to reduce the storage costs, overflowable predictable technology uses the overflowability of data and avoids the substantialization of the overflowable amounts of the empty lines and columns in the data network; on the contrary, overflowable charts are used to save the durable query time. overflowable predictable amounts are the first method to the time of query to allow overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable amounts of overflowable
The Flexible License Framework (FAF) defined by Jajodia et al. [2001] provides a policy-neutral framework to determine access control policies that are sufficient to describe many known access control policies. Although the original format of the FAF shows how to add or delete the rules from the FAF specifications, it does not deal with the deletion of access licences from users.
The web page is based on a browsing mode that makes it difficult to obtain and integrate data from multiple websites. Today, the only way to this integration is by building specialized applications that take time to develop and maintain difficulties. We are solving this problem by creating technologies and tools, rapidly building information intermediaries, extracting from web sources, queries and integrating data. The result system, known as Ariadne, allows it to quickly build information intermediaries, access to existing web sources.
In this article, we reviewed the dual-space fusion algorithms and showed how they combine multiple input. In addition, we explored the application of synchronic channels (ST), a method that processed all input while not producing intermediate results. Then we integrated two methods into one engine, including ST and dual-space algorithms, using dynamic programming to determine the best implementation plan. Results show that in most cases, multi-space fusion is best processed by combining ST and dual-space methods.
Users usually see the data as a multidimensional database. Each cell of the database is a view, by an accumulated interest, such as total sales. Many of the values of these cells depend on the values of other cells in the database. A common and powerful query optimization technology is to put some of these cells or all, rather than from the original data every calculation. Business systems are mainly different from their methods to the database. In this paper, we study the question, which cells (visions) to, when it is too expensive to all the views. A mild framework is used to express the dependency between the views. We introduce the calculator, starting from this point of view to work and determining a good point of view.
This name gives a new SQL standard draft, which could become an international s label alternative to SQL92 in 1996 or 1997 containing several object-oriented extensions. When defining these extensions, the X3H2 (the U.S. Committee responsible for SQL Specifications 3) and the DBL (the International Committee for the same purpose) had (and still is making) some of the same decisions made by other object-oriented language designers.
In September 1999, the first part of SQLJ was adopted as NCITS 331.1-1999 and can now be purchased from NCITS. It is worth mentioning that this specification is very close, with a long tutorial part introducing its more standardized elements. Sybase brought the first part of SQLJ to the SQLJ group at the beginning of 1997.
We study a set of linear transformation in the Fourier series represents a series that can be used as the basis of similarity queries in time series data. We show our transformation group rich enough to develop operations such as moving average and time rotation. We introduce a query processing algorithm, using the basis of the R tree index of multi-dimensional data settings to effectively respond to similarity queries. Our experiments show that this algorithm performance is the competitor's processing of ordinary (precision matching) queries using the index, and compared to the series scanning.
We have developed a web-based architecture and user interface to quickly store, search and obtain from scientific simulation of large, distributed files. We have demonstrated that the new DATALINK type defined in SQL management of external data standards can help overcome problems related to limited bandwidth when trying to archive large files using the Web. We also show that the user interface specification with user interface processing can provide some benefits. We provide a tool to automatically generate a default user interface specification, in the form of XML document, for a database.
Based on the network of data sources, especially in life sciences, in diversity and quantity growth. Most data collection is equipped with common document search, hyperlink and access tools. However, the user’s desire is often beyond simple document-oriented queries. With regard to complex scientific issues, it becomes necessary to help knowledge from huge mutual dependence, so it’s more difficult to understand the data collection.
The use of social media in defence and transnational defence has caused concerns about the privacy and security of people in defence and their social media contacts. This chapter introduces an advanced summary of social media cases and activism from the point of view of information war and information security. From the analysis of these, the influence and relationship of social media in transnational defence and information security is being discussed. While online defence can be considered a form of information war that matches the theory of cyber power, it can be considered that social media defence will have a negative impact on information security as it encourages various participants to actively attempt to violate security.
In the temporary OODB, it requires an OID index (OIDX) from the OID map to the physical location of the object. In the temporary OODB, OIDX should also index the object version. In this case, the index input, we call the object description (OD), also includes the promised timetable of the object version of the transaction. In the non-temporary OODB, OIDX only needs to be updated when the object is created, but in the temporary OODB, OIDX must be updated every time the object is updated.
We reviewed the selective estimates of the regional and spatial merger in the real space database. As we previously showed, the actual point setting: (a) continuously violates the "coherence" and "independence" assumptions, (b) can often be described as "division" with non-integrated (division) sizes. In this paper, we show that in the unlimited division size family, the so-called "interrelation size" Dz is we need to predict the selectiveness of the spatial merger.
One key aspect of the interaction between data-intense systems is the mediator of the data-intense systems and the mediator of the database boundaries.A way to the mediator between the local database and the remote database is to insert remote data into the local data, thus creating a common platform to be possible through information sharing and exchange.
In this article, a new detection-based distributed terminal detection algorithm was proposed by Chandy's et al. It was an improved version of the algorithm initially proposed.. The new algorithm has been proven to be error-free and has suffered a few performance decreases from the top of the additional terminal detection. The algorithm has been compared to the modified detection-based and time-extraction methods.
At the end of 2000, the work completed another part of the SQL standard, we introduced our readers in the early version of this column. Although the SQL database system manages a lot of data, it does not have a monopoly on this task. The huge amount of data remains in the ordinary operating system files, the network and the base database, as well as other storage databases. Need for query and manipulation, these data with SQL data continue to grow. The database system provider has developed many methods to provide such integrated access. In this (some guests) article, the new part of SQL, external data management (SQL/MED), is studied to the readers a better concept of how applications can use the standard SQL to access their SQL data and non-SQL data.
A order dependent query is one of the results (interpreted as multiple) of changes, if the order input record is changed. In a stock pricing database, for example, the recovery of all references about a database data on a date is not dependent on the order, because the collection of references is not dependent on the order. In contrast, finding a stock of five price moving average on the trading table gives a result, depending on the order of the table. Based on the relative data model query language can process the order dependent query only through the add-on. SQL:1999 for example, there is a new "window" mechanism that can classify the data in a limited part of the order. The add-on order depends on the order, and in the order, we can optimize the standard of the file that we can display a natural order.
Improved Pay-per-view (EPPV) model provides continuous media-on-demand (CMOD) service, combining a display frequency with each continuous media clip, which depends on (may) different display frequency, frequency and length of the clip. Our main objective is to increase the number of customers that can serve simultaneously, exceed the capacity limit of available resources, while ensuring the response time limit. This is achieved by sharing regular continuous media stream between multiple customers. In this article, we provide a comprehensive study of the resource programming issue, with supporting the EPPV continuous media clip with (may) different display frequency, frequency and length. Our main objective is to maximize the number of bands, effectively programming data, and storing in real time this formula, we provide a complex solution
Real-time virtual channel data represents virtual environment (VEs) increasingly bigger to better simulate reality scenes, which raises interesting challenges, organizing, storing, and navigating data for interactive VEs, or channels. A large VE is usually composed of thousands of 3D objects, each of which can be represented by hundreds of polymers and may take thousands of gigabyte of storage space. The number of data is so large that it is impossible to store all of these in the main memory.
Random data interference (RDP) is discussing the method of keeping personal records in the statistical database. in particular, it shows that if the confidentiality properties are permitted as the query defined variable, serious confusion may lead to the query answer. it also shows that even if the query defined by the confidentiality variable is not permitted, confusion can still occur in the query answer, such as involving proportions or calculations. in any case, serious confusion may occur in the user statistical analysis.
Today's network consists of HTML files designed only for the human eye, although many of them are created automatically by applications, but other applications are difficult to read and handle them, which may quickly change due to a series of new standards, the World Wide Web Alliance focuses on XML (extended label language). XML is designed to express the document content, while HTML expresses its presentation. In short, XML is a data exchange format that is easy for the application to understand. It allows data exchange on the network, both within the enterprise, cross-platform (intranet) and between the enterprise (internet).
Data loading is one of the most important operations in any database, but it is also one of the most ignored operations by the database provider. data must be loaded into the warehouse within a fixed package window, usually at night. During this period, we need to maximize the machine resources to load the data as effectively as possible.
In this article, we introduce a new flexible architecture (known as HODFA) to dynamically connect these unusual databases, forming a uniform federal database system and supporting the conversion of an unusual information system to a uniform environment. We further develop an increased methodology of uniformization in the background of our HODFA framework, which can promote different degrees of uniformization in a step way so that existing applications are not affected by the process of uniformization.
As we increase our dependence on computers and computer data, we’ve expected more from our computers. We’re no longer expecting our computers as big expensive computers, these computers are just excluding bills and payment checks. We now, in addition, expect our system to be quickly accessed and interactive to show us a lot of accurate data. In fact, our expectations have occurred so much, in the last decade we’ve no longer focused on what our systems are, but what they do. We’re no longer mentioning our systems as computer systems, but information systems. These new expectations have brought new information systems responsibility.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth seeing why we do research, and this paper has made a huge contribution to the self-examination process.
Electronic Dictionary System (EDS) is developed on the basis of ObjectStore’s object-oriented database technology. EDS consists of two parts: the database building plan (DBP) and the database query plan (DQP).DBP read in the SGML label coded dictionary and built a database consisting of trees, holding dictionary entries, as well as multiple lists, containing different dictionary categories of items.
This tutorial introduces the latest advances in the field of Java and Relationship Database. The material is based on the efforts of the SQLJ Alliance, which aims to use Java technology for SQL processing. The efforts of SQLJ are driven by major industry suppliers such as Oracle, Sybase, Tandem, JavaSoft, IBM, InfoMix, etc.
In this article, we ask whether the traditional relative query accelerated technology has a summary table and covering index similar to the trail expression query or the graphic structure of XML data. Our answer is --- the front and back indicators already proposed in literature can be considered structural similar to the summary table or covering index.
One important step in integrating the database is to match the same properties: determine which fields in the two databases refer to the same data; in a series of integrations, the properties are compared in a pair of ways to determine their similarity; automation is essential for integration because the capacity of the data or the number of integrated databases increases; Semiut "discovers" how to match the same properties from the information that can be automatically extracted from the database; on the contrary, requires human intelligence to predict what makes the properties the same.
First, introducing dbjobs, the database work, and describing its features and architecture, second, introducing statistics of the dbgrads system, after 18 months of continued operation, and finally, describing the interesting future of the SIGMOD Online project.
Experimental results show that distributed commitment processing can have a greater impact on the performance of the channel than distributed data processing, and the choice of the commitment protocol clearly affects the degree of this impact. In the evaluated protocols, the new optimistic commitment protocol provides the best trading channel performance for various work loads and system configurations. In fact, OPT's top channel is often close to the peak of achieved performance. More interestingly, OPT's three-stage (i.e. non-block) version is better than the top channel performance of all the standard two-stage (i.e. block protocols) evaluated in our study.
Many negative results demonstrate the ability to type check queries for the only existing standard of the object-oriented database. One of the negative results is that the type check OKL queries cannot be carried out in the ODMG object model and its defined language of the type system. The second negative result is that the OKL queries cannot be carried out in the type system of the ODMG standard Java connection. The solution proposed in this article is to extend the ODMG object model to clearly support the parameter diversification (universal typing). These results show that Java cannot be a feasible database programming language unless extended to the parameter diversification.
In the 1980s, Chrts Dare’s “12 Rules” distribution of the database system includes copying.Repi ication makes the problem of remote access transparent, data collection management.
The Italian surgical community lost one of the best sons as this tragedy is striking our civilization world. The world-famous surgeon, Professor Valerio Di Carlo, began his career in the emergency surgical department of the Milan Police Clinic, directed by Professor Vittorio Staudacher.
Question optimization is by creating a chart of queries and moving predictions around the chart so that they will be applied early in the chart generated optimization queries. Predictions first spread from the chart's children's nodes to the parent's nodes, then down to different children's nodes. After the predictions are moved, the turning predictions are detected and removed.
Strud system applies the concept of the process from the database management system to the building of the site.Strud's key idea is to separate the site data from the management, the creation and management of the site structure, as well as the visual presentation of the site page. First, the site maker created a unified model, all the data available on the site. Second, the maker uses this model to expressly define the site structure, by applying a "site defining query" to the basic data. Evaluating this query result is a "site map", representing the site's content and structure. Third, the maker specifies the graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph
Time reference data in most real-world databases are transparent.The latest advances in the temporary query language show that these databases applications can benefit significantly from the built-in temporary support in DBMS.To this goal, the temporary query optimization and evaluation mechanism must be provided, whether in the DBMS appropriate or as a source-level translation from the temporary query to the conventional SQL.This paper proposed a new method: using a intermediate software component at the top of a conventional DBMS.The component accepts the statement of the temporary SQL and produces a corresponding query program, consisting of the algorithm and the conventional SQL part.The algorithm part is processed by the intermediate software, while the SQL part is processed by the DBMS.
BIRCH continuously and dynamically integrates multi-dimensional measurement data points, trying to generate optimal quality integration with available resources (i.e. available memory and time limits).BIRCH can usually find good integration through individual data scans and further improve quality through some additional scans.
Time parameters queries (TP queries are short) obtain (i) the actual results when queries are issued, (ii) the validity of the results, according to the current movement of queries and databases objects, as well as (iii) resulting in the end of the results changes. Due to the very dynamic nature of multiple space time application, TP queries are important, both the individual methods, and the building of blocks more complex operations. However, on its effective processing has been done a little work. In this paper, we presented a general framework covering the most common space queries of time parameters, i.e. window queries, core queries and space.
One important problem in data mining is the finding of the association rules in the transaction database, each transaction consists of a set of projects. In this discovery process, the most time-consumed operation is the frequency of calculating the interesting subgroups of the transaction database (the so-called candidates) occurring. To predict the wide space of the candidates, most existing algorithms, only consider those who have the user defined the minimum support of the candidates. Even in the forecast, finding all association rules requires a large amount of calculation power and time.
The rules in an active database system may be difficult to program because the rules are processed by an unstructured and unpredictable nature.We provide static analysis techniques to predict whether a rule group is guaranteed to end, and whether the rule implementation is confused (assure that there is a unique final state).Our method is based on the previous technical analysis rules in an active database system.We significantly improve the previous technology by providing an analytical standard, less conservative: Our methods are often determining whether a rule group will end or confuse when the previous methods cannot make this decision.
This article introduces it from TOS, a unique Ldbit operating system, to the parallel improvement of the Teradata database port of the SVR4Unix system, providing an architectural overview of how the Teradata database solves the main VLDB problems: performance and reliability.We will introduce it from the database computer DBC/lOlZ node (Interface Processors-IFPs and Access Module Processors AMPS) to the virtual processor (vprocs), while running on, the collection of the SMP node.We also introduce the parallel database environment (PDE) added to the Unix, which makes it possible We will discuss our performance improvement work results and future direction.
Dwarf is the high-pressure structure of the database.Dwarf identifies the budget and the suffix structure discount and guides them by collecting its warehouses.Budget discount is high in the intense areas, but suffix discount is significantly high in the distributed areas.The two are gathered together, integrating the high-size full-circle exposure size into a significantly compressed data structure.Elimate suffix discount is also significantly reduced in the calculation as avoiding the discount of suffix.
Imagine you’re a “knowledge worker” in the coming millennia, which means you have to synthesize information and make such decisions as “What benefits are planned to use?” “What rules say about this action process?” “How does my work fit for the business plan?” “What should I pay attention when I approach this customer?” even “How does this program work?” if the dream of the digital library is to give you all the relevant materials you might find yourself drowned a long time ago.
In this article, we introduce an effective way to do the online restructuring of the rare population of B+ trees. it restructures the leaves first, compressed in the short operation of the leaves group with the same parents. after compressing, the new leaves can be replaced or transferred to the empty page so that they are in the key order on the disk. after, the leaves restructures, the method of reducing the trees, by making a copy of the ceiling, while leaving the leaves in place. a new competitive method is introduced so that only a small number of pages are locked during the restructuring.
In this article, we study what problems exist in a very large real database and describe what mechanisms the Informix Extended Parallel Server (XPS) provides to solve these problems.The current largest customer XPS database contains 27TB of data.The database server must provide a mechanism that can the appropriate performance and relieve availability.We will introduce the mechanisms to solve these two problems and describe them in the example of a real customer system.
In the existing relative database systems, the processing of the component and the calculation of the integrated function has been delayed until all the component are completed. In this article, we introduce the transformation that allows the component to drive the operation of more than one or more component groups and can potentially reduce the cost of processing a query significantly. Therefore, the component group should decide according to the cost estimate. We explain how the traditional system R style optimizer can be modified by containing the enthusiasm of our development.
We describe the SCC-kS, a speculative monetary control (SCC) algorithm that allows a DBMS to effectively use the system’s available additional calculation resources to increase the possibility of transaction promises in time.Using the SCC-kS, to the shadow transaction to speculate, represents an uncommanded transaction to protect from obstacles and recovery risks.The SCC-kS allows the system’s scale speculation level, each transaction is allowed to be carried out, thereby providing a simple transaction resource mechanism of time.
Data Mining: Practical mechanical learning tools and technologies provide a thorough basis for the mechanical learning concepts, as well as practical advice on applying mechanical learning tools and technologies in real-world data mining cases. This highly anticipated third most popular work data mining and mechanical learning will teach you all you need to know about preparing input, interpreting output, evaluating results, and algorithmic methods in the core of successful data mining.
The eighth International Knowledge Representative Database Seminar (KRDB) took place at the Ponti cia University in Rome, until after VLDB.KRDB was launched in 1994, providing researchers and practitioners with the opportunity to exchange ideas and results from two fields.This year’s focus is on modeling, querying and managing semi-structured data.
These search engines are usually incompatible because they support different query models and interfaces, they don’t return enough information with query results to properly combine the results, and ultimately they don’t export about their index collections (e.g., help resources find). this paper describes STARTS, a emerging Internet query and search protocol to query multiple file sources tasks. STARTS has developed a unique way. it’s not a standard, but a group of efforts, coordinated by Stanford’s digital library project, and involves 11 companies and organizations targets that they don’t export about their index collections (e.g., help resources find). this paper describes STARTS, a emerging Internet query and search protocol to query multiple files.
In order to avoid this problem, we introduced a new directory organization, using a split algorithm to reduce transition and further exploit the concept of supercode. The basic idea of transition to reduce divisions and supercode is to keep the directory as sequential as possible while avoiding divisions in the directory, leading to high transition
The front domain independently solves this problem depends on the multi-properties between the standard text similarity functions (e.g., editing distance, synchronizing measurement). However, this method leads to a large number of false positive if we want to identify the specific domain shortcuts and conventions. In this article, we develop an algorithm to eliminate the double size table in the database, usually associated with a series. We use a series to develop high-quality, extended double elimination algorithm and evaluate from a database of real data.
This chapter discusses how to manage personal databases or databases cross. Databases management problems have entered everyone's life. To realize its existence, it is enough to sit down and survive an essential electronic data source in modern society. As long as the data source is independent, the device will never be replaced, and the new device will not enter the existing field, it will easily survive in the digital jungle. However, life runs a different track. Every time a person meets a new person, you may need to synchronize multiple databases with his address information.
Today’s Internet-based business needs a level of interaction that will allow trading partners to gather smoothly and dynamically together, do business, no specific and characteristic integration. This level of interaction involves being able to find potential business partners, find their services and business processes, and conduct the business “on the aircraft.” This dynamic interaction process is only possible through the standard B2B framework. In fact, some B2B e-commerce standard frameworks have recently appeared. Although most of these standards are beyond and competing, each with their own tensions and weekend, a closer survey reveals that they can complement each other in a way. In this article, we describe such a implementation where an EBML infrastructure is generally described by Discovery and Discovery (Discovery) and Discovery (Discovery) (Discovery)
While the study of temporary databases has been active for about 20 years, the implementation has not yet appeared, which is one of the reasons that the current business databases only provide limited temporary functionality, this paper summarizes the broad state of the art of the temporary databases implementation rather than very specific to each system, we try to provide functional instructions, as well as more information indicators, hoping that this will lead to more efforts to promote the implementation of temporary databases in the near future.
Question answers use extended information recycling technology to be ranked and produced in a similar order to rankings.Advanced indexing technology is developed to promote the effective implementation of XSEarch.
In this article, we introduce OPOSSUM, a flexible, customizable and extensible chart management system. Working in the framework of the direct operation of the editing chart, OPOSSUM adopts several new technologies to provide the following abilities: through the user’s specific information enhanced chart; through the visual expression of the choice of the chart; and creating new visual expression styles when the existing chart proves not satisfied. We discuss the system’s architecture and guide its development methods, and by using the example describe its most important characteristics.
The process is increasingly used to make the complex application logic clear.The use of the process programming has significant advantages, but from a system perspective, it raises a difficult problem, the interaction between the processes cannot be controlled through the traditional technology.In terms of recovery, the process steps are different from the operation in the transaction.Everyone has its own terminality, and there is a different dependency between the steps.On the control of the competitor, the process control processes are more complex than the tablet transaction.A process can, for example, partially rotate its implementation or can follow a variety of alternatives.In this article, we solve the atomic and insulation problems in the context of the process.We have proposed a unified model to control the competitor and the process of recovery, and in the practice how this model can provide a complete application framework
Recovery can be extended to new domain names by using "logic" logic operations, and in the recovery process, logic logic operations can read data values from any recoverable object, not just from the values on the log or the updated object. therefore, we don't need to record these values, which is a significant savings. in [8] we developed a recovery theory, processing general logic operations, and proof that the stable database is still recoverable when interpreted in the installation chart.
The project focuses on the field of technological information systems that need to support the simulation tools of complex objects.Designers in this field usually use gradual design or gradual prototypes, as this seems best suitable for users to deal with complexity and uncertainty about their own needs or needs.
Recent requirements for big data queries reveal a variety of disadvantages in the traditional database system, which in turn leads to the appearance of a new query pattern, closer to query. online integration is a sample-based technology to approach query. in the era of information explosion, it becomes very essential. online integration continuously provides some error estimates (usually confidence intervals) of closer results until all data is processed.
Our results show that these policies are effective in achieving user-defined I/O operations and database waste percentage, and we also investigate the policy sensitivity of various objects connected, and evaluate that semi-automatic, self-adjusting policies are a practical means for flexible control of waste collection rates.
A single channel calculation and visualization engine will be demonstrated to allow an interactive analysis to contain dozens of records of VLDB. The engine allows one to calculate millions of different amounts from one channel. Each calculation can be carried out throughout the multi-domain and all the available subdomain by limiting one or more differences to expand its values to any subgroup and one or more continuous variables to expand its predefined characters to any subgroup.
Specifically, we use modern concepts from shapes, n; that is, the "model spectrum" of shapes, maps of each shapes to a point in n size space. Next this book is a guide to the methodology of the engineering basis, a development field involves the definition of the technical design software system. The method is based on the model, building a model about a collection of other models. This book applies to the model method in five case studies, each describes one solving a problem in a specific field. Suitable for classroom use, this book is also useful reference practitioner. The first book introduces the theoretical basis of the model method of engineering, discuss information, model potential software development, model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model
This text is a guide to the basis of methodological engineering, a field of development involves the definition of the technology of the design of software systems. The method is based on the model, the model building about the other models. This book applies the model method in five case studies, each describing one solving a problem in a specific field. Suitable for classroom use, this book is also useful for reference practitioners. This book first introduces the model theoretical basis of methodological engineering, discusses the information model, the software system development model potential, and introduces the model tool ConceptBase.
The camp properly focuses on some important aspects of the relationship theory evolution since 1969, especially the idea that our respected originator E.F. Codd chose to call the topic. The camp's voice sometimes shows that we (in the relationship camp) were the guilty of the war on the questions we later mentioned, it's too late. I think it's an exaggeration that we can do a reasonable opposition so that we can submit all the clarifications, after very careful research, these years, match what we said before.
This article describes external strengths that motivate financial institutions to collect, integrate, analyze and mining data so that it can be converted into one of the information, the most valuable assets of financial institutions. In this article, we call this strategic information asset "information currency". In general, we describe the situation of the bank and influence the rapid global change of financial institutions. We analyze how the U.S. Bank (BofA) creates and uses its information currency using the TeradataTM Relationship Database Management System (Teradata RDBMS).
In this article, we study the wood model reduction, both in the absence and existence of integrity limits (ICs) in the bottom of the wood structured database, we can develop a technology-based AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based, AC-based
Due to the simplicity and communication efficiency, some existing object-based database management systems are based on the page server architecture; the data page is its minimum unit of transmission and client transmission. Despite its efficiency, the page server is often criticized as too limited when involving competition, because the existing system uses the page as the minimum lock unit too. In this article, we show how to support the object-level lock on a page server background. Some methods are described, including an adaptive size method, using the page-level lock most of the pages, but exchanged to the object-level lock when it takes more accurate sharing.
Successful companies organize and carry out business activities in an efficient way. core activities are completed in time and within a specific resource limit. However, in order to remain competitive in today's markets, companies need to constantly improve efficiency - business activities need to be completed faster, higher quality and lower cost. For this purpose, people are increasingly aware of the benefits and potential competitive advantages, and can provide a carefully designed business process management system. In this paper, we discuss an agent-based method: showing how agent technologies improve efficiency, ensuring business activities better planning, implementation, monitoring and coordination.
In this continuation of Estra Dufflo's deep investigation into the influence of women's leaders in India from 1998 to 2008, the goal is to measure whether there are women's prime ministers (state leaders) with increased educational investments compared to those where men remain dominant in the leadership positions.
IRIS, an integrated retinal information system, has been developed to provide medical professionals with easy and unified access to the scan, trends and progress of diabetes-related eye diseases in the database.
Major changes in the business environment - as well as aggressive initiatives for the rebuilding of the business processes - have caused the corresponding changes in the information technology architecture of large enterprises. These changes are possible by the proximity of a long-term list of mature new technologies. As one of its many consequences, the new IT architecture requires a review of the assumptions on the design and deployment of the database.
Information integration provides a competitive advantage for the business and is crucial in the computing of demand. This is the strategic field of software companies investing today, its objective is to provide a unified view of data, regardless of the differences in data format, data location and access interface, dynamic data management positioning to meet the availability, monetary and performance requirements, and to provide independent functions to reduce the burden of IT personnel managing complex data architecture. This paper describes the motivation for information integration to demand computing, explains its requirements, and describes its value through the use of scenes. As shown in the paper, there is still a large number of research, engineering and development work that needs to make information integration vision a reality, and the software company will continue to invest in information integration vision.
This article introduces a database programming language, theme that supports classification and class, and allows to define integrity limitations in the global and declaration method. We first describe the language’s outstanding characteristics: type, name, class, integrity limitations (including methods), and transactions.
Relationship with technology development databases, this basis in logic means that driven databases are able to process a lot of information, as well as execute the rationalization of this information. There are many applications in the fields of driven databases technology. A field is decision support system. In particular, the use of a organization's resources requires sufficient information about the current and future situation of the resources themselves, but it is also an effective rationalization of the future of the program. The current generation of decision support systems is seriously lacking when it comes to the rationalization of the future plan. Driving databases technology is the appropriate solution to this problem. Another useful application field is the expert system. There are many computing applications, in which a lot of information from these factors can be spread to a simple process, but can be through a simple process, through a simple
The aerospace industry poses major challenges to information management, unlike any other industry.Data management challenges generated from different sectors of aerospace business are determined by describing scenarios.These examples and challenges can provide focus and stimulation for further research in the field of information management.
In the past few years, workflow management has become a popular topic for the research community, especially on the business stage. Workflow management is a multidisciplinary nature that covers many aspects of computing: database management, distributed client server systems, transaction management, mobile computing, business process heavy engineering, heritage and new applications integration and the abnormality of hardware and software. Many academic and industrial research projects are underway.
Visual information, especially video, plays an increasing role in our society, both work and entertainment, as more sources become user availability. The upper box is designed to allow household users access to video, not only from TV channels and personal recordings, but also from the Internet in the form of downloads and streaming videos of different types. Existing methods such as e-program guides and video search engines search for video assets of a type or from one source can easily be found through many types of video assets from a large number of video sources and easy to use user files.
After the system collapse, the database returns to the final transaction, but the application usually collapses or cannot continue.The purpose of Phoenix is to keep the application state stable in the system collapse, the transparency of the application.This simplifies the application programming, reduces the operating costs, covers the user’s failure, and increases the application availability, which is crucial in many scenarios, such as e-commerce.In the Phoenix project, we study how to provide the application recovery efficiency and transparency by re-logging.This paper describes the concept framework of the Phoenix project, as well as the software infrastructure we are building.
Classification is an uncontrolled process because no predefined classes and no examples will indicate the classification properties in data concentration.The majority of the classification algorithms act differently according to the characteristics of the data set and the initial assumptions of the defined group.Therefore, in most applications, the result classification program requires some assessment of its effectiveness.Evaluation and evaluation of the results of the classification algorithms are the main topics of the classification effectiveness.
The end of the Cold War brought significant changes, GM Hughes Electronics, one of the world's leading satellite and defense electronics companies. Their reaction, the loss of defense income was married to their satellite communication expertise with the rapidly expanding entertainment industry producing DIRECTV, "the first full-digital live satellite (DBS) service in the United States. For years, in rural areas, customers have been using large, neglected satellite meals to receive TV programming. The costs, scale and complexity of these systems have limited their attractiveness. Hughes now launched two geosynchronous satellites, using high-power transmitters to send the electrical flow of compressed digital data to 18 inches of the antenna, which can be allocated by wireless special video processors and video transm
This article introduces a method to protect the semi-originity of flexible transactions (a weaker form of originality) that allows the local site to independently maintain sequence and restorability.We provide the basic characteristics of the flexible trading model and accurately define the semi-originity.We study the commitment dependency between the flexible transactions.These dependencies are used to control the commitment order of the sub-transactions.
For example, in the case of 70% to 80% of transactions under the global locked protocols are abortioned, only 10% of transactions are abortioned under the copy-based protocols. studies show that the copy-based protocols provide practical techniques for copying management.
Publishing/Subscribe mode is a simple use of interactive pattern by the information provider who publishes the event to the system; and the information consumer who subscribes the event interests within the system. Publishing/Subscribe system ensures the subscriber’s timely notification of the event occurs. The event can be considered as the data item (image, column, or table) in the relative database model and subscribe closely similar to the database query.
In this article, we introduce the first index structure, called the QIC-M tree, which can process the user’s defined query in the genetic measurement space, i.e., where the only information about the index objects is their relative distance.QIC-M tree is a measurement access method that can be processed simultaneously with several different distances: (1) a query (user defined) distance, (2) an index distance (for building a tree), and (3) a comparison (approximately) distance (for quick departure from the search uninterested part of the tree).
Materialized view and view maintenance are critical for databases, retail, banking and billing applications.We consider two relevant view maintenance issues: 1) how to maintain the view after the base table has been modified, 2) how to reduce the time the view is unavailable during maintenance.
We use the ordinary hardware of the server and client and systematically review the bottle and optimize options to reduce the weaknesses and increase the maximum number of customers that the system can support.We show that the diversity of customer performance features can be considered, so all customers are supported to delay sensitive recovery in an unusual environment.We also show that their features can be used to maximize server input under server memory restrictions.
In the next few decades, we presented the COUGAR system, a new distributed data management infrastructure, with the growth of the sensor interconnection and computing capacity, our system is located directly on the sensor nodes and creates an abstract of a single processing nodes without having to concentrate data or calculate.
Data exchange formats were originally intended to move data between the programs and in a platform independent file format between the researchers group. They are mainly self-described, containing data elements definitions and databases data, although in some cases they involve a standardized external data dictionary. DXS allows the exchange of data structures between the programs, not just bitstream. They tend to not support a behavior component as part of the exchange format, although some are claimed and derived data elements. They are usually as a program library, it is connected to an application, and some separate uses, the interesting phenomenon is that DXS is used for data management, but in some cases they are originally used for the exchange of data using these group formats rather than their unique flow.
This paper takes the following logical step: it considers to catch the time and effective time in the background of the transaction. The paper initially identifies and analyses several issues, using simple time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time
SilkRoute combines an application query with a public view query, translates the results into SQL, performs it on the relationship engine, and integrates the result’s double stream into the XML query file. This work makes some key contribution to the XML query processing. First, it describes an algorithm that translates the XQuery expression into SQL.
Based on this, we proposed a new index structure, called the TPR tree, through a set of improved building algorithms, taking into account the unique characteristics of the dynamic objects, we provide a cost model to determine the best performance achieved by any data zone space time access method.
This article presents the three seminar reports organized by Brian Cooper, for the new editing of the seminar reports and technical notes.The first article summarizes the activities and discussions of the EDBT Summer School on XML and the database, contributed by Riccardo Torlone and Paul Atzeni.The second article by Ioana Manolescu and Tannis Papakonstantinou, provides an overview of the XQuery implementation, experience and prospect seminar, held this year in Paris, in combination with the ACM SIGMOD meeting.
We introduced a new framework and tool (ToMAS) to automatically adjust the map because of the chart development. Our methods not only take into account local changes in a chart, but also take into account the changes that may affect and convert many chart components. We take into account a comprehensive chart category relative and XML chart with the choice of type and (custom) limitations. Our algorithm detects the map being influenced by the change of structure or limitations, and produces all rewrite, in line with the chart chart. Our methods clearly model chart options made by a user and keep these options, where possible, chart and chart development. We describe the implementation of a chart management tool based on these chart chart chart chart chart chart chart chart chart chart chart chart chart ch
To support effective similarity search in NDDS, we presented a new dynamic indexing technology called ND Tree. The key idea is to expand the relevant geological concepts and some indexing strategies used in CDS to NDDS. Providing effective algorithms for building ND Tree. Our experimental results on synthetic and genome sequence data show that ND Tree’s performance is much better than linear scanning and M Tree’s performance in high NDDS.
For years, TIBCO (Information Bus Company) has been the pioneer of using Publish/Subscribe – a push technology – to build a flexible real-time and smooth allocation application. Today, Publish/Subscribe is used by the world’s top 300 financial institutions, deployed in 6 top 10 semiconductor manufacturers’ factory buildings for implementing large-scale internet services such as Yahoo, Intuit and ETRADE, and selected by many world-leading companies as a business infrastructure to integrate different applications.
Question optimizers are now looking for many sources of information about the database to optimize queries, they use operating time statistics in cost-based queries estimates, they use integrity limitations in queries rewrite, mainly and external critical limitations have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost forecasts.
The main component of MIND is a global query processor, a global trading manager, a program integrator, an interface-supported database system and a user graphic interface. In MIND, all local databases are embedded into a general database object, with a precise single interface. This method hides the difference between the local database, from the other parts of the system. The integration of the export program is currently manually carried out using an object-defined language (ODL) based on the interface-defined language.
There are several main storage access structures, such as dividing trees, and four trees, their characteristics, such as good bad behavior, make them attractive database applications. Unfortunately, the number of disk pages passed through the route is usually "long and thin", while the disk data structure must be "short fat (i.e., with high tone and low height) to try to reduce I/O. We consider how to divide the nodes (i.e., the map nodes to the disk page) of the main storage access structure, for example, although the route can pass through many nodes, it can only pass through a few disk pages.
The paper describes an advanced development program to create a medical information system called the National Medical Knowledge Bank (NMKB). This five-year program is partly sponsored by the National Standards Institute and the Advanced Technology Program. The program’s objectives, covering computer-assisted diagnosis, medical training, remote consultancy and medical record storage, are defined.
In this special issue on data management, we introduce new work on the creation, collection, management and understanding of data, which emphasizes the reality of lack of data and effective management technologies, which is currently one of the biggest challenges for meaningful use and sharing of existing data (or should be said to be Grot).
The general theme of the FQAS Conference is innovative query systems designed to provide easy, flexible and intuitive access to information. These systems are designed to promote access to information from databases, libraries and the World Wide Web. These storage systems are usually equipped with standard query systems that are often inappropriate, with FQAS focus on developing more expressive, informative, collaborative and productive query systems.
The paper proposed a system of personalized web portals. a specific implementation is the discussion of a reference to a web portal containing a news transmission service. the technology is proposed for effective classification, management, and personalized news transmission from a live news line service. the process is made by two steps: first manual input is necessary to create domain name knowledge, which can be specific to the site; then the automatic component uses this domain name knowledge to perform personalized, classified and presented.
The increase in XML encrypted data exchanged through in-ternet increases the importance of XML-based publishing subscriptions (pub-sub) and content-based routing systems.
The paper describes a method: an open geological data interface (OGDI), allowing application software to access various space data products. discuss OGDI methods compared with other standard efforts, and describes the characteristics and use of OGDI, which is in the public field.
In this article about Levin, we explore the topic of the dedication of the local church to commemorate the heritage of Paul and Cae Watson.The following thought and paper was written by those who witnessed this faithful service to Christians.Paul and Cae have dedicated time, love and spiritual gifts to Durham, North Carolina.
In this article, we presented new technologies to SVD-based scale reduction in a dynamic database.When the data distribution changes significantly to reduce the accuracy of the query, we re-calculate the SVD transformation and incorporate it into the existing index structure.To re-calculate the SVD transformation, we proposed a new technology using the integrated data from the existing index rather than the whole data.This technology reduces the SVD calculation time without damaging the query accuracy.We then explore effective ways to re-calculate the SVD transformation into the existing index structure.
This sample proposal describes Amit, a intermediate software framework that solves a major problem in this field: the gap between the events that occur in various channels, as well as the real circumstances that the user needs to react; the reaction situation. These circumstances are the composition of the event or other circumstances (e.g., "when four similar events occur") or the content filter event (e.g., "only the event related to the IBM stock") or both ("when four times the purchase of more than 50,000 shares on the IBM stock takes place a week").
Oracle Corporation is the world’s second-largest software company and the leading supplier of corporate information management software. The company has two major companies, one providing the lowest cost information technology infrastructure and the other providing business and competitive advantage through high-value applications, Oracle is one of the first software companies to implement corporate software management models through network computing, and the first large software company to provide full products on the Internet in an electronic way.
Our algorithm has the following characteristics: (1) it only requires one through data; (2) it is defined; (3) it produces the correct low and upper real value of the quantum; (4) it does not require a primary knowledge of the distribution of the data set; (5) it has an extended parallel format; (6) the additional time and memory calculates the additional quantum (except the first) is a continuous quantum.
The morality of the editor is the requirements of the writer, editor and reader in the academic communication on the basis of the identification of the academic publication environmental values.
We introduced a new framework of any combination of XML and relationship charts, one of which is high-level, user-specific charts translated into source data to target charts with meaningful queries. Our methods work in two stages. In the first stage, high-level charts, expressed as a combination of intercharts match, converted into a combination of charts, captured design choices, in the source and target charts (including their sequential organization and their definition of reference charts). The second stage converts these charts into charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts charts
Metadata describes complex works, which are the topics of formal design activities, such as business processes, application interfaces, database (DB) programs, engineering drawings, software configurations, and documentary libraries, their demand is growing by corporate reengineering, integrated CASE, database, network management systems, computer systems, information resources, files, websites, etc., which is hard 10 measuring product revenues, because the storage library is often embedded in other products, but it may have been a billion dollars of annual business.
From the early activities of the International Institute of Biology (IIE) established in 1911, we reviewed the historic progress of the development of the biological society, especially emphasizing the latest Constitution of the Spanish Biological Society (SEBD) established in 1994, as well as the historic progress of the Portuguese Biological Society (SPBD) established in 2006.
In this paper, we provide an overview of the latest SQL standards SQL:1999 and show how large are the different concepts and language structures proposed in the big business (object)-related database management systems.
Four different indicator exchange techniques allow the object exchange to be investigated and compared with the performance of the object administrator using no indicator exchange.A wide quality and quantitative assessment - of which only a part can be presented in this article - indicates that there is no superior indicator exchange strategy for all application profiles.Therefore, an adaptable object base operating time system is considered to use the entire range of indicator exchange strategies, depending on the characteristics of the application profile, which are determined by monitoring, for example, combined with sample, user specifications and/or program analysis.
Starting in December 2000, as a small prototype designed to test the XQuery static system, Garux has now become a solid implementation designed to fully match the XQuery 1.0 specification of the family.
We presented a new concept of amazing time patterns on the market. stock data, and algorithms find such patterns, style. it is different from finding frequent patterns, such as discussing in general mining literature. we think. once the analyst.
FFT is used to calculate the cross-range relationship between the forecast sequence (the already reached value) and the database mode, and to obtain the distance between the forecast time sequence in many future time locations and the database modes.When the actual data value arrives, the forecast error is used together with the forecast distance to filter modes that cannot be approaching or approaching neighbors, providing a quick response.
We introduce and study a new query category, we call OPAC (Optimization under Parameters Integration Limit) query. These queries are designed to identify databases integrated component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component component
Aditi is a client-based server model drive system; it is a multi-user essence and is able to take advantage of the parallelity of multi-processor memory. The background uses the relative technology in managing the efficiency of the data based on a disk and uses the optimized algorithm specially developed for the underlying evaluation of logical queries involving the return. The front end interacts with the user in a logical language, with more expressive strength than the relative query language. We introduce the structure of Aditi, discussing its components in certain details and the performance of the current state.
As a primary relationship operation, one of the main points to implement sample is sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample
In this article, we introduce FeedbackBypass, a new method for interactive similarity query processing. it adds the role of the relevant feedback engine, by storing andining with feedback path determined query parameters, using wave-based data structure (Simplex Tree). for each query, a favourable query parameter can be determined and used, either "transition" the seen query feedback path completely, or starting the search process from a near-optimized configuration.
In the past few years, the number of interactive multimedia presentations prepared by different individuals and organizations has increased significantly. In this article, we introduce the algorithms of query multimedia presentations in a database. Unlike the relative algorithms, the algorithms of interactive multimedia presentations must run on the tree, its branches reflect the different possible presentations of the family. Query language supports the choice of type of operation to find the object and the demonstration path, the user is interested, adds the type of operation, combines the presentation from multiple databases to a demonstration document, and ultimately set theoretical operation to compare different databases.
In this article, we consider some different possible options in an active DBMS behavior, based on a broad analysis of some of the most familiar implementation systems and prototypes. We coded these options in a user-readable form, called extended ECA. A rule from any existing system can be rewrite in this formatting, making all the series options clear. Then an ECA rule can be automatically translated into an internal (non-readable) format, based on a logical style, called the core format: the execution of the series of the series is specified as a simple conversion involving the core rule fixed point as an important prerequisite, this study, a series of databases and transactions are also created without a state, including data and events.
The contribution of the project is three times: (1) through the hash method for the effective production of large project groups (2) on the divide method for the effective reduction of the project group scanning required (3) on the reduction of the amount required for the database scanning (3) we suggested the hash and divide based on technology, HD algorithm, for the candidate for the generation of large project groups which is produced by HD by the number of candidates less than many methods, such as the Apriori algorithm, the DHP algorithm and the DIC algorithm, according to our simulation results, the recommended method is more effective than any existing algorithm.
Earth Observation (EO) and simulation data share some core characteristics: they are similar to a certain space time size Luster data; complete objects are very large, well in the capacity of Tera and Petabyte; data is generated and obtained following very different access modes.
The structure we consider is a line representative, the distance measurement is a line editing distance, allowing the length of the variable not to worry. Our technology includes line matching algorithms and new fonts, used to detect and optimize, most of which are generalized to other combination structures.
PBSM algorithms will enter into managed fragments and combine them using calculated geology-based flight sliding technology. This paper also introduces a performance study, comparing the traditional index of the mucous membrane combination algorithm, a space combination algorithm based on the space index, as well as the PBSM algorithm. These comparisons are based on these algorithms in heaven's complete implementation, a database system to process GIS applications. using real data sets, performance research examines the behavior of these space combination algorithms, in a variety of cases, including when two, one or no input to the appropriate index.
In addition, the graphic process simulation tool should support the definition of all these features. an important aspect of this simulation tool is the transparent integration of the process management environment. in addition to the serial composition of the transaction, several other operating times are circular. usually, several series of the same network services are available in different places. a process performing infrastructure should be equally distributed in all network service providers.
In this paper, we review and compare the existing algorithms of high-size data classification and show the impact of scale fraud on its e ectiveness and e science, we developed a new classification technology called OptiGrid based on data the best network classification method (such as BIRCH or STING) is the most prospective candidate for achieving the required e science, but it also shows that essentially all data-based methods have serious weaknesses, compared to its e ectiveness in high-size space, it shows a very serious weakness in a large-size CAD data to overcome these problems, we developed a new classification technology called OptiGrid based on the best network classification method.
The database field is one of the fields of computer science, these fields are very directly driven by application requirements; this is now real in three ways: first, users want more application specific support from the database and want DBMS to have more grammar application knowledge. second, users want the database to support new applications, these applications sometimes away from traditional database applications and introduce new requirements, as well as the need for double integration of the database technology with other advanced technologies (such as the neuron network) in one application.
In the past few years, work has been shown that waves can be effectively used to compress a large amount of data and provide quick and quite accurate answers to queries, rarely emphasizing the accuracy of the use of waves in close to data sets containing multiple measurements. The existing decomposition method will either run separately on each measurement, or process all the measurements as the values of the vectors, and at the same time process them. In this article, we show that the results of the individual or combination of the storage method is the different measurement factors of the waves, from these existing algorithms can lead to low-optimized storage use, thereby reducing the accuracy of the queries. In order to alleviate this problem, we will introduce this work without extended waves as a real value value value
It distributes business model computing in many query blocks, making the application coding in SQL difficult to develop. The limitations of RDBMS have filled the distribution tables and specialized MOLAP engines, which are good formulas of mathematical models, but lack of forms of relative models, difficult to manage and display scale issues. This presentation introduces an extended, mathematical rigorous, and performance of SQL expansion relative business model, known as SQL distribution tables. We introduce the typical business model computing with SQL distribution tables and compare them with the standard of SQL display performance advantages and programming convenience.
After revealing the most advanced state algorithms to the strong performance defects of the nearest neighbor search (Korn et al. 1996) we introduced a new multi-step algorithm that guarantees the minimum number of candidates.
In this paper, we presented a new idea to create a reduced table of data. The method is naturally able to very effectively estimate each point of the local indication size, thus creating a variable size reduced table of data. This technology has the advantages that it is very suitable to adjust its table, depending on the data point’s immediate location of behavior. A interesting feature that table’s table technology is, different from all other data reduced technology, the overall efficiency of compression improves, with the data base’s increase in size. This is a very meaningful feature, because the problem itself is by the data set’s large-scale engine. Because its sample method, this fast-scale process is very suitable, depending on the data point’s location, there is a behavior characteristic of the table, the technical characteristic of the table
In a federal component database system, the functional base methods and mechanisms are described.On the background of a functional object database model, a support unit of information and behavior sharing technology is proposed.The implementation of the functional base sharing mechanisms is described, its basic algorithms are described, its practicality and effectiveness are evaluated.
Lotus Notes is a business product that allows individuals and organizations to collaborate and share information.Notes enables easy development of applications such as communication, document management, workflow and non-synchronous meetings.Notes applications can be deployed across the globe, in independent organizations, in an unparalleled connected computer network from small laptops to large multiprocessor systems.
Recently, the use of XML files, some of which data is clear, while other parts are defined by the program that produces the relevant data, began to gain popularity. This chapter involves such files as the intensity of the file. Materialization is the process of quoting some of the programs in the XML files and replacing them. This sample aims to support these intensity of the XML files in the exchange between the applications and describe the new possibilities and they bring the application design with enormous flexibility.
Remote backup is a copy of the main database, retained in a geographically separate location and used to increase data availability. Remote backup systems are usually based on logs, which can be divided into 2 security and 1 security, depending on whether the transaction is carried out simultaneously on the two websites, or first on the original, and then spread to the backup. We built a experimental database system, we evaluated the performance of the time and depended on rebuilding algorithms, we developed two 1 security algorithms. We compared 1 security and 2 security methods under different conditions.
We extend this work by considering how to use these languages in practice. in particular, we take into account a limited category of higher order views and show the strength of these views in the integrated heritage structure. our results provide an insight into the characteristics of reorganization that requires solutions for differences. in addition, we show how using these views allows charts to browse and new forms of data independence, which is crucial for the global information system. in addition, these views provide a framework for integrating semi-structured and non-structured queries, such as keyword search, in a structured query environment.
We have proposed a new category of algorithms that can be used to accelerate the execution of multi-way joining queries or involve one or more joining and one composite queries. These new evaluation techniques allow to perform multiple hash-based operations in one step without allocating intermediate results. These techniques work especially well to join the level structure, for example, to evaluate the functional connection chain along the key / foreign key relationship.
TPC-DS is a new decision support standard that is currently being developed by the Transaction Processing Performance Committee (TPC).The paper provides a brief overview of the new standards.The reference model supports decision-making features of retail product suppliers, including data loading, multiple types of queries and data maintenance.The database consists of multiple snowflakes with shared size tables; the data is scanned; the queries are large.
DataJoiner (DJ) is an unusual database system that provides a single database image of multiple databases. it is accessible through the user’s defined alliance (Nick name) to the tables of the remote database. DJ is also a fully functional relationship database system. DataJoiner query optimizer some of the outstanding features are: (1) query submitted to dataJoiner optimizes using a cost model, taking into account the ability of the remote query optimizer in addition to the ability to process remote query and (2) if the remote database system lacks some functionality (e.g., classification), dataJoiner compensates.
Supporting delayed connection and reversal function system must be able to solve (y)-.x for a given x and unknown y whenfn is delayed connection, that is, the solution (execution function name) applies to y is based on the type of choice y. This combination of delayed connection and reversal function call requires new query processing capacity to fully exploit the indicator mentioned delayed connection function call. This article presents a method to manage delayed connection in query processing.
Assume that each object in the database has a m grade, or a score, each m property one. For example, an object can have a color grade, which describes how red it is, and a shaped grade, which describes how circular it is. For each property, there is a classification list that lists each object and its grade under that property, classified as a grade (highest grade first). Each object is allocated to a total grade, which is by combining the attribute grade using a fixed unit integration function, or combining rules, such as min or average. In this review, we discuss and compare algorithms to determine the top k object, i.e. the object with the highest level etc.
This sample shows several advanced use cases of location-based services and shows how these use cases are provided through a medium media space information, the Nexus platform. This scene shows a mobile user can access the location-based information through the so-called virtual information tower, record space events, send and receive geographical messages or by showing other mobile users to find her friends.
In this article, we introduce the first increased classification algorithm. Our algorithm is based on the classification algorithm DBSCAN applies to any database containing data from a measurement space, for example, to a space database or WW log database. Due to the density of the DBSCAN base nature, insert or delete an object affects the current classification only near this object.
In theoretical terms, we prove that there is a high probability that it produces a result, which is a ( + ε) factor close to the closest Eclipse neighbor. In practice, it is obviously very effective, often exploring not more than 5% of the data to get very high quality results. This method is also database friendly, in which it accesses the data mainly in the scheduled order, no random access, and, with other methods of close to the closest neighbor, almost no need for additional storage.
In this article, we introduce the concept of expanding characteristics of objects to obtain similarity. The conventional method for finding similarity in the database maps of each object in the database is a point in some high-size characteristics space, and defining similarity as a certain distance measurement in this space. For many similarity search issues, this characteristic is based on method is not enough. When searching for a part of similar polymer, for example, search cannot be limited to the marginal sequence, because similar polymer part can start and end in any place of the marginal polymer.
The service component is getting power as a potential silver forecast of the Semantic Web. It is committed to taking the network to unexplored efficiency and provides a flexible method to promote all kinds of activities in the Web of tomorrow. The application is expected to make a large use of the Web service component including B2B e-commerce and e-government. So far, allowing the synthetic service has been largely an ad hoc, time-consuming, and the wrong process, involving repeated low-level programming. In this article, we propose a ontology-based framework to automatically compose the Web service. We introduce a technology to produce the synthetic service from a high-level statement described. We define a formal guarantee, by using the synthetic meaning of these rules and rules of compliance, and the process involves a low
The popularity and availability of smart sensors, such as network cameras, microphones, etc., creates an exciting new category of distribution services. While these sensors are cheap and easy to deploy in a wide area, achieving useful services needs to address some challenges such as preventing big data from transmitting on the network, effectively detecting the relevant data in the distributed sensors collection, and delivering it to interested participants, and effectively processing static data information, live sensors transmission, and history data.
The range query applies to integrated operations on all selected cells in the OLAP database, where the choice is determined by providing a sequence of values for the digital dimension.We provide fast range query algorithms for two types of integrated operations: SUM and MAX. These two operations cover the technology required for the most popular integrated operations, such as operations supported by SQL.
We develop algorithms to extract detailed information about the query program through a narrow optimizer interface and to characterize the database statistics published using the TPC-H standards and a wide range of storage parameters.We show that when data structures such as tables, indicators and classifications operate on different storage devices, optimizers can get significant benefits from accurate and timely storage devices accessing cost information.
In this article, we describe an effective non-locked rearrangement mechanism that can be transmitted through voluntary data flows from files, indicators and continuous data. We also study several policies of rearrangement based on the performance targets of a variety of typical applications. We introduce the results of the implementation used in the Informix dynamic server with universal data options, as well as in the classification and scanning in a large-scale distribution table. Our experiments show that for a variety of data distribution and applications, rearrangement is a response to dynamic preference changes, the minimum overweight applied in the total completion time, and provides a significant improvement in the quality of time feedback.
In particular, the answer space began a super final answer, and it was subtle, because the user’s control of the relative speed from the internal query blocks is subtle. For the intermediate query blocks is meaningful, they must be explained by the collective from the internal query blocks. We also proposed a multi-line model in evaluating such query blocks: each query blocks are allocated to a line, while the wireless query can be simultaneously and independently evaluating the time.
In this article, we introduce an extended relationship algorithm that has universal or quantitative classes as properties. the proposed extension can significantly improve the expression capacity of the relationship system and significantly reduce the size of the database to a small additional calculation cost. We also show how the proposed extension can be built on a standard relationship database system top.
In this environment, access to relevant and accurate information is increasingly complex by the characteristics of widespread distribution, independent, diversified and dynamic sources of information, and this complexity is aggravated by these potential global, interdisciplinary, multicultural and rich media technologies’ continuous development of systems, academic and structural abnormalities.
The λ-DB OQL compiler is a C++ pre-processor, receiving a language called λ-OQL, which is C++ code with built-in DML commands for transactions, queries, updates, etc. The pre-processor translates the λ-OQL program into C++ code, containing the call to the λ-DB evaluation engine. We also offer a visual query format interface called VOODOO, and a translator from visual query to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation.
This article introduces a series of interfaces developed by Microsoft, the aim of which is to allow applications equally access to data stored in DBMS and non-DBMS information containers. The application will be able to take advantage of the database technology without having to transfer from its source to DBMS. Our method is to define an open, extensible interface set, which includes the correct, repeatable part of the DBMS function. These interfaces define the boundaries of DBMS components, such as recording containers, query processors, and transaction coordinators, allowing equally, transaction data access to these components.
In the past decade, some techniques for calculating the merger between two spatial data sets have been proposed, some of which methods take into account the existing index of the merger input, while other processing data sets have no index, providing a solution in the case of at least one input is the intermediate result of another database operator. In this paper, we analyzed the previous work on spatial merger and proposed a new algorithm known as spatial merger (SISJ), effectively calculating the spatial merger between two input, only one of which is indexed by a R tree.
Our assumption is that automatically extracted keywords from the file can support this goal. We a user study that compared the user’s results to a precise classification of the file on a small screen when the file extracted is composed only of keywords or file title. We did not find a significant performance difference between the two conditions.
We are studying how to apply the hash merger mode to space merger and define a new framework for space merger. Our space division function has two components: a set of external data sets divisions and a division function that can map the data project to multiple divisions. In addition, the division function for two input data sets may be different. We have designed and tested a space hash merger method based on this framework.
We propose a file structure to index high-size data, which are usually pointed in some functional space. The idea is to use only a few functions, using additional functions only when additional discrimination is absolutely necessary. We detail our tree structure design and related algorithms, dealing with such "different length" function vectors.
When the user requires to update a view, the relationship stored in the database may not have a unique way to the required updates. Choose one of the alternatives to update the relationship stored may not reflect the changes that occur in the real world; In the presence of other derived view, the database may actually show the user a very wrong world model. The problem is even more serious. To avoid this problem, we introduce a new view update, called accumulated updates. The key idea behind accumulated updates is that the update mechanism should wait for further update requests to solve double problems.
In its 17th edition, SBBD2002 contains 21 optional theses, 4 curricula (invitation and submission), 5 small courses and 3 invitation conversations. It gathers more than 450 participants between researchers, students and interns who participate in research issues related to the main topics of the modern database field. As a major national forum for discussion and research results, developed in Brazil and abroad, SBBD hasined the collaborative status of ACM-SIGMOD since 1998.
SIGMOD has had a super vision over the past few years that allows all information in the database field to be free to access. Of course, this goal goes into the economic reality of scientific publication. Therefore, the SIGMOD Executive Committee has adjusted its initial objectives to the following strategy, emphasizing free CDROM availability first, then online availability, and then lower subscription prices.
From client servers to multi-layer computing movement created a series of so-called intermediate software systems, including application servers, workflow products, AI systems, ETL systems and federal data systems. In this paper, we believe that the explosion of intermediate software created countless incomplete systems with superfunction.
Today’s data security issues are far beyond the traditional RDBMS authorisation/abrogation issues and we’ll discuss what the new research agenda should be.
In this article, we describe the Oracle large user population demonstration and emphasize the extensible mechanism of the Oracle8 universal data server, which allows it to support up to 50,000 users on an Oracle8 database without any medium-level TP monitoring software. Supporting such a large user population requires many mechanisms of high competitiveness and breakthroughness.
In this article, we introduce a mechanism of approximately translation of Brian queries restrictions in various sources of information. achieving the best translation is challenging because the source supports different restrictions, submitting queries, and often these restrictions cannot be translated accurately. for example, a query [pointing > 8] may be "perfect" translated to [pointing > 0.8] on one site, but only close to [pointing = A] on the other.
There is a wide range of programming languages that can express the logic defined by the user. The main advantage of this method is that the workflow logic is kept directly in the location of the workflow data, leading to a more efficient, simpler and more compact system design. It also helps to integrate the workflow of the database center into a larger framework application as DBMS is part of all the business applications.
In this "deep network", we observe two differential characteristics, providing a new perspective, considering program matching: first, as a network size, there is a wide range of sources, providing structured information in the same fields (e.g., books and cars). second, while the source plants, their integrated program dictionary tends to match in relatively smaller sizes. Motivation of these observations, we propose a new paradigm, statistics matching: with the traditional methods using double characteristics matching, we take a comprehensive approach to match all input program by finding a model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model
In order to provide high availability of continuous media (CM) data, the CM server usually cuts data on multiple disks. Currently, the most widely used cutting system of CM data is round machine conversion (RRP). Unfortunately, when the RRP applies to variable bit (VBR) CM data, the load occurs on multiple disks, thereby reducing the system performance. In this article, the performance of the RRP's VBRCM server is analyzed. In addition, we also propose an effective cutting system called continuous time conversion (CTP), which will take into account the VBR characteristics and get more balanced load than the RRP.
To solve this problem, we have developed Cache Investment – a new way to integrate query optimization and data configuration, which looks beyond a query performance.Cache Investment sometimes deliberately creates a “low-optimized” program for a particular query to better data configuration after query benefits.Cache Investment can be integrated into a distributed database system without changing the interior of the query optimizer.In this article, we present Cache Investment mechanisms and policies and analyze its performance.
The latest, as well as the topic of this comment, is a set of notes published by the Canadian Society of Minerology, written for a short course of PGE exploration that was recently held at the PGE seminar in Ulu, Finland.
Hello everyone, I hope everyone enjoys your summer because our idea is now turning down, and it brings all the miracles.
Continuous questions about the data flow may be blocked and/or unrelated to waiting, which may delay the answer until some related entries arrive through the data flow. These delays may convert the answers when they arrive, to users who sometimes have to make any help decisions is old. Therefore, it can be useful to provide the assumption answers - “to provide the current information that may be X will become the real moment” - rather than any information.
As an object-oriented model becomes a trend in the database technology, it requires a relative conversion to an object-oriented database system to improve productivity and flexibility. conversion includes graph translation, data conversion and program conversion. This paper describes the method of integrating graph translation and data conversion. graph conversion involves object conversion and graph conversion to object-oriented graph. data conversion involves the double conversion of the relationship into a sequential file and the conversion of it into a object-oriented class file.
In this article, we introduce a new limitation of integrity, which we call the statistical limitation, and discuss its applicability to improve the accuracy of the database. The statistical limitation shows the internal relationship between the current properties in the database and is characterized by its probability nature. They can be used to detect possible errors that are not easily found by the traditional limitation. The methods to extract the statistical limitation from the relationship and implement these limitations are described.
While the number of database management systems (DBMS) increases, and various DBMS become more and more complex, there is no unified method to build DBMS. Therefore, developers are forced to start more or less destroying each system, resulting in time, effort and cost. Therefore, the database community is challenged by developing a suitable method, that is, time-saving application engineering principles (e.g., repeated use).
In this paper, we study the effective method of calculating the glacier circle, using some commonly used complex measurements, such as average, and develop a method, adopting a weaker but resistant state to test and spread search space. In particular, to effectively calculate the average measurement of the glacier circle, we proposed a top average spread method and expanded two previous research methods, Apriori and BUC, to top Apriori and top BUC. In order to further improve performance, an interesting high blood pressure structure, known as H tree, was designed and developed a new glacier circle method, known as top H-Cubing.
Our solution codes the route as a line and inserts these lines into a special index that is very optimized for long and complex keys.We describe the index sheet, providing the efficiency and flexibility we need for the index structure.We discuss how the "question route" is used to optimize the ad hoc query of semi-structure data, and how the "clean route" optimizes the specific access route.Though we can use the knowledge of the query and data structure to create the clean route, no such knowledge is necessary for the original route.
We proposed a mathematical definition of the concept of the best projection group, from the natural requirements of the point density in the subspace. This enables us to develop a Mont Carlo algorithm to calculate the projection group. We prove that the calculation group is good, there is a high probability. We implement a modified version of the algorithm, using weight calculation to accelerate calculation. Our wide range of experiments show that our method is more accurate than the previous method.
In fact, the database related materials are included in the various courses proposed by international organizations and famous universities. However, it requires a systematic database knowledge institution (DBBOK), similar to other work in software engineering (SWEBOK) or project management (PMBOK).
The recent work of data integration has shown the importance of statistical information on covering and covering sources for effective query processing. Despite this recognition, there is no effective way to learn the necessary statistics. In this paper, we introduce StatMiner, a system to estimate covering and covering statistics whileining the necessary statistics in close control. StatMiner uses a level of classification query and is based on the boundaries of data mining technology for variables to dynamically determine the level of resolution, where to learn statistics. We will show StatMiner's main features and study the effectiveness of statistics in BibFinder, a publicly available computer science book, we develop resources.
The tutorial "Software as a Service: ASP and ASP Integrated" will provide the customer (abonnent) with an introduction and overview of the concept of "leasing" software access.An Application Service Provider (ASP) is an enterprise hosting one or more applications and providing access to the subscribers through the Internet through the browser technology.
DBMS is widely used and successfully applied to a wide range of applications. however, trading patterns, which are based on variations such as mobile trading and workflow systems, sustainability and accuracy are not suitable for many modern applications.
The summer school curriculum on the new boundaries of data mining was submitted under the sponsorship of the DIMACS Theoretical Computer Science Center of the University of Rutgers. It took place in the DIMACS Center of Rutgers from August 13, 2001 to August 17, 2001. Dimitrios Gunopulos and Nick Koudas were the organizers of the curriculum, which included a large number of invited speakers and speakers.
Recently, technological advances have led to the widespread availability of commercial products, based on robots, third-party storage libraries. therefore, these libraries have become a key component of modern large-scale storage servers, taking into account the very large storage requirements of modern applications. despite the topics of the best data configuration (ODP) strategy have been obtained to other storage devices (such as magnetic and optical disks and disks), the best data configuration issues in third-party libraries have been ignored.
In this article, we present a unified framework that can implement multiple access control policies within a system. The framework is based on a language that can be specified by the user to apply to specific access. The language allows to determine the positive and negative licenses and includes the concepts of licensed derivatives, conflict resolution and decision-making strategies. Different strategies can be applied to different users, groups, objects, or roles, based on the security policy needs. The overall result is a flexible and powerful but simple framework that can easily catch many traditional access control policies and protection requirements, existing in the real world of applications, but we rarely support the existing system.
In CS, the server manages the disk version of the database. After the client gets the database page from the server, it is stored in their bubble pool. The client is updated on the storage page and creates log records. The log records are stored locally in the virtual storage and then sent to the single log on the server. ARIES/CSA supports the proper recovery in the client server (CS) architecture. In CS, the server manages the disk version of the database. After the client gets the database page from the server, it is stored in their bubble pool.
Relative database systems do not support complex queries that include quantity (quantitative queries), these queries are increasingly important in decision-making support applications. Generalized queries provide an effective way to express these queries in nature. In this article, we consider processing quantitative queries in the generalized queries framework. We prove the current relationship system is inappropriate, both language and queries level processing, processing these queries. We also provide understanding of the internal difficulties related to processing these queries. We then describe a quantitative queries processor, Q2P, which is based on the multi-dimensional and graphic structure implementation.
Database support multi-dimensional sequence is an increasingly important field; a variety of high-capacity applications, such as space time data management and statistics / OLAP becomes the focus of academic and market interests. RasDaMan is a domain-independent sequence database management system, based on server queries optimization and evaluation. the system is fully operated and used in international projects. We will show space time recycling using rView visual queries client. examples will include 1-D time sequence, 2-D image, 3-D and 4-D audio data.
In this meeting, each participant has 10 minutes of time to send a short message using only one slide.The researchers were asked to answer this question: "What do you think is the most urgent, unresolved problem in oral lie testing?"
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
Many applications calculate integrated functions (such as COUNT, SUM) beyond a specific point (or integrated specific point) to find integrated values beyond a certain specified range. We call it such an ice search, because the number of results of the above range is often small (the ice point), compared to a large amount of imported data (the ice). Such ice search is common to many applications, including data storage, information recycling, stock analysis in data mining, classification and duplication findings.
The goal of Paradise project is to apply objective-oriented and parallel databases technologies to accomplish tasks that are capable of managing large (DotaBat) data sets of parallel GIS systems, such as the data sets that will be produced by NASA’s EOSDIS project (Car92).
Soon, the world will need a bigger database, then we all imagine; however, ironically, no too much care, VLDB, as we know today, can remain on the path. The way we think, design and building a huge database will have to completely change if we are going to be involved in this revolution. So far, everyone, including the database person, realizes that the computer world is experiencing one, but two revolution changes. First, of course, there is who imitates personal computers, cargo hardware and increasing speed and ability.
In this article, we introduce a method to automatically divide unformated text records into structured elements. Today, many useful data sources are based on human texts, while convenient use requires data as structured records to be organized. The main motivation is the problem of storage address cleaning, which will be stored in a large enterprise database as a single text field to fields like "city" and "street".
A matrix composed of waterless beta-1,3-glucan Gel in a diameter of about 5 to 1000 mm passes, for example, in waterless beta-1,3-glucan alkaline water solution disperses in waterless organic solvent, and adds organic acid to the produced dispersion.
With the add-on of the bits, which allows us to solve a common basic issue of text access: considering an object relative to the table T column represents the file, with a collection of the type of column K represents the keyword, we show an effective algorithm to find the file, with some query lists of the term share the greatest number. on these issues publishing work many exist in the information access (IR) field. We introduce the algorithm, we call the bits term match, or BSTM, using a comparative performance method to the most effective known IR algorithm, the current DBMS text search algorithm has a significant improvement, its advantage is only the number we provide the database operations.
Our three British guests (Mark Solms, Peter Fonagy, and Mary Target) presented informative and challenging papers, as well as Peter Blos's discussions on Solms papers, as well as the subsequent discussion groups, presented many important issues about the mental and physical relationships of children.
HYPERQUERY is an object-oriented image base system’s supertext query language, first we discuss the term-based object calculation and then the example query is used to describe the language facility, this query language has a similar taste to QBE, as the object-oriented image base management system OISDBS’s high non-programming and dialogue language.
This article introduces BUCKY, a new reference mark for the object relationship database system.BUCKY is a query-oriented reference mark that tests many key features provided by the object relationship system, including serial types and heritage, reference and route expressions, atomic values and reference collections, methods and delays linking, as well as the user defines abstract data types and methods.To test the maturity of the object relationship technology, we provide the object relationship technology with the same object and the same object with the same object with the same object with the same object with the same object with the same object with the same object with the same object with the same object with the same object.
Based on this probability model, we have developed three predictive algorithms based on practical theory to predict how users will interact with the presentation document. These predictive algorithms allow for effective visualization of the query results based on the basic specifications. We have built a prototype system containing these algorithms.
However, for non-traditional applications, such as large-scale unstructured data transfer to a structured data, or in interdisciplinary fields (e.g., in environmental sciences), existing ETL (extract conversion load) and data cleaning tools write data cleaning program is not enough.Their main challenge is to design a data flow map, effectively produce clean data and be able to effectively perform big data input.Their difficulties arise from (i) a clear separation of specific data and the implementation of substantial data conversion (e.g., in the environment), existing ETL (extract conversion load) and data cleaning tools are not sufficient to write data results.
Browsing ANd Keyword Searching (BANKS) allows almost unable Web to publish relative and eXtensible Markup Language (XML) data, otherwise it will leave (at least part) invisible web pages. The relative database stores a lot of data, requires the use of structured query language. The user needs to know the basic and query language so that the data can be made meaningful ad hoc query. This is a significant obstacle for the random users, such as the web information system users. HTML tables can be provided for the forecast query. The university website can provide a format query interface for the college and students. The query department also needs another format, because it will provide query courses. But for each task, creating a format and query language, this is also a significant obstacle for the user.
In this article, we introduce a waste collection algorithm, known as transaction cycle reference calculation (TCRC), to an object-oriented database. The algorithm is based on a variable of reference calculation algorithm proposed by the functional programming language the algorithm keeps tracking assistant reference calculation information to detect and collect cycle waste.
However, some XML compressors do not support query compressed data, while other XML compressors support query compressed data blindly coded labels and data values, using predefined coding methods. Therefore, the query performance for compressed XML data decreases. In this article, we propose XPRESS, a XML compressor that supports the direct and efficient assessment of query for compressed XML data.
While supporting fast, ad hoc query processing big data warehouse has caused recent introduction of many new index structures, there are several obvious exceptions (i.e. LSM trees and step integration methods) that little attention is given to the development of new index programs that allow fast insert. Because adding to a large warehouse can be millions per day, the index requires a disk search (even a significant part of the search) each insert is unacceptable.
We are focused on the JOQR phase and the development of optimized algorithms, computing communications and calculation costs.Using a model based on the representation of the distribution of data as a color, we develop an effective algorithm of the issue choosing the distribution properties in a query tree to minimize the total cost.We expand our models and algorithms to integrate the interaction of the data distribution with the traditional optimized options, such as access methods and calculation strategies.Our algorithms are applied to these algorithms, such as the distribution of data as a color, we develop an effective algorithm to solve the problem of the distribution of the properties in a query tree to minimize the total cost.
The four disturbance patterns are objectively determined by the classification method, each of which shows similar trends and characteristics.Based on the classification distribution, intervals 2 are determined as the worst overall state.LSTM networks are used for each classification performance forecast.LSTM also shows good forecast efficiency compared to traditional multi-layer nerve networks.The forecast data is consistent with the observation data, with the corresponding rate of R2 equivalent to 88.4%.
We proposed a solution to eliminate this reliable authority. The solution builds a centralized privacy protection index with a distributed access control to perform the search protocol. Two alternative methods to build a centralized index that allows the efficiency and security of transactions. The new index provides a strong and quantitative privacy guarantee, even if the entire index is public. The experiment in a real data set verifies the performance of the system. Our solution calls is twice: (a) the content provider keeps full control of defining the access group and ensures that it is compliant, and (b) the system executives maintain a balance of privacy and efficiency issues for their specific fields.
In order to address these limits of current reorganization technology, we presented the SERF framework, which is designed to provide a rich environment for the complex user-defined transformation in a flexible, easy and correct way. Our work is aimed at improving the availability and availability of the SERF framework and making it available for reorganization issues beyond OODB evolution. To this goal, we provide the availability transformation through the concept of the SERF templates that can be packed into the library, thereby increasing the portability of these transformations. We now also have a first step to provide consistency guarantees to the users of this system, a regular optimizer, providing some performance improvements by improving query optimization technology, focusing on the initial reorganization in this framework.
The article proposed an extensive copy management protocol in a large-scale copy system. The protocol organizes the site and data to a wooden, serial collective architecture. The basic idea of the protocol is to complete the complex task of updating the data with a very large amount of copy through a set of related but independent transactions. Each transaction is responsible for updating the copy in a single collective and quotes the additional transactions of the member collective. The main copy (every collective) is updated by a cross-collective transaction. Then each collective is independently updated a separate transaction.
Building such a database system requires a fundamental change in the architecture of the query processing engine; we introduce the PREDATOR system-level interface that supports E-ADT and describe the interior design details.
Many valuable web pages accessible database content is only accessible through the search interface, so the traditional web page "browser" is invisible. Recent studies estimate that this "hidden web page" size is 50 billion pages, while the size of the "browser" web page is only estimated to 2 billion pages. Recently, commercial web pages have begun manually organizing the web page accessible database to Yahoo! similar level classification program. In this article, we introduce a method to automatize this classification process, using a small number of query evidence. To classify a database, our algorithms will not obtain or check any file or page from the database, but only use the number produced in each query database.
We introduce a method of database interaction, using the integrity limitation of the database definition provided.We identify two integrity limitations roles in the database interaction. First, the integrity limitation of the status of the integrated view can be derived from the limitation of the database definition.In addition, the local integrity limitation can be used as the specification of the integrated view of the objects.We describe our ideas in the context of the instance based on the database interaction model, in which the object rather than the category is the integrated unit.We introduce the concept of objectivity and subjectivity as an indication of whether there are limitations of the effectiveness in the specific background of the database and without these effects.
A new approach, known as the M tree, suggests to organize and search from a general "measure space" big data set, i.e., the object is defined by only one distance function, meeting the motivation, synchronous and triangular inequality of the offer. Our detailed algorithms to insert the object and divide management, keep the M tree always balanced - several alternatives to the concrete divide are considered and experimental assessment.
PMG provides the user with an possible pathway method for the purpose of asking the user whether the pathway method is required one. If the pathway method is rejected, then the user can take advantage of his current increased knowledge database request (there are additional parameters provided) another offer from PMG. PMG is based on access weight added to the connection between classes and pre-calculation access relevance between each pair of OODBs. Specific rules of access weight allocation and calculation access relevance appear in our previous files [MGPF92, MGPF93, MGPF96].
Key success factors for the data storage project have been identified. it captures all types of information required for the analysis, design, construction, use and interpretation of data storage content. to spread the use of data storage content, to make the interaction between the storage and the integration of tools in the data storage architecture, it requires the criteria for the data storage composition and exchange. this paper takes into account two criteria and is compared according to the specific areas of interest in the data storage.
Language understanding and production generally assumes using the same expression, but repeatedly raises a question to this point of view: this structure is produced regularly, but is considered very unacceptable.
In this article, we presented a big algorithm, TIX, and described how it as the integrated basis of information access technology, becomes a standard pipe database query engine.
In more detail, we use sliding windows on the data sequence and extract its functions; the result is the orbit in the functional space. We proposed an effective and effective algorithm that divides these orbits into sub-orbits, which are then represented by their minimum border rectangles (MBRs). We develop new evaluation strategies to obtain good performance, including based on the architecture of the TermJoin algorithm, to effectively evaluate the synthetic elements. We report a wide range of experimental evaluation results, including the new TermJoin access method that exceeds the use of the same function of the standard operator's direct implementation.
The increasing number of database applications requires high availability, combined with the online scale and soft real-time transaction response, which means the scale must be done online rather than blocked. This paper expands the main/heat station method to high availability and online scale operations. The challenge is to do so without damaging the response time and transaction transition, and supports high availability throughout the scale. We measure the impact of the online scale on the response time and transition using the di erent planning program. We also show some recovery problems that occur in this setting.
Big-size multi-dimensional computing integrators are a performance sign for many OLAP applications, and getting accurate answers to integrated queries may be expensive in terms of time and/or storage space in the database environment.
The increase in the use of web services has led to a great deal of interest in the field of service discovery.Web services finds that a machine process can describe a web service, which may be previously unknown and meet certain functional standards of the process.In the industry, many applications are available on the Internet by calling different web services.These applications are very dependent on finding the correct and effective web services.This paper provides an overview of the process of web service discovery from a multi-dimensional view.
We presented the algorithms for backup and recovery of files on the file server. Our coordinated backup and recovery algorithms have been implemented in the IBM DB2/DataLinks products. We also proposed an effective solution to keep the file content and related data stored in the DBMS from the view of the reader, without keeping a long time locked on the data table. In the model, an object is directly accessed and edited on the site through the normal file system APIs using a reference to the file changes obtained through SQL Query in the database, the user through the update of the DBMS and promises to update the file and data.
The aim of the seminar is to bring together researchers and practitioners from the academic community and industry to discuss how to incorporate the seminar into the perspective of the current geographical information system, and how this will benefit the final users. the seminar is organized in a way to stimulate interaction between participants. Three or four experts from the same or closely related disciplines reviewed each of the 32 articles to the authors. I want to sincerely thank the program committee and other experts who have done good work in the careful review of the paperwork: they have made a huge contribution to the quality of the seminar.
Mainstream database management system is designed specifically for general use. a variety of commitments have been achieved to meet the most common users and the largest market. an application has been mostly ignored, is the network equipment made for telecom operators. the equipment used in the telecommunications industry has different requirements than the traditional database application, in terms of availability and real-time performance.
This chapter introduces a common technology called Progressive Fusion (PMJ), which eliminates the type of fusion algorithm blocking behavior. The basic idea behind PMJ is that the fusion produces results as the external fusion produces initial running. Many of the most advanced fusion techniques require input relationships almost completely before the beginning of the actual fusion processing. Therefore, these technologies begin to produce initial results, only after a significant time. This blocking behavior is a serious problem that the subsequent operator must stop processing in order to wait for the initial result of the fusion.
We describe the magical set transformation implementation in the Starburst extended relationship database system, for our knowledge, this is the first magical set transformation implementation in the relationship database system, Starburst implementation has many new features that make our implementation interesting database practitioners (except database researchers).
In traditional software systems, the emphasis is on keeping the modules well separated and consistent, thus ensuring that changes in the system are placed into several modules. repeated use is considered a key method for achieving this goal. The system based on Ontology in the Semantic Web is just a special type of software system, so the same principle applies. In this article, we introduce an integrated framework to manage multiple and distributed ontologies in the Semantic ic Web.
External detection is a component of the statistical model and estimate. for high-size data, the classic method of Mahalanobis distance is usually not applicable. we propose an external detection procedure to replace the classic minimum visibility determination estimator with high-division minimum fragmentation product estimator. the cutting value is obtained from the unparallel distribution of distance, which allows us to control type I errors and provide strong external detection. simulation studies show that the recommended method performs well on high-size data.
The results show that there are performance and energy transactions between different queries.The nature of queries also plays an important role in determining energy performance transactions.In addition, technological trends and building improvements affect the relative behavior factors of the index structure.The work in queries on how and where (on a mobile client or server) should be performed and energy savings.
A new implementation model is based on the use of “Limited Double Rules” (LARs), allowing separation in the rules action. implementation model is essentially conducting a wide range of first exploration of the alternative expansion of the user requirements update. taking into account an object-based database program, whether integrity restrictions and specifications of derivatives and properties are written to a limited double rule of the family. theoretical analysis shows that the method is correct: implementation model returns all the effective “complete” user requirements, or ends the appropriate error notification.
This chapter reveals that the chargers and compressors convert XML files to a compressed but query format. the compressed library stores the compressed files and provides: the methods of accessing these compressed data, as well as a set of compressed specific uses, allowing, for example, the comparison of two compressed values. the query processor optimizes and evaluates the compressed files of the XQuery query. its complete collection of physical operators allow effective evaluation of the compressed library.
The overall performance can be improved through algorithms, allowing operations to adjust their memory use in time to respond to the actual size and volatility of their input and total memory needs. Classification is a common operation in the database system. It is not only used to produce classification output, but also to use many algorithms based on type, such as combination, copy removal, classification merger and setting operations. Classification can also improve the efficiency of algorithms through indicators, such as classification merger and serial recovery. This paper focuses on the dynamic memory adjustment of classification, but the same method can be applied to other memory intensity operations.
FileNet's Integrated Document Management (IDM) product consists of customer applications and image and electronic document management (EDM) services, which provide a solid facility for document creation, updating and deletion, as well as the ability to search for documents.Document properties are stored in the Basic Relationship Database (RDBMS); document content is stored in a file or specialized optical disk geographical storage manager.
Computer running database management applications often manage a large amount of data. usually, I/O subsystems price is a large part of computer hardware. intense price competition requires all possible savings. damaged data compression methods, when properly integrated to dbms, generate significant savings. by the way, a slight increase in the CPU cycle is more savings in I/O subsystems. various design problems occur in the use of data compression in dbms From the choice of algorithms, statistical data collection, hardware to software compression, location compression function in the computer system's overall architecture, compression unit, update, and application to data compression.
We represent the time of an event occurring with the possible moment of the collection, defining when the event may occur, and in this collection of the probability of the distribution. We also describe the query language structure to obtain information in the existence of uncertainty. These structures allow users to determine their reliability in the basic data and their reliability in the relationship between these data. A describing grammatics for SQL's choice declaration with the optional reliability and reliability structure is provided. We show that this grammatics is reliable, in which it never produces the wrong information, is the maximum, in which if it is extended to more information, the result may not be reliable and reduced to the previous grammatics, when there is no uncertainty.
In this world, the services will be combined in an innovative way to form the fine service of building blocks of other services. This is on the common basis of the vocabulary and communication protocols that operate in a secure environment. Currently, a large number of standardized efforts (UDDI, WSDL, eXML, RosettaNet) are aimed at achieving this common basis. We explore the possible architecture that can be used to deploy the internal services of computerized traders. This includes the structure and features of the "traders" and "services" and the form of these features realized in the actual implementation.
In a recent article, we proposed to add a STOP AFTER condition to SQL to allow the cardiacity of the query results to be queryed by the author and query tool clearly limited. We showed useful this condition, showing how to expand a traditional cost-based query optimizer to the hostel, and through DB2 basic simulation showing large performance performance is possible when STOP AFTER query is clearly supported by the database engine. In this article, we presented a few new strategies to effectively deal with STOP AFTER query. These strategies, mainly based on the use of range divide technology, provide significant additional savings to deal with STOP AFTER query to the size of the results setup.
Data copying has recently become a topic of increased interest among customers. Multi-database suppliers offer products for data copying, the ability of these products and they solve customer problems is widely different. This conversation begins by identifying some dimensions of copying solution space, including delays, competition, logic and physical units of copying, network link requirements, abnormality, copying topology, copying transparency and data conversion requirements. The digital equipment company offers three products that allow customers to copy data.
In order to pack more entries in one nod, the CR tree compresses the MBR key, occupying 80% of the index data in the two-dimensional case. It first represents a MBR key of the coordinates relative to its parents MBR in the lower left corner to eliminate the leading O from the relative coordinates representatives.
The customization of the web page to understand the trust of the web page in the community chart and calculate the page ranking in the chart is described. the trust factor of the page plays an important role in the page ranking. the web page can be effectively searched through the page related to a particular topic, if from the point of view of the search topic. customizing the chart to the page chart only the relevant features, removing the quotes from the non-related pages, therefore using only the relevant weight to calculate the page ranking.
The paper discusses the design and implementation of a database system that supports a series of data.SEQ simulates a series as an order record collection and supports a statement series query language based on an operator query algorithm, thus allowing the algorithm query to be optimized and evaluated.
Data storage is the latest "popular topic" in the industry. the market predicts to be $8 billion in 2000, all tastes suppliers claim their product adequacy and superiority, which leads to a lot of confusion, such as OLAP, ROLAP, MDDB, decision support systems (DSS) and data storage are defined, redefined, and sometimes even interchangeable.
In this article we present a diversified system architecture and implementation of software component market prototypes, we emphasize the ontology transformation idea by discussing the ontology simulation of the component market, the extended ontology simulation of the UML, and the diversified system adaptation to the dynamic change ontologies.
Open a series of specific work to follow, this visual paper identifies, motivates, and abstract model management issues. It suggests supporting “models” and their “map” as a high-end structure, advanced algorithm operating manipulation. In the winter of 2000, I was a start-up college at UIUC, this paper inspired me for countless moments when I had to create my own research agenda. I have always been interested in information integration in a variety of topics, such as query translation and data maps. The field is excited to me because it is full of “real world” problems.
This article explores the problem of minimizing a wide range of fragments of XP (i.e., XP) where the most common operators (children, descendants, wild animals and branches) use is permitted by certain synthetic restrictions.The fragments reviewed are by expressions, these expressions have not yet been specifically studied in the relationship environment: they are not only connective queries (because the combination of "//" and "*" allows a random form of separated expression) and will not match with separation (because the latter is more expressive).
The Dali system is a major memory storage manager designed to provide durability, availability and security guarantees, which are usually expected from no database while providing very high performance as it tends to support memory data. The Dali follows the philosophy of processing all data, including system data, uniformly as a database file, can memorize maps and direct access/update user processes.
This article describes alternative methods for data access available by developers using Java #8482; platforms and related technologies to create a new generation of business applications.
In order to facilitate the program's evolution, we recommend supporting the exceptions to the rules of consistency of behavior, without sacrificing the type of security. The basic idea is to detect unsafe statements in the method code when writing the time and check them in the running time. Runtime check is carried out by a specific condition, automatically inserting unsafe statements. This checking condition warns the programmer of security issues and allows him to provide an exceptional action code.
Over the past decade, the speed of the commodity CPU has made clear progress, in memory delayed. main memory access is therefore increasing in many computer applications performance string, including the database system. In this article, we use a simple scan test to show this string serious effects. obtaining insight is translated into the database architecture guidelines, in terms of data structure and algorithms. We discuss how vertically divided data structure optimizes storage performance of serial data access. Then we focus on equi-join, which is usually randomly accessed operation and introduced the algorithm of divided string.
The design of a distributed database system differs from the design of a traditional non-distributed database system, which requires a distributed design, both the database and the control database. In this article, we discuss the problem of the rule distribution. We consider the cost of the data communication and the rule implementation as the main basis of the rule distribution. The rule distribution problem can be described in the oriented asicture where the nodes represent the rule or the relationship, the margin represent the relationship between the rule or the rule of use.
The experience of the work prototype optimizer shows that (i) the additional optimization and start optimization of the dynamic program is dominated by its advantages when running; (ii) the dynamic program is as powerful as the "gross" method of running optimization; that is, the dynamic program keeps its optimization even if the parameters change between editing time and running time; (iii) the start optimization of the dynamic program takes much less time than the full optimization when running.
We introduced a method to find a genetic DNA sequence, using an adapted Sufx tree data structure deployed on the general purpose of the durable Java platform, PJama. Our implementation technology is new, it allows us to build a volunteer sequence of Sufx tree on the disk, for example, the longest human chromosome consists of 263 million letters. We recommend using these indicators as an alternative to current practice of serial scanning. We describe our tree to create algorithms, analyze the performance of our indicators and discuss the interaction of data structure with the object warehouse building.
On the contrary, our method is based on the special characteristics of high-size space optimization, so it provides the almost optimal distribution of data elements between the discs. The basic idea of our data decomposition technology is to allocate the different directions of the corresponding data space to different discs. We show that our technology - unlike other decomposition methods - ensures all the corresponding neighboring directions allocate to different discs. We use a lot of real data (up to 40MBytes) to evaluate our method and compare it with the most familiar data decomposition method, the Hilbert Curve.
The analysis of the expected and experimental results of various combined algorithms shows that the combination of the best combined blocks and the best combined algorithms usually provides the maximum cost efficiency unless the size of the relationship is a small amount of memory. The algorithm quickly determines the minimum cost of a distribution producing each of these algorithms. When the size of the relationship is a small amount of main memory (usually up to three to six times), the combined algorithm is priority.
We study various types of operations in the database background and show how using SIMD instructions can accelerate the operations in the internal circle. using SIMD instructions there are two immediate performance advantages: it allows a degree of parallelity so many operators can be processed simultaneously. it also often leads to the elimination of conditional branch instructions, reducing branch error predictions. We consider the most important database operations, including serial scanning, integration, indexing operations, and joining. We introduce the implementation of these techniques using SIMD instructions. We show that there are significant advantages in re-designing traditional query processing algorithms so that they can better use SIMD technology.
In this article, we describe a approach to technology that reduces the storage costs of the cube without causing a false assessment of the operating time costs. The idea is to provide an incomplete description of the cube and a method to estimate the lack of input with a certain accuracy. The description, of course, should be part of the entire cube space, the estimated program should be faster than the calculation of the data from the base relationship. Because the cube is used to support data analysis, analysts are rarely interested in the accuracy of the collective value (but more in the trend), providing close-range answers in most cases is a satisfactory compromise.
Database Management is one of the main research fields of the Oklahoma University School of Computer Sciences (OU). The goal of the database research team is to help solve the many problems and challenges faced by the database research community, related to emerging technologies. Currently, many projects are conducting the following fields: real-time databases, objective-oriented databases, mobile databases, multimedia databases, data mining and databases. These projects have been funded by federal and national institutions as well as the private industry, such as the National Science Foundation, the U.S. Department of Education, the environmental quality, goals and goals of Oklahoma.
We follow the standards of the query language, which is a new formal and intellectual paradigm to integrate query and programming to an object-oriented database. query is considered a general programming expression that can be used for macro-obligatory statements such as creating, updating, entering and deleting data objects. query can also be used as a program parameter, as well as determining the production from functional programs (SQL similar views).
The purpose of the text database is to store text files. These files have not only text content but have structure. Many traditional text database systems are only focused on content or structure queries. Recently some models have emerged, integrating two types of queries. We discuss this integration and focus on these recent models, covering the representative sample fields of recommendations. We especially pay attention to the expression and efficiency exchange, showing the model's commitment. We discuss achieving good compromise because in both aspects, the model is useless for many applications.
In this article, we explore the predictive configuration options considered in the installation of DBMS, introducing a series of algorithms that gradually form more complex and effective optimization solutions. Through the analysis and performance measurement of the installation of SQL queries, we classify queries and highlight the simplest solutions that will correctly optimize each class. We showed the limitations of the previously published algorithms and discussed the challenges and feasibility of implementing various algorithms in business-level systems.
In this article, we explore a method to exchange a Bush execution tree with a Hash filter to improve the execution of multiple queries. Similar to a distributed queries processing, the Hash filter can be applied to eliminate the non-compatible double relationship before the execution of a combination, thereby reducing the combination cost. Note that in the different execution stages of the Bush tree the Hash filter can have different costs and effects. The Hash filter effect is first evaluated.
The University of Michigan’s MultiView project is a five-year NFS effort aimed at developing and applying objectively-oriented visual technologies to meet recent applications such as data storage and workflow management systems, the need for sharing, virtual reorganization and data storage.
In this article, we presented two algorithms to produce the best and close to the best vertical class separation program. The cost-oriented algorithm provides the best vertical class separation program, by listing, thoroughly, all the program, and calculating the required number of disc access to implement a specific application. For this purpose, a cost model was developed to implement a OODB system method group. Because the comprehensive list is expensive, for classwork with only a few variables, developed a ski ski ski ski ski ski ski ski ski ski ski ski ski.
In this article, we developed the core of the official data model of the web directory and presented a series of effective computing query languages, with increasingly strong expression capacity. the directory data model can naturally represent the abnormality rich forms displayed in the real world. the query answers expressed in our query language can show the same abnormality. We introduce external memory algorithms to evaluate the query submitted in our directory query language and prove the effectiveness of each algorithm in terms of its I/O complexity. our data model and query language share the flexibility and practicality of the latest semi-structure data model proposed, while effectively addressing the specific needs of the web directory, we represent the instance.
In this article, we describe the architecture and interface of KODA, a production intensity database core, KODA is the unique capacity of the industry to support two different data models, such as Oracle Rdb (a relative database system) and Oracle CODASYL DBMS (a CODASYL database system). our design and implementation experience proves the feasibility of KODA, implementing multiple data models on a common core, the advantage of L is to use the performance and functionality improvement of a variety of products, the advantage of L is to maintain a common code base, the transfer and interoperability between the two products, no customer needs to re-learn the common core tools such as data backup, file organization and analysis tools.
When building a database, it is necessary to design a friendly interface that will allow the user to easily access the data interested.V ery often, this interface uses the power of visual and direct manipulation mechanisms.As usual, it is not suitable to connect "any" visual representative task with the database, but the visual representative should be carefully selected to eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
Data flow is a new category of data that is becoming very important in a wide range of applications, from sensor networks, environmental monitoring to finance. In this article, we presented a new online evolutionary framework for multi-dimensional flow data, which will include repeated volatility density estimates in the background of speed density estimates. In the proposed framework, the change of flow data is characterized by the use of local and global evolutionary rates.
In this article, we introduce the formalism of the expression and discussion of the command properties: commands and combinations have physical representative limits; in this way, we can explain how the relationship is arranged or combined, either in the primary and sub-level meaning of the command properties. After formally defining the command properties, we introduce a plan to improve the command properties as the middle and final query results, based on the known query input, and then use these conclusions to avoid unnecessary classification and combination.
Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases.The prototype of the multimedia data mining system, MultiMediaMiner, has been designed and developed.It includes the construction of a multimedia database that facilitates multi-dimensional analysis of multimedia data, mainly based on visual content, as well as the mining of a variety of knowledge, including summary, comparison, classification, association and classification.
Bitmaps are the popular indicators of database (DW) applications, and most database management systems offer them today.This article presents options for using bitmaps for queries and optimization strategies.There are continuous and controversial options criteria.Quests optimization strategies are divided into static and dynamic.Statistical optimization strategies discussions are based on the best design and logic reduction, as well as algorithms.
In this list, we will review these three books.
When I first met the AMS paper, when I began to be interested in the data streams, in the spring of 2001, reading this paper was a real openness for me, I was surprised to see how simple randomized ideas and basic probability tools (such as Chebisev's inequality and Chernov's boundaries) gathered together to provide an elegant, space-efficient random approach algorithm for the estimation of the problem, which at first appears unresolved.
The World Wide Web (WWW) is a growing, distributed, non-managed, global information resource. It lives in a computer network around the world and allows access to unusual information: text, images, videos, audio and graphic data. Currently, the wealth of this information is difficult to extract. A person can manually, slowly and boringly browse the WWW or use the index and library, built by an automatic search engine (the so-called knowledge robot or robot). We have designed and are now implementing a high level of SQL similar language supporting effective and flexible query processing, processing the structure and content of the WWW section and its different types of data.
We will consider academic and non-academic positions, with particular attention to some academic job search skills. when solving these issues, we will refer to our combined personal experience of two-person work hunting, such as 2003, such as 1987, as well as the information of two-person work search friends and colleagues.
Over the past two years, Nimble Technology has developed a product for this market, starting with more than a decade of personal data integration research, which has been deployed on several Fortune-500 beta customer websites, this abstract report tells us about the key challenges we face in product design and emphasizes some of the issues we think need more research community attention.
In this context, the load balance must be processed on two levels, locally between each shared memory nodes processor, globally between all nodes. In this paper, we proposed a dynamic implementation pattern to maximize the local load within the shared memory nodes, and to try to reduce the need for the shared load between the nodes. This is by allowing each processor to perform any operator that can be processed locally, thus fully exploiting the inter- and internal operator balance.
The database is a collection of data from a variety of sources that may be distributed and connected smoothly to respond to the organization’s OLAP queries.The relative view is both as a specific technology and as a derivative implementation plan for data storage.In this location paper we summarize the diversity of the relative view and its potential.
The data network is built around the world as a next-generation data processing system to manage one-fifth of the data and storage space between organizations.Data network (datagrid) is a logical name space composed of storage resources and digital entities created by an autonomous organization and its users.Data network management system (DGMS) provides services for the integration of organizations and the management of data and resources in the data network.
Given the complexity of the many queries about the database (DW), it is interesting to budget and store in the DW some of the necessary operations, the so-called materialist views. In this article, we introduce an algorithm, including its experimental assessment, which allows to simultaneously materialize multiple views without losing the vision of the processing costs of using these materialist views queries.
Due to the widespread use of the Internet and external technology, especially electronic document exchange, we have to adapt to the policy of document conservation to this new challenge: “Even the lack of memory in public administration.”
To these goals, Rainbow allows users to configure and programming the environment, transaction and transaction management protocols, and observe local and global implementation (history and measurement behavior and performance).
We proposed a framework to design an effective migration plan in a declared and flexible way and to implement a migration tool called RelOO, which aims to the relative side of any ODBC matching the system and the object side of the system. The framework includes (i) a declaration language to determine the transformation from the relative to the object database, but also the physical properties in the object database (class and classification) and (ii) a algorithm-based program rewrite technology to optimize the migration processing time while taking into account the physical properties and transaction disintegration.
In order to meet the dynamic scenarios of rapidly changing business needs, companies have used network technology to manage business processes, but being able to integrate business processes such as purchases, customer relationship management, finance, human resources and manufacturing into a typical network supply chain is a challenging task.
His book database and transaction processing constitutes a standard database text for advanced postgraduate and postgraduate courses - although a bit different focus compared to the created books. as the title "application-oriented methods" shows, the author emphasizes teaching the system use of the database system - rather than focusing on building the database system's implementation technology.
Here we propose a PeerOLAP architecture that supports online analysis processing queries. a large number of low-end clients, each containing a most useful result of cache, through a voluntary P2P network connection. if a query cannot be responded locally (i.e. by using computer cache content, it is distributed), it is spread through the network until a cache answer is found. a answer can also be built by many opponents' part results. therefore, PeerOLAP as a large-distributed cache, which enhances the benefits of traditional client side cache.
The paper briefly describes a method of commercial to commercial internet trading, according to a n-providers:m-clients scene to provide a virtual market position on the internet.
The appearance of the Internet-based electronic marketplace and increasingly popular, in its various forms, presented the universal challenge of exploring the market design. In this paper, we introduce a domain-specific software architecture, describing an abstract component of the general market, and determining the limitations of control and data flow, as well as a framework that allows the appropriate component to implement specific market policies.
The database supports the analysis of historical data. This often involves integration within a period of time. In addition, the data is usually included in the growing order of time properties, for example, the date of sale or the time of temperature measurement. In this article, we propose a framework that uses this attachment only to update the nature, because of a time properties. The framework allows us to integrate a lot of new data into the warehouse and effectively produce historical summary.
In this article, we use these properties to develop an archive technology in its use space is effective and retain the continuity of the elements, through the database version, which is not provided by the traditional minimum editing distance of the DIF method. The method also uses the timetable. All versions of the data are integrated into a series, one of which elements appear in multiple versions only once stored with the timetable. By identifying the continuity of the elements and integrating them into a structure, our technology is able to make changes from the database version, thus allowing us to find a specific response from a given timetable.
Successful companies organize and carry out business activities in an efficient way. core activities are completed in time and within a specific resource limit. However, in order to remain competitive in today's markets, companies need to constantly improve efficiency - business activities need to be completed faster, higher quality and lower cost. For this purpose, people are increasingly aware of the benefits and potential competitive advantages, and can provide a carefully designed business process management system. In this paper, we discuss an agent-based method: showing how agent technologies improve efficiency, ensuring business activities better planning, implementation, monitoring and coordination.
XQuery is a XML query language that is currently being developed in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C work projects and several reference implementations have been available on the web. If successful, XQuery is likely to introduce one of the most important new computer languages in a few years. This tutorial will provide an overview of the synthesis and grammar of XQuery, as well as an insight into the principles of language design.
In this article, we describe an open market using the workflow technology and the current data exchange and generic data representation standards that appear on the network. In this market architecture, e-commerce is achieved through the adaptable workflow template provided to its users. There is a workflow template for the e-commerce process, the result is based on the architecture of the components in which the components can be an agent (buy and sell) as well as the existing applications referred to by the workflow.
Model-based engineering technology provides a prospective approach to addressing the complexity of the platform and expression field concepts in the third generation of languages.
Starting in 1989, the advertising gathers of DBMS’s senior researchers gathered regularly to conduct a “group gathers” to evaluate the art status of DBMS’s research and forecasts on which issues and issues are worth more attention.
We live in an exciting moment in the field of clinical research and the changes that occur will change the way health care is provided and the work of clinical research professionals will guide these changes.
In addition to frequent updates, the database collection is often very dynamic: new databases often appear and the databases (as a website) disappear.
IP network operators collect integrated traffic statistics on the network interface through the Simple Network Management Protocol (SNMP). This is part of the regular network operations of most ISP; it involves a large infrastructure for multiple network management stations to request information from all the network elements and to collect data transmission in real time. This presentation will show a tool to manage the SNMP data transmission in a full-function ISP on the industry scale. The tool is mainly used to study the interconnection of network traffic by providing a user-friendly interconnection interface and a rich mixture of advertising queries based on the expertise of the network operator.
In the past few years, the unusual growth of the computer network has created an incredible range of products and services on the network. this vast amount of information makes a single person unable to analyze all existing products on the network and decide which of them is more suitable for her needs.
This article describes a new way to use data mining technology in the Internet data so that it can be found in the e-commerce scenarios. The data is believed to cover not only various types of server and web meta information, but also marketing data and knowledge. In addition, its abnormal solutions and Internet and e-commerce specific pre-treatment activities are embedded. A general web log data mixer is officially defined and provides a organized design for analysis and forecast activities. From these materialist points of view, various online analytical web pages using data mining technologies are shown, including marketing expertise as a domain name knowledge and designed specifically for e-commerce purposes.
Question Optimization is a powerful process of calculation, especially in the current data storage and mining applications. Question Optimization internal superquestion is made up of a new question usually optimized to the fact that there is no chance to overcome these superquestion priority optimization. Although the current business question optimizer for the previous question generated implementation plan (such as "Save Output" in Oracle 9i), question matching is very limited - only if the upcoming question has a close similarity to the text of a storage question is related to the plan to perform the new question.
ROLEX is a closely connected XML relationship interaction research system [2]. While typical XML-based applications interact with existing relationship databases through the “shared and open” method, the ROLEX system seeks direct access to XML data through the XML interface at a hidden speed. To this goal, ROLEX is closely integrated with DBMS and applications, through the most XML-shared supported standard interface, document object model (DOM).
VideoAnywhere has developed this ability to expand the architecture as well as to use the latest Internet programming (Java, agent, XML, etc.) and the applicable standards. It automatically extract and manages an extended data set of the main types of videos that can be searched through properties or keywords. It also provides user configurations that can be filtered with queries processing. a user-friendly interface provides all system features and capabilities management.
This is a very simple paper that I think contains a lot of ideas that will reappear in a different way every decade! The paper proposes a dictionary of a copy (principally a set of key and value pair) to all the relevant sites in the distributed system. Updates and deletions are spread through the system, because the site communicates with each other, using a simple concept of logs.
The database community has been studying similarity time sequence databases for years. The technology developed in this field may shine to the query by adjusting the problem. In this presentation, we deal with the melody and user adjusting entries as a time sequence. This method allows us to integrate many databases indexing technologies into a query by adjusting the system, improving the quality of this system, through the traditional (about) sequence databases methods. We design special search technologies that are unchanged, time adjusting and local time adjusting which makes the system stable and allows more flexible user adjusting entries.
We have expanded Rainbow, our existing XML data management system, as shown in Figure 1.Rainbow received a XQuery query or update request, from the user extension XQuery synthesis.XQuery is divided into an algorithm representative known as XMLAlgebra Tree (XAT).XAT then is optimized by the global query optimizer using the algorithm rewrite the rules.We introduced a separate XAT cleaning phase, which includes XAT tables clean and cut unnecessary XML operators.
The Brazilian Database System Seminar (SBBD) is a traditional Brazilian conference, sponsored by the Brazilian Computer Association, and the SBBD's technical program includes the following activities: the competitors review the full technical thesis, the invitation of the conversation, the tutorial (invited and selected from the submission), the discussion of the panels and the introduction of the tools.
XML has become common and XML data must be managed in the database.The current industry standard is to convert XML data maps into relative tables and store this information in the relative database.These maps constitute expressive power issues and performance issues.In the TIMBER project, we are exploring issues involving XML stored in local format.We believe that the key intellectual contribution of the system is a comprehensive timely query processing capacity in the local XML store, all the standards of the relative query processing components, including the algorithm rewrite and cost-based optimizer.
The OASIS prototype is being developed at the University of Dublin, Ireland, and we describe a multi-database architecture, using the ODMG model as a channel model, and describe the extension of building virtual charts in multi-database systems.
The company is using the network information spread to stimulate interest in the model and effective mechanisms to control access to information. In this context, it is important to ensure that the XML file is important. Most work has been studied model to define the XML access control policy, focusing on issues such as access and conflict resolution. However, there is a few work to perform the access control policy for queries. A naive two-step solution to ensure queries evaluation is first calculating the queries results and then using the access control policy to filter the results.
The greatness of the advanced therapist: a mixture of multi-rolls of clinical and ethical consequences” is Robert S. PEPPER, C.S.W., the second paper of the doctor, which will appear on this very important topic in contemporary practice psychotherapy.
This article describes problems and solutions related to the creation of a corporate product information database and uses the database as the basis for the implementation of an electronic directory. Today, product information is usually managed in the document composition system and communicated on paper. In the new cable world, these processes are undergoing fundamental changes to address the market pressure and the precise, complete and structured presentation of product information.
With the popularity of XML, it is increasingly common in XML format, which emphasizes an important question: considering XML documents S and DTD, how to extract data from S and build another XML document T so that T matches XML conversion? Let’s mention this as DTD matches XML to XML conversion. The need for this is obvious, for example, data exchange: enterprises exchange their XML documents with some pre-DTD. Although some XML query languages (e.g., XQuery, XSLT) are currently using XML data conversion, they cannot guarantee DTD matches.
The South Korean Ministry of the Environment (MOE) - G-7 project - in collaboration with two national research institutions, university research companies and consultancy companies launched a long-term water quality research, which includes the development of computer software on the total water quality management system, known as ISWQM (Integrated Water Quality Management System).ISWQM includes four main components: GIS database; based on artificial intelligence (AI) two expert systems to estimate the pollution load and provide cost-efficient waste water treatment systems for small and medium urban areas; as well as computer programs to integrate the database and expert systems.
The popularity and availability of smart sensors, such as network cameras, microphones, etc., creates an exciting new category of distribution services. While these sensors are cheap and easy to deploy in a wide area, achieving useful services needs to address some challenges such as preventing big data from transmitting on the network, effectively detecting the relevant data in the distributed sensors collection, and delivering it to interested participants, and effectively processing static data information, live sensors transmission, and history data.
For years, you’ve been listening and reading a new standard, all known as SQL3. designed as a major improvement to the current second generation of SQL standards, commonly called SQL-92 because it’s the year of its release, SQL3 was initially planned to release around 1996... but things didn’t go as planned.
In this article, we describe the system architecture and its basic technologies, and report our ongoing implementation efforts that take advantage of the open source base of PostgreSQL, and we also discuss the open issues and our research agenda.
A method to measure the racial proximity between the large, integrated components of the model, as well as an example describing its application. It concludes that many issues related to the weak model racialism can be solved by adding the logic and background-based tools to show the ethnic weaknesses to the end user.
The phrase match is a common IR technology to search for text and identify the related files in the document collection. The phrase match represents new challenges in XML as the text can interact with voluntary labels, overthrow the search technology, requiring strict dialogue or close proximity to keywords. We introduced a technology to match the phrase in XML, allowing dynamic specifications, whether the phrase match and labels are ignored. We developed an effective algorithm, our technology, using the phrase word and XML labels.
Stanford’s STREAM project is developing a general system to deal with multiple continuous data flows and storage relationships, designed to deal with a large number of complex continuous queries with high capacity and explosive data flows, we describe the system’s status to the beginning of 2003 and list the direction of our research.
The paper describes the principles of the academic and teaching interaction mechanisms built in the European knowledge pool system developed by the European research project ARIADNE, which is the core characteristic of ARIADNE, consisting of a distributed storage of the teaching documents (or learning objects) of a variety of particles, origin, content, language, etc., which are stored for its use (and reuse) in electrical training or teaching courses.
I am pleased to announce that SIGMOD is financially strong, our meetings are technically best, and this research excellence contributes to financial health, allowing the meeting to continue producing more than enough income to cover its spending.
MTCache is a prototype middle-level database cache solution that achieves this transparency for SQL servers. it is based on SQL servers support for materialized viewing, distributing queries and copying. we describe MTCache on the TPC-W reference label and report the experimental results. experiments show that a large part of the query work load can be transferred to the cache server, thus significantly improving the reading-driven work load of the reference label.
In this environment, access to relevant and accurate information is increasingly complex by the characteristics of widespread distribution, independent, diversified and dynamic sources of information, and this complexity is aggravated by these potential global, interdisciplinary, multicultural and rich media technologies’ continuous development of systems, academic and structural abnormalities.
This information is usually best used in structurated or relative forms, suitable for complex query processing, with relative databases integrated and data mining. For example, newspapers and email files contain information that may be useful for analysts and speech agencies. Information extraction system produces a structurated information representation, "buried" in text documents. Unfortunately, the processing of each document is expensive and does not apply to large text databases or web pages. Many databases are larger than millions of documents, processing time becomes a bottle of using information extraction technology.
In this demonstration, we introduced a prototype Peer-topeer (P2P) application known as PeerDB, which provides the database capacity, which is developed in collaboration with the University of Singapore and is improved through more features and applications, the concept behind PeerDB is similar to the publication of personal websites, unless now applied to personal databases.
The objective of the paper is to describe the MOMIS (Mediator EnvirOnment for Multiple Information Sources) method to integrate and investigate various unusual sources of information, including structured and semi-structured data.
GridDB is an innovative solution built in Toshiba to solve these complex problems that its many customers face, and GridDB’s principle is to provide a diverse database, optimize IoT, provide high scalability and high reliability for high performance.
In the past few years, obtaining high-quality information has become a challenging task as data should be collected and filtered from a wide, open and frequently changing data source network, with a confusing sequence, and without the structure and availability of data sources. This technological challenge is highlighted by the appearance of the world wide network, its current state and development nature. This situation creates the demand for information services building technology, capable of providing high-quality information, obtaining the necessary basis from distributed, unusual and independent data sources.
In the U.S. Department of Defense (DoD), seed interactivity is a growing challenge. In this paper, we describe the basis of the reconciliation infrastructure of the relevant but heterosexual properties. It describes the three types of information that can be used to judge the character background, clearly hide the seed conflict and enable it to properly adjust the value. Through an extended example, we show how an automatic integrated agent can produce the four tasks required in a simple seed reconciliation.
Database queries are usually performed in the form of relevant SQL queries. interrelationship refers to the value of use in an external queries block to calculate internal queries. This is a convenient paradigm for SQL programmers and closely simulates the functional reference paradigm in a typical computer programming language. The queries related are also often created by SQL inventors, translated from a specific language to SQL. Another important queries category using this relevant queries table is involving the use of a temporary database of SQL. The performance of these queries is very important in a large database.
One core operation of the database is to build a database that can be considered a multi-level, multi-level database, integrating data in multi-level.
We deal with the combination of dozens of classifications to a better problem, our first contribution is to introduce the concept of classification portfolio, we build a complete chart, each classification portfolio is one nod and a margin, weighing the similarity between the classification portfolio, the result of the community structure is found from this chart, using the most advanced Louvain algorithm, our second contribution is the level combination methods driven by these communities.
With these new expectations, new responsibilities for information systems have emerged, we can no longer just worry about keeping our system running, now we need to care about subjective concepts such as response time and transition time, what is now expected, performance adjustment has become crucial.
In addition, the Internet is an open environment, information sources, communication links and agents themselves may appear and disappear unpredictable. Therefore, an effective, automatic search and selection of related services or agents are human users and agents are also essential.
As our dependence on computers and computer data increases, we’ve expected more from our computers. We’re no longer expecting our computers as big expensive computers, these computers just exclude bills and pay checks. We also expect our systems to be able to quickly access and interact with us to show a lot of accurate data.
The rapid growth of the number of files, its diversity and terms change make it more and more difficult to manage the Federal Digital Library. The appropriate abstract mechanism requires the construction of a meaningful and extensible document collection, forming a cross-digital library information space for browsing and grammatical search. This paper discusses the above issues, proposes a distributed grammatical framework, achieving the logical distribution of the information space based on the subject area, and provides the facilities for the background and landscape of the available document settings.
Given the complexity of the many queries about the database (DW), it is interesting to budget and store in the DW some of the necessary operations, the so-called materialist views. In this article, we introduce an algorithm, including its experimental assessment, which allows to simultaneously materialize multiple views without losing the vision of the processing costs of using these materialist views queries.
In this regard, we recommend the transition from the cardinalistic method to the rate-based method and provide an optimized framework designed to maximize the yield of the query assessment plan. This method can be applied to cases where the cardinalistic method is not used. It may also be useful because by focusing on the rate, we are not only able to optimize the time the final result occurs, but also can optimize the number of answers for any determined time after the query assessment begins.
Welcome to SIGMOD2002!We think you’ll find the meeting and the environment exciting.The eternal beauty of British Columbia will provide a suitable comparison to the dynamics of our large-scale, high-performance and increasingly intelligent database systems are designing and deploying.This dynamics reflects in our (extreme) focus presentations, tutorials, research workshops, presentations, industrial workshops and product presentations.The only unfortunate side of our plan is that the five parallel meeting structures can stop you from listening to every speech you are interested in.
The InfoSleuth T M project developed a distributed agency architecture that addresses the need for interaction between information sources and analytics tools in various applications fields. InfoSleuth is being used as an important component of the environmental data exchange network (EDEN). The current EDEN pilot display allows integrating access to the environmental information resources provided by these agencies in several states. On the application level, InfoSleuth provides interactivity between users by allowing application developers to express the concepts and relationships of the application field in advanced terms and then translate into a database or text and image resources.
HG tree is a multi-dimensional indexing tree designed by the point and is a simple modification of the Hilbert R tree to the space data index.HG tree data search method mainly uses the Hilbert index value to search for accurate data instead of using the conventional point search method, as used in most R tree paper.Hilbert curve values and MBR can reduce a space coverage of a MBR.
The lack of most research efforts is a means of guiding the classification process and understanding the results, which is important if the data is considered to be high and not collected for the purpose of analysis.
The main objective of the design process is (a) to combine the web page with its advanced descriptions, which can be used for search, evolution and maintenance; (b) to provide multiple views of the same data; (c) to separate the information content from the web page, navigation and presentation, which should be independent and independent; (d) to store the information collected during the design process in a dynamically created storage of the web page; (e) to collect information about the use of the web page, which can be used for static (user registration) and dynamic (user tracking); (f) to support selectivity of the user-based information; (g) to use the web page's personal creation rules and personal creation rules to improve the web page's personal rules.
Currently, genetic expression data is being produced at the rate of the phenomenon, and the overall goal is to try to better understand the function of cell tissue, in particular, a specific goal is to link genetic expression to cancer diagnosis, forecast and treatment, however, a key obstacle is the availability of the tool or its lack, which prevents the use of data, making it difficult for cancer researchers to analyze effectively and effectively.
The chairman of the Association Trust Committee of ACRP tells him how he left the production and research and development post a decade ago, which was a move that included him in a role he rarely knew as he did not take part in clinical research in advance.
There are several database systems available for free to study the community, full access to the source code. Some of these systems result in completed research projects and others have been developed outside the research community. How is the database community best to take advantage of these publicly available systems? The most widely used open source database is MySQL. Their goal is to become "the best and most commonly used database in the world."
Widespread in academic and corporate research settings, ACM Database Systems Trading (TODS) is a key publication of computer scientists working on data abstraction, data models and data management systems design, with topics including storage and access, transaction management, distribution and federal databases, data science, intelligent databases and related operations and algorithms.
From recent meetings, location papers, special magazines issues, and informal exhibition hall discussions, we witnessed the rapid increase in the interest of the database research community in query and processing data flows. Many modules applications run data in the form of fast, continuous flow, in many of these applications, simply transfer the entire flow to a traditional DBMS, and in the traditional way query them in terms of performance and functionality is impossible.
Currently, many projects are being carried out in the following areas: real-time databases, object-oriented databases, mobile databases, multimedia databases, data mining and databases. These projects have been funded by federal and national institutions as well as private industries, such as the National Science Foundation, the U.S. Department of Education, the Oklahoma Department of Environmental Quality and Goals, Inc.
Large network operators and network service providers need to monitor and analyze network traffic through their system flow. monitoring requirements from long-term (e.g., monitoring link use, calculating traffic matrix) to advertising tremors (e.g., detection of network infiltration, deteriorating performance problems). many applications are complex (e.g., rebuilding TCP/IP meetings), query layer 7 data (find streaming connection), running a lot of data (Gigabit and higher speed link), and have real-time reporting requirements (e.g., improving performance or infiltration alerts).
This article is designed to classify existing methods that can be used to query an abnormal source of data, and we consider one of the methods - mediation query methods - in more detail, and provide a classification framework for this.
Our presentation document consists of two main components: a set of Berkeley TinyOS battery-driven wireless sensors "sensors" (see Figure 1) generating and processing data, as well as a desktop-based query processor that processor distributes queries, distributes these queries, and collects and displays answers.
The Internet search engine has been popular based on keywords search. Although the relative database system provides Powerfifl structured query language, such as SQL, does not support the keywords search database. As a query pattern, the simplicity of keywords search provides compulsory value of data exploration. In particular, keywords search does not require a initial knowledge of the program. The above is significant because a lot of information is increasingly available in a company in its internal network. However, it is unrealistic expectations of users who will browse and query such information with detailed knowledge of the available database.
We are pleased to announce an excellent technical program at the 6th International General Computing and Communications Conference, which covers a wide range of topics in the field of General Computing and Communications, this year the project committee submitted 160 papers, resulting in a highly competitive selection process, resulting in a high-quality paper program.
Welcome to Santa Fe's IPDPS in 2004, this year's program includes 17 workshops, a total of 306 workshops, many workshops have been growing stable, now running with parallel meetings or more days, we're glad to welcome this year's new workshops in the field of high-performance network computing, as usual, we're looking for new workshops for the next IPDPS.
Databases allow quick online analysis of large databases, which is attractive in many applications. Despite several available cubic-based OLAP products, users may still face the challenges of exploring the efficiency and efficiency of the big databases, as well as the big computing space of the databases. CubeExplorer is an integrated environment of the online exploration of the databases. It integrates the latest technologies that we have developed to calculate the ice sheet, to extract the functions of the cubic and to gradient analysis, and to make the function of the cubic exploration efficient and efficient. In this presentation, we will show the characteristics of the CubeExplorer, especially its strength and flexibility in exploring and mining the big databases.
Content configuration algorithms: an ACDN must determine which applications will be deployed where and when.
We proposed a multi-solution transmission mechanism that allows the various organizational units to transfer and browse the information on the basis of the amount of access, we define the concept of the information content of each organization unit as the sign of its access, the concept of the information content as the basis for the definition of the information content, determining the transmission order of the various units, our mechanism allows the Internet customer to explore more content parts of the web text in advance in order to be able to stop browsing irrelevant documents in advance.
DBCache also contains a cache launch component, taking a backup database chart and SQL queries in the workload, and creating a middle-level database chart for cache.
The seminar was organized by the author of the report as part of the 1998 Computer Support Work Conference (CSCW-98), and was held in Seattle on November 14, 1998, with a seminar of approximately 30 visitors, including invitation presentations, paPer presentations/discussions and panels.
In the next few decades, we presented the COUGAR system, a new distributed data management infrastructure, with the growth of the sensor interconnection and computing capacity, our system is located directly on the sensor nodes and creates an abstract of a single processing nodes without having to concentrate data or calculate.
In this column, I will introduce myself, commentary can be published in this section of the type of software reviews, and encourage other tasks in professional consideration to review the statistical software package.
As WW becomes increasingly popular and powerful, how to search for the database on the web becomes an important research topic.COMMIX, a DB group developed at the University of Beijing (China), is a system to build a very large database, using data from the web page to extract information, integrate and query answers.COMMIX has some innovative features such as ontology-based packaging generated, XML-based information integration, view-based query answers, and QBE-style XML query interface.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
In eBXML, traders cooperate with the same business process and complementary roles, so it is necessary to standardize the business processes, in this case, using the already developed expertise through RosettaNet PIP becomes essential, we show how to create and use eBXML “Dynamic Cooperation” based on RosettaNet PIP, and provide a GUI tool that allows users to build their eBXML business processes by combining RosettaNet PIP graphics.
A database of 250 active errors is collected in the Caribbean and Central America regions to describe the region’s earthquake risk and astronomy as part of the Global Earthquake Model (GEM) Foundation’s Caribbean and Central America risk assessment (CCARA) project.
In this article, we will describe a patria tree-based B tree variable suitable for OLTP. In this variable, each page of the B tree contains a local patria tree instead of the common sequence of keys.
Only a small portion, less than 1%, spends $1149 trillion a year on the promotion of health care, although we have made progress in the development of health care, it is not recognized by any respected scientific group as a mature science.
Therefore, it is important that a database system used for implementing such a database can effectively process operations on such a program. In this article, we will describe one of the operations, combined operations --- the most important operations --- implemented in IBM Informix Extended Parallel Server (XPS).
After the first International Engineering Federal Database System Seminar (EFDBS’97) held in June 1997, the second seminar aims to gather researchers and practitioners interested in various issues related to the development of the Federal Information Systems, extending its scope to the databases and non-databases information sources (the change from EFDBS to EFIS reflects this).
Conceptual data models used for complex applications, such as multimedia and space time applications, often lead to big, complex and difficult to understand charts. One of the reasons is that these charts often involve repeated independent, meaningful parts, capturing similar situations and characteristics. By identifying these parts and processing them as a unit, the charts can be simplified, as well as the concept model process. We recommend capturing automatic and meaningful charts that often occur as a model model.
We presented a comprehensive solution that has been closely integrated with a business sharing - no parallel database system optimizer. Our methods use the query optimizer itself, both recommending each table of candidates, which will be favorable to each query on the work load and evaluate the various combinations of these candidates.
In this article, we introduce a new technology to estimate the cost of the user-defined method in the database system. This technology is based on a multi-dimensional diagram. We explain how the system collects statistical data, the database user defines and adds to the system. From these statistics, a multi-dimensional diagram is built.
In this paper, we presented a new theory to create the reduced representativity of data. This method is naturally able to highly effectively estimate the local indicative size of each point, thereby creating the variable reduced representativity of data. This technology has the advantages that it is very adaptive to adjust its representativity, depending on the immediate location of the data point of behavior. A interesting feature of the model technology is that, unlike all other data reduction technologies, the overall compression efficiency increases with the size of the database.
The majority of the features required to support the first class view can be semi-automated if the derivatives between the layers are declarative (e.g., SQL, not Java). we introduce a framework where the rules of dissemination can be defined, allowing flexibility and increasing the view parameters of the specifications, even not the programmer.
This paper describes the Dwarf structure and the Dwarf Cub building algorithms. further optimization to improve the classification and query performance. Currently implemented experiments include the comparison of detailed measurements of the actual and synthetic data sets, with the previously published technologies. Comparison shows that Dwarfs in all of these computing techniques: storage space, creating time, query response time and library updates.
In this article, we will discuss the impact of the application of traditional trading management technologies in a multi-layer architecture in a distributed environment, we will show the performance costs related to distributed transactions, and discuss how really manage their distributed data to avoid this performance breakthrough, our goal is to share our experience with the database research and the supplier community to create easier to use and expandable designs.
Recently, some important relationship database tasks such as index selection, chart adjustment, closer to query processing and statistical choices have recognized the importance of work load. These tasks are often presented with large work load, i.e. a set of SQL DML statements, as input. The key factors affecting the scale of these tasks are the scale of work load. In this article, we introduce new work load compression issues that help improve the scale of these tasks. We introduce a principle to solve this challenging problem. Our solutions are widely applied to a variety of work load driven tasks, while allowing the embedded specific task knowledge.
We have developed a web-based architecture and user interface to quickly store, search and obtain from scientific simulation of large, distributed files. We have demonstrated that the new DATALINK type defined in SQL management of external data standards can help overcome problems related to limited bandwidth when trying to archive large files using the Web. We also show that user interface processing user interface specifications can provide some benefits. We provide a tool to automatically produce default user interface specifications, in the form of XML files, for a database.
In this article, we describe the two potential aspects of the data management system: backup/recovery and data consistency.We introduce the algorithms of file on the file server for backup and recovery of DBMS data.Our coordinated backup and recovery algorithms have been implemented in the IBM DB2/DataLinks products.We also proposed an effective solution to keep the file content and related data stored in the DBMS from the view of the reader, without keeping a long time locked on the data table.
nvited Talk I.- Some advances in data mining technology.- Web exploration.- Worldwide query label documents.- WWW exploration query.- E-mail filter strategy Content base and sociological filter combined with user type.- Interactive query expansion in a meta search engine.- Database technology.- on optimization of query containing conventional path expressions.
Dual Match, recently proposed for FRM, significantly improves performance by using the point filter effect. However, it has a problem with a smaller permissible window size -- half FRM -- to the minimum query length. smaller window adds false warnings due to the window size effect. General Match offers two benefits: it can be reduced by using a large window, such as FRM, while using the point filter effect, such as Dual Match.
Classification is the process of making a set of objects similar to objects. While the definition of similarity from one classification model to another, in most of these models, the concept of similarity is based on a distance, for example, the Oaklid distance or the Isot distance. In other words, similar objects need a close value for at least one set of sizes. In this paper, we explore a more common type of similarity. In the pCluster model we propose, the two objects are similar, if they show a consistent model in a subgroup size. For example, in the DNA micron analysis, the expression level of two genes can rise and fall synchronically in response to a setting of environmental stimulation despite their expression level of intensity, the expression level may not show a larger type of similarity.
The paper proposed a simple model of time-driven start and warning system. This system can be used with a relative and object-related database system. Time-driven start system has many advantages, while the traditional start system, testing the start conditions and running the start action to respond to updates events. They are relatively easy to implement because they can use a intermediate software program that is just running SQL statement on DBMS.
We have developed new type algorithms, eliminating almost all comparisons, providing functional parallelity, which can be used by multiple executive units, significantly reducing the number of channels and improving data location.
For each algorithm, we study the clear performance effects of double removal and reference integrity, the variables of greater input than memory, as well as parallel implementation strategies.
Many social applications, such as healthcare, land use, catastrophic management and environmental monitoring, are increasingly dependent on geographical information for their decisions.
Today, web services are considered the revolution of the next generation of e-commerce, with its technological architectures including UDI, WSDL, SOAP, XML and so on.
In this article, we report our success in building an effective extensible classifier in the form of a decision-making table, by exploring the ability of modern relations database management systems. In addition to high classification accuracy, the unique characteristics of the method include its high training speed, linear extensibility and the simplicity of implementation.
This article introduces the algorithms to identify the unchanged parts of the data flow and reorganize the evaluation plan to reuse the stored intermediate results, and we also recommend an effective way to teach existing merger optimizers to understand the unchanged features, thus enabling them to produce better merger plans in the new background.
This sample describes how a comprehensive database settlement tool is able to describe data quality and data settlement problems in complex reality applications.Telcordia’s data settlement and data quality analysis tools include the rapid generation of appropriate pre-processing and matching rules, suitable for training groups created from data samples.
In this article, we define and review a specific category of queries called group queries. group queries are a natural queries that support many decision-making applications. the main characteristic of group queries is that it can be performed in a group way. In other words, the basic relationships (s) can be divided (based on certain properties) to divisions, each group can be processed separately. We provide a synthetic standard to identify these queries and prove their sufficiency.
There are several alternatives for managing large XML documents, from file systems to relative or other database systems to customized XML database management systems. In this article we introduce Natix, a database management system for storing and processing XML data. Unlike the common belief, XML data management is just another application of the traditional database, such as the relative system, we show almost every component of the database system is affected in terms of suitability and performance.
AQR-Toolkit divides the query to two collaborative processes: query improvement and source selection. it is known that the widespread definition of query necessarily produces many false motivations. query improvement mechanisms to help users submit query, will return more useful results and can be processed effectively. as a complementary process, the source selection is reduced by identifying and finding relevant information providers from a large number of available sources.
We introduced the design of ObjectGlobe, a distributed and open query processor of the Internet data source. Today, the data is published on the Internet through the web server, if in general, there is a very localized query processing capacity. ObjectGlobe project aims to build an open market where the data and query processing capacity can be distributed and used by any type of Internet application.
We are exploring existing value-based methods, developing a reference architecture that helps to compare methods and classify component algorithms. We explain the options to collect value metal data and use these metal data to improve search, ranking results, and improve information browsing. Based on our survey and analysis, we then point out a few open issues.
MENTOR (Middleware for Enterprise-Wide Workflow Management) is a joint project between the University of Salamanca, the Federal Bank of Switzerland and the University of Zurich (1, 2, 3).The project focuses on the entire business workflow management.The workflow category can be divided into several organizational units, each unit has its own workflow server, involving various unusual information systems, requiring thousands of customers to interact with the workflow management system (WFMS).
The popularity of the online document database caused a new problem: finding which text database (many candidates choose) is the most relevant to the user. identifying the relevant database to a problem is the text database finding a problem. The first part of the paper introduces a practical solution based on the estimation of a problem and the size of a database. The method is called the Glossary of Servers Server. The second part of the paper evaluates the effectiveness of the Glossary based on a real user query trace.
In the coming years, an increasingly reliable and increasingly capable Internet will introduce new opportunities to create closely integrated databases distributed in multiple institutions. These new capabilities, as well as some of the technologies produced from the emerging fields of computer finance, can ultimately transform an important part of the world’s business and financial activity into a basic way. This conversation will focus on some of the most important changes that could cause the structure of the world’s financial system and the mechanisms of global trade.
We solved the problem of order access to multiple information sources in order to maximize the possibility of receiving answers as soon as possible.We described the formalism of these types of probability information and presented the algorithms of information source orders.
Recently, some important relationship database tasks such as index selection, chart adjustment, closer to query processing and statistical choices have recognized the importance of work load. These tasks are often presented with large work load, i.e. a set of SQL DML statements, as input. The key factors affecting the scale of these tasks are the scale of work load. In this article, we introduce new work load compression issues that help improve the scale of these tasks. We introduce a principle to solve this challenging problem. Our solutions are widely applied to a variety of work load driven tasks, while allowing the embedded specific task knowledge.
Relative OLAP tools and other database applications produce a series of SQL statements, sent to the database server as a single information request provided by the user. Unfortunately, these series cannot be effectively processed by the current database system as they are usually optimized and processed each statement in isolation. We proposed a practical method, this optimization issue, called the "margin optimization", supplementing the traditional query optimization phase.
The parameters of the continuous time of the Markov Chain model, the probability of joint access to certain files and the time of interaction between continuous access, are dynamically estimated and adjusted to the evolution of the workload model, by keeping online statistics, the comprehensive policy has been implemented in a prototype system. The system makes the Markov Chain model profitable for use, and also a third stored library timetable.
In the past few years, the way of using computers has occurred at least two significant changes. the first has the fact of its origin, computers are increasingly connected to each other. the second is caused by the increasing reduction and availability of hardware components and power supply, as well as the development of wireless communication routes. the two trends combine strong, but relatively low, portable computer development. despite these changes, it is rarely noticed to reach consensus and develop strong infrastructure in this field.
There seems to be disagreements between the research topic that the database research community is pursuing and the key issues that information systems decision makers (such as chief information officials) face.
Morgan Kaufmann Publishers is a global exclusive distributor listed in the VLDB file: ISBN 2000 Cairo, Egypt 1-55860-715-3 1999 Edinburgh, Scotland 1-55860-615-7 1998 New York, United States 1-55860-566-5 1997 Athens, Greece 1-55860-470-7 1996 Mumbai, India 1-55860-382-4 1995 Zurich 1-55860-379-4 1994 San Diego, Chile 1-55860-153-8 1993 Dublin, Ireland 1-55860-152-X 1992 Vancouver, Canada 1-55860-151-1 1991 Barcelona, Spain 1-55860-150-390 Brisbane, Australia 1-55860-160-549 1989 X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X
We show that in this case, there may be several maximum acceptable subgroups, and all maximum acceptable subgroups can be described as a PRA’s 3 value stable model, we show that in the user’s request a specific group of reference actions, the presence of acceptable check and calculate the subsequent database status, as well as the derivatives of (for unacceptable updates) leakings, all in ptime, therefore, the complete reference action can be effectively implemented.
Unfortunately, there are few funds to support the program’s health promotion initiative. health promotion has received a small amount of $1.7 billion in NIH research budget, and a small amount of health promotion programs are spent $40 billion a year on Medicare and Medicaid.
The paper discusses the concepts of LHAM, including comparative control and recovery, our complete implementation of LHAM, as well as the results of the implementation of the experimental performance. The detailed comparison, analysis and practical implementation of the TSB Tree suggests that LHAM is very excellent in input performance, while query performance is at least as good as the TSB Tree in almost all cases; in many cases it is better.
We describe the TIGUKAT object basic management system being developed at the University of Alberta.TIGUKAT has a new object model, its identification features include pure behavioral parameters and unified methods of objects.Everything in the system, including type, category, collection, behavioral and functional information, is the first class of objects that define behavior.Therefore, the model abstract everything, including traditional structural concepts, such as example variables, method implementation and chart definition, to the unified parameters of objects behavior.
This paper describes the specific index structure of linear optimization query, linear optimization query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query
WALRUS uses a new similarity model, in which each image first disintegrates into its area, then the image's similarity measurement between a pair of images is defined as a part of the image's two areas covered by the similar area from the image. To extract the image's area, WALRUS considers different sizes of slides and then group them according to their signature.
In this short article, I described some methods that university courses in database systems can become more efficient and the time of the staff is more productive. These ideas are transferred to other programming-oriented courses, many of which apply to any academic topic.
In terms of meeting human information needs, the current digital library (DL) systems suffer from the following two disadvantages: (i) lack of high-level cognitive support; (ii) lack of knowledge sharing facilities.
In this article, we propose the transformation of the intuition of the Alien: instead of calculating the Alien while retaining its arguments, the Alien delivered from the Alien or the Alien from the Alien and the Alien from the Alien from one of the Alien, one of which is the Triangle Alien. One of the advantages of this view is that, in contrast to the retention, the separation is the exchange and cooperation, which is a significant attribute to the creation of the intuition, formalism and implementation of the plan.
This article describes an incomplete database design. an incomplete database is simulated as a cube alliance. an incomplete cube is a complete subcube within an incomplete database. an incomplete cube is built by providing a short high level of specifications for each cube. an effective algorithm to obtain the total value from an incomplete database is described.
Complex database queries require the use of memory-intensive operators, such as type and mixed. These operators need memory, also known as SQL memory, to process their input data. For example, type operators use a work area to make memory type a set of lines. These operators allocate memory significantly affect their performance. However, only a final number of memory can be shared in the system by all current operators.
The external merger time is usually dominated by I/O time. This paper focuses on reducing I/O time in the merger phase. it proposes three new bubbles and readahead strategies, known as equal bubbles, extending predictions and classifications. they take advantage of the fact that almost all modern disks are cache and serial read. The latter two also gathered information during the running formation (the last key of each running block) and then used for budget reading.
We also describe an adapted page sample algorithm to higher efficiency by using all the values in a sample page, but adjust the number of samples according to the values in the page.
Different relationships based on information processing systems, involving quality representation and processing space knowledge, have been developed in many applications. In this article, we identify the common concepts of quality space knowledge representation, we compared the representative properties of different systems and listed the computing tasks involving the relationship based on space information processing. We also describe the symbolic space index, the relationship based on structure, combining several ideas of space knowledge representation.
One objective of the Biomedical Informatics Research Network (BIRN) project is to develop a multi-institutional information management system for neuroscience to gain a deeper understanding of several neurological barriers. Each institution specializes in engaging in different disciplines and producing its experimental or calculated derived data base; a median module performs a neutral integration in the database, allowing neuroscientists to analyze that cannot be done from any institution's data.
E-commerce sites are increasingly using dynamic web pages as they can provide a wider range of interaction than static HTML pages.Dynamic web site creation technology allows a site to create a page in running time, based on different parameters.Delay content decisions until running time a site represents the feasibility of customized page content, thus enriching the user's web experience.
Data mining develops to a gathering of application problems and effective solutions to relatively special issues, all focusing on finding the relevant information hidden in the database’s huge size. in particular, one of the most studied topics is found by the association rules. this work proposed a unified model that allows a unified description of the association rules. the model provides SQL similar operators, known as the MINE rules, able to express all the problems, so far in literature about the association rules. We are through several examples, some of which are classic, while the other parts are completely original and fresh applications.
Application servers (ASs), these servers have become very popular in the past few years, providing the online world of transactions, the platform for server-side applications.ASs are the modern couples of traditional transaction processing displays (TPMs) such as CICS. In this tutorial, I will provide an introduction to different ASs and their technologies.ASs in allowing e-commerce to play a key role in the web background.They are based on more standardized protocols and APIs than traditional TPMs.The appearance of Java, XML and OMG standards plays an important role in this regard.
This paper formally develops a rule-based policy framework, which includes terms and obligations, and studies the rationalization mechanisms within the framework. policy decisions can be supported by multiple derivatives, each associated with potential different terms and obligations portfolios (known as the global PO portfolio). rationalization mechanisms can produce all the global PO portfolios for each specific policy decision and promote the digital weight allocated according to terms and obligations, as well as the best choice of ethnic relations between them.
We emphasize some pleasure of using Java encoding, as well as some pain of encoding around Java in order to get good performance on data-intense servers. For those with painful issues, we propose specific suggestions to develop the Java interface to better adapt to the serious software system development. We believe these experiences can provide insights to other designers to avoid the hostages we encounter and decide whether Java is suitable for their system platforms.
We study the optimization and evaluation of queries with universal quantitative in the background of the object-oriented and object-related data model. queries are divided into 16 categories, according to the so-called range and quantum forecasts. for the three most important categories, we list the well-known queries evaluation plans and submit some new. These alternative plans are mainly based on the counterparts, divisions, general portfolio and calculation collections, and set the differences. to evaluate the quality of many different evaluation plans, a thorough performance analysis was carried out on certain sample database configurations.
In this article, we set a series of optimization results for the existing encoding program; in particular, we prove that no one of the two known programs is the best for the bilateral range query class. we also proposed a new encoding program and proved it is the best of the class. Finally, we presented a experimental study, comparing the performance of the new encoding program with the performance of the existing program, as well as four mixed encoding programs, for a simple choice of query and a more common form of membership query.
As you may know, people have searched hundreds of times for their chosen books, such as this basic object relationship database third declaration, but linked to harmful download.
The results show that the current hardware technology trends have significantly changed the performance weaknesses considered by the Institute in the past, the new results-based simplified data configuration strategies are being developed and proven to be good on various work loads.
BeSS is a high-performance, memory map object storage manager, providing distributed transaction management facilities and sustainable support, in this article we introduce an overview of the object architecture and discuss issues related to space management, inter-object reference, database corruption, operating patterns, cache replacement and transaction management.
We presented the state updates of the Cougar Sensor Database project in which we are studying the database methods of the sensor networks: the customer "programming" sensors through the advanced declaration language (such as the SQL variables). in this article we provide an overview of the activity of energy efficiency data dissemination and query processing. due to space limitations, we are unable to provide a complete result menu; instead, we decide to only put the reader's appetite and energy efficiency routes and the network integration of some issues, and how to approach them some ideas.
We presented the prototype of the information mediator called Kind, which has recently been developed as part of the SDSC/UCSD Integrated Neuroscience Working Group project within the NPACI project. The broad goal of the work group is as a environment in which, in other tasks, neuroscientists can ask the mediator to obtain information from multiple information sources and use the results for their own data analysis.
A ∈ approximate quantum summary of a sequence of N elements is a data structure that can respond to the sequence of quantum queries to a precise N. We introduce a new online algorithm calculating ∈ approximate quantum summary of a very big data sequence. the algorithm has the worst situation space requirements of &Ogr; (1 ÷ ∈ log(∈ N)). This improves the previous best results of &Ogr; (1 ÷ ∈ log2 (∈ N)).
The democratization of virtual computing, the increasing connection of corporate databases to the internet, as well as the natural resources of today’s web hosting companies and databases service providers, emphasizes the need for data privacy. The chapter proposes a solution called C-SDA, which allows for encrypted data queries while controlling personal privileges.
PointCast Inc., an inventor and leader of internet broadcasting news, was founded in 1992 to transmit directly to the audience computer screens from leading sources such as CNN, New York Times, Wall Street Journal Interactive.
In response to the pressure reduced product lead time, manufacturing companies are increasingly aware of the need for some form of integration throughout the product chain. Engineering tasks must coordinate and exchange data with a variety of special tools. A company has two main information flow routes, that is, technology and management, and product data management routes two routes. On the technical routes, the application is highly specialized in support tasks, such as product design (CAD) and programming of digital control machines (CAM).
Some researchers are interested in the design of global network systems and applications. Our paper here is that the principles and technologies of the database community play an important role in the design of these systems. The starting point is the root of the database research: we convert the concept of data independence into the physical environment outside the storage system. We notice the similarities between the development of the database index and the new generation of structured to the right network. We observed through the database lens a series of recent network facilities and applications to describe the appearance of data independence in the network.
The CQ project, funded by DARPA, aims to develop an extensive toolkit and technology for up-to-date monitoring and event-oriented information delivery on the network.The main characteristics of the CQ project are based on a continuous query of "Personal Update Monitoring" toolkit.In comparison with pure guidance (such as DBMS, various web search engines) and pure guidance (such as Pointcast, Marimba, Broadcast) technologies, the CQ project can be seen as a mixed method of guidance and guidance technology, by supporting personalized updates monitoring, by combining client guidance and server guidance patterns.
Starting in 1989, the advertising gathers of DBMS’s senior researchers gathered regularly for a “group gathers” to evaluate the art status of DBMS’s research, and for predictions on which issues and issues are worth more attention.
Our presentation document consists of two main components: a set of Berkeley TinyOS battery-driven wireless sensors "sensors" (see Figure 1) generating and processing data, as well as a desktop-based query processor that processor distributes queries, distributes these queries, and collects and displays answers.
This paper discusses the problem of finding the closest dialogue between two spatial data sets, each of which is stored in the structure belonging to the R tree family. Five different algorithms (four repetitions and one repetitions) were submitted to solve this problem. The closest 1 pair of cases were considered a special case.
In this article, we introduce an original and comprehensive method to monitor what relationship query processing in a shared system. A new control mechanism was introduced, allowing the detection and correction of the optimizer estimate errors and unbalance of load. We specially focus on the management of communication within the processor, as well as the transition of communication and calculation. Performance assessment in a concrete and network connector shows the efficiency and strength of the recommended method.
In this study, we represent our operating environment in two different ways. first, we describe the basic physical database of the whole Cyprus search system. second, we created a set of rebuilt logical documents that match the high-level organizational conceptual categories, such as jurisdiction, practice area and document type. to keep the memory of the final user, we focus on the performance issues related to the choice of the best database, where the field experts have provided a complete preliminary judgment of the characteristics of each of our physical and logical database models.
In this article, we study genetic format matching algorithms, in addition to any specific data model or application. We first introduced the past solutions, showing that there is a rich range of technology available. Then we presented a new algorithm, Cupid, found a map based on its name, data type, limitation and format structure between the format elements, using a wider technical combination than the past methods. Some of our innovations are language and structure matching integrated use, background depending on shared type matching, as well as a transition to a sheet structure, in which most format content lives after describing our algorithm, we presented the experimental results, comparing Cupid with other two format matching systems.
In this model, relative data can be presented as a fixed depth tree, on this tree, UnQL is equivalent to a relative algorithm.UnQL's innovation is its voluntary depth data and cycle structure programming structure.While strictly compared to query language has a route expression, such as XSQL, UnQL can still be effectively evaluated.We describe the new optimized technology depth or "vertical" UnQL query.
The interactive computer graphics system is useful in evaluating, designing and training in virtual environments, such as those that find the architecture and mechanical CAD, simulation, and virtual reality. The interactive visual system shows the image of the three-dimensional model on the screen of the computer workstation view from the simulation view of the observer control of the user. If the image is smoothly and fast, it can an illusive real-time exploration of the virtual environment by the simulation observer through the model.
This article focuses on wireless queries (also known as queries separation), an optimization, although it significantly improves performance and does not properly process (if in general) the majority of OODB systems. Our framework generalizes many wireless technologies recently presented in literature and is able to remove any form of queries wireless through a very simple and effective algorithm. The simplicity of our method is due to the use of a single understanding of calculation as a medium form of OODB queries.
XQuery is a real and virtual XML file and the collection of these files, its development began in the second half of 1999. completed about 3 years of work, it was a long time we provided the initial description of this language, and it felt in its development cycle. XQuery developed within W3C. Each type of alliance has its own rules and its own way to complete its work.
In this article, we reviewed the repeated problems on the transition-based list. In general, the different sequence of transition rules may eventually produce the same elements, the optimizer must detect and eliminate these repeated elements generated by multiple routes. We show that the common interchangeability/connectivity rules of connection generate the O(4^n) repeated operator. We then propose a plan --- on the transition-based general framework --- to avoid repeated production, to reach the O(3^n) bottom line of the connection list.
This chapter shows the basic functions of the basic system, using the Oracle database installed in the San Diego supercomputer center. the database has expanded to the data type representation surface, which also carries out all the necessary operations. surface objects are specialized in the regional surface objects and flat objects. brain researchers, neurologists and neurosurgeons get the idea of 3D brain images from normal and sick objects, study the characteristics of the brain, compare and measure the changes caused by age and disease factors.
The important scientific data for biologists in the Humitn genome project is not only in the traditional databases, but also in the structured files retained in a variety of different formats (e.g., ASN.1 a.nd ACE) as well as its series analytics packages (e.g., BLAST and FASTA). these formats and packages contain some types of data that do not exist in the traditional databases, such as lists and variables, and may be deeply immersed and immersed.
The nearest query problem occurs in a large number of database applications, usually in the background of similarity search. Late, there is increasing interest in building search/index structure for similarity search high-size data, for example, image databases, document collections, time-series databases, and genome databases. Unfortunately, all known technologies to solve this problem fall on a large-size orbit. that is, the data structure size is poor data size; in fact, if the number of sizes exceeds 10 to 20, search in k-d trees and related structures involves checking a large part of the databases, so it cannot do better image databases, time-series databases and genome databases because it is based on this problem, the data structure is poor, the data structure is poor, the data structure is poor.
We discuss the development of the standardized theory of the object-oriented data model, which supports the common characteristics of the object. We first provide a functional dependency expansion to deal with the relationship between the object with a rich ethnicity, known as the path dependency, local dependency, and global dependency limitations. Using these dependency limitations, we provide the normal form of the object-oriented data model, based on the user interpretation of the concept (user specified dependency limitation) and the object model. Depending on the traditional data model, one of which the standardized object has a unique interpretation, in the object-oriented data model, a object may have many different interpretations, forming the object model will be only in the normal form, if the user's interpretation is based on the object-oriented concept and the object-oriented
This requires a complete e-commerce site package (i.e. edgecaches, web servers, application servers and DBMS) distributed along the white field database copy. The main advantages of this method, such as Caches, the possibility of service dynamic container, from a close to the user location, reduces the network availability.
In this article, we presented a new technology called Structural Function Insert, which includes repeated functions used in queries, by fully exploiting the available type of information.Based on this technology, we developed a new method to write and optimize the structural repeated queries.The new method provides more accurate result types for queries.In addition, it provides the best algorithm expression for queries about the type of information.
The TRAPP system provides accuracy control of the transaction gap between accuracy and performance: the hidden storage range ensures that the current data value is connected rather than the fixed accuracy value. The user provides accurate limitations and each query. To answer a query, the TRAPP system automatically chooses a combination of local hidden limits and accurate main data remote storage to provide a connected answer, by a range, not exceeding the specified accuracy limitations, ensures that the accurate answer is included and calculates as quickly as possible.
This paper proposes an effective key management solution to prevent hardened embedded equipment from side-channel attacks, which uses side-channel bandwidth restrictions and uses an effective update mechanism to prevent key material from being exposed, which forces the attackers to launch more expensive and invasive attacks to prevent embedded equipment and has the potential to defeat unknown semi-invasive side-channel attacks.
Classification results is an important topic in the background of model recognition.We review methods and systems in this context.In the first part of this paper, we introduce classification verification methods based on internal and external standards.In the second part, we present a review of classification verification methods based on relative standards.In addition, we discuss a experimental study results based on widely known effectiveness indicators.
This is a very simple paper that I think contains a lot of ideas that will appear again in a different way every decade! the paper proposes a copy of a dictionary (principally a set of key and value pair) to all the relevant sites in the distributed system.
In the pre-optimization phase, WQO chooses one or more WSIs as pre-planned; pre-planned represents a WSI-based choice of query assessment plan (planned) space.WQO uses the cost-based essence to evaluate the WSI task choice in the pre-planned and choose a good pre-planned.WQO uses pre-planned to drive the extended relationship optimizer to get the best plan of the pre-planned.
In order to facilitate the survey of semi-structure data, proposed various structural summaries, structural summaries obtained directly from the data, and as an assessment indicator for the route expression of semi-structure data or XML data, we presented the D(k) index, the adaptation structure summaries for the overall graphic structure document. Based on the previous work, the 1 index and the A(k) index, the D(k) index is also based on the double concept.
In this article, we analyzed several quantum types to better understand their behavior in practice. the results were challenged by many assumptions behind the quantum base copying. our assessments show that the traditional reading one / writing all available methods are the best choice for a wide range of applications requiring data copying. we believe that this is an important result for anyone to develop a code computing set because reading one / writing all available strategies is easier to implement and more flexible quantum-based methods. in this article, we show, in addition, it is also the best choice to use some other options standards.
In this article, we first introduce the synthesis of the temporary forecast (TP) relationship, and then show how they are converted into a clear, significantly more spatial consumption form, called anonymous relationship. Then we introduce a theoretically registered temporary algorithm (TATA). It is clear that TATA is easy to explain how the algorithm operations should act, but it is useless because the registration relationship is quite large.
This article describes the current INFORMIX IDS/UD release (9.2 or Centaur) and compares its features with the characteristics of the SQL-99 language standard.INFORMIX and Illustra have carried out the spirit of implementing the SQL-99 standard for five years.In this article we review our experience of working with ORDBMS technology and believe that although SQL-99 is a huge improvement to SQL-92, it takes further work to make the objective-related DBMS really useful.
We define the problem of e-commerce content integration and show how it is fundamentally different from the traditional issues around data integration, application integration, data storage and OLTP. Content integration includes directory integration as a special case, but includes wider applications and challenges. We explore the features of content integration and any solution required services.
The archives will allow astronomers to interact with data. data access will be assisted by multi-dimensional space and properties indicators. the data will be divided in many ways. the most popular properties composed of the small label objects will accelerate frequent searches. dividing the data into multiple servers will allow parallel, extensible I/O and parallel data analysis.
In this article, we argue that this method is less than a rare category, because of two issues: dividing the false positive and the wrong small division. Motivation of our two stages of design, we design a variety of synthetic data models to identify and analyze the situation, in two the most advanced methods, RIPPER and C4.5 rules, whether it is unable to learn a model or to learn a very bad model. In all these cases, our two stages methods to learn a model, significantly better memory and accuracy level. We also introduce a comparison of three methods in a challenging real life network of detection of data sets.
Clustering is one of the most important tasks in data mining applications. This article introduces the effective SQL implementation of EM algorithms to do Clustering in a very large database. Our version can effectively process high-size data, a lot of Clusters and more importantly, a lot of data records. We introduce three strategies for implementing EM in SQL: vertical, vertical and mixed one. We expect this work to be useful for data mining programmers and users who want to integrate big data sets in the relative DBMS.
This chapter discusses the Toronto Literary Publishing/Subscribe System. can meet this requirement of intermediate software including the event-based building, such as the publishing subscription system. The pub/sub paradigm has recently gained significant interest in the database community to support information spread applications, other models are obviously unsuitable. In the pub/sub system, customers are independent components, through publishing events and subscription events categories they are interested. In these systems, publishers produce information while subscribers consume it.
In this article, we extended this initial proposal, it only involves the behavior aspects to solve the data aspects. in this context, we show how the process algorithm LOTOS (and its toolbox CADP) can be used to solve this problem.
The CONTROL project in U.C. Berkeley has developed technology to provide data-intensive applications for online behavior. using new query processing algorithms, these technologies are constantly improving estimates and trusting statistics. in addition, they react to user feedback, thus giving users control of long-term running operations. this sample shows the changes and results of the database system on the integrated query, data visualization and GUI widgets. We then compare this interactive behavior with integrated processing alternatives.
The meaning of the mathematical model is extended to the calculation of specific meaning of keywords, which are used to receive images without any doubt and dynamics. The main characteristic of this model is that in the correct sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence
The DataLinks technology developed by the IBM Almaden Research Center can now introduce a new type of data in DB2 UDB 5.2 called the DATALINK database to refer and manage files stored externally.
The number of unstructured text and supertext data is much greater than the number of structured data. Text and supertext are used in digital libraries, product catalogues, comments, news groups, medical reports, customer service reports, etc. Currently measured by billions of dollars, global internet activity is expected to reach billions of dollars by 2002. Database researchers have kept some cautious distance from this action. The tutorial aims to expos database researchers to text and supertext information collection (IR) and mining systems, and discuss the emerging issues in the fields of breakthrough of databases, supertext and data mining.
In addition, the Internet is an open environment, information sources, communication links and agents themselves may appear and disappear unpredictable. Therefore, an effective, automatic search and choice of related services or agents are human users and agents are also essential. We distinguish three general agents categories in the web space, service providers, service requests, and intermediate agents. Service providers provide a kind of service, such as distributing information, or performing certain domain characteristics problems solving.
We provide an overview of the query processing in the parallel database systems and discussed several open issues in the field of parallel machine query optimization.
Different types of computer systems are used behind many parts of the telecommunications network to ensure that it works efficiently and without barriers. These systems are large, complex, expensive real-time computer systems, which are key to the task and contain a database engine as a key component. These systems share some common database problems with traditional applications, but they also show quite unique features presenting challenging database problems.
MapInfo SpatialWare is an integrated space information management system that uses the Universal Data option on the Informix dynamic server as a “space extender” (GA to determine), or as an Oracle space server. It provides online space data services and improves key business processes and operating applications. It allows space data to be stored in RDBMS and quickly accessed through sophisticated RTree index.
In this article, we introduce a completely automatic content-based method for organizing and indexing video data, our method consists of three steps: <ul><li> steps 1: we divide each video into a shot, using camera tracking technology, this process also extracts each shot function vector, which includes two statistical variables.
For the effective assessment of OLAP integrated queries, one of the recently proposed techniques is the use of classification access methods. These methods store the database according to the size of the classification of the facts, using special properties called a series exchange key. In the presence of these access methods, new processing and optimization techniques have been recently introduced. An important this optimization technique, called a series exchange, uses a series exchange key to collect the facts exchange key as soon as possible and avoid conversion connections.
MineSetTM is a highly integrated client server tool for the visualization of high-end mining and very large enterprise databases.MineSet represents a combination of several important software and hardware technologies: data mining algorithms, fast multiple database servers, interactive 3D data visualization new technologies and powerful graphic workstations.MineSet provides integrated facilities for data extraction from multiple sources, data extraction algorithms, and 3D results visualization tools.
The database and the knowledge base are used to represent the relevant parts of the application field and allow easy access to stored information. The knowledge representative study (KR) initially focuses on expressive formalism with complex rational services, usually assuming the size of the knowledge base (KB) is relatively small.
Data Mining sets specific requirements for the performance of the DBMS query and cannot be satisfied by existing OLAP parameters. DD parameters - here defined - provide a practical case and technique to explore the DBMS can support data mining applications. It is produced by our Data SurveyorTM tool in real-time data mining tasks performed on various DBMS backups. We describe the initial results obtained by using the monetary system and relative DBMS products as a backup.
We study the effectiveness of the probability choices of the merger requested evaluation plan, not depending on the tree transformation rule. On the contrary, each candidate plan is randomly selected separately from the space of the effective evaluation command. This leads to a non-transitional strategy where the sequence of the random plan is produced and the plan is compared to their estimated cost. The success of the strategy depends on the proportion of the "good" evaluation plan in the alternative space, the effective random candidate production, and the accurate estimation of their costs. In order to avoid unreliable exploration space, we solved the problem of the open opening of the random, uniformly distributed evaluation command, with a chart for queries. This optimization or in the random choice of the plan is based on the success of the "good" evaluation
The paper introduces a domain-specific language (DSL) to express the business-friendly language and sufficient forms for machine processing. The core characteristic of the DSL is that its grammar uses the business dictionary and business rules (SBVR) standards, which is a model that uses a natural language to define the business grammar model. Our DSL provides a custom editor for business stakeholders, automatically completed, automatically highlighted, content assistant, error processing and model appearance.
Two new algorithms, "Jive joining" and "Slam joining", are the combination of two relationships with a joining index. The algorithm is double: Jive joining the range division input relationships TupleIDs, then processing each division, while Slam joining the form command input relationships TupleIDs, then joining the result. The two algorithms do a single sequence channel through each input relationship, in addition to one by joining the index and two through a temporary file, the size of which is half of the joining index.
We analyze this algorithm and prove it is best when the most specific phrase is "small."We also point out its limitations.We then introduce a new algorithm, binary and advanced algorithm, and prove the worst case complexity limits are beneficial for the general case.Our results use the concept of super graphic conversion.Our analysis shows that a priority algorithm can solve the problem listing super graphic conversion, improving the result known before a special case.
Space Distance Fusion is a relatively new type of operation for space and multimedia database applications. Additional ranking and stop cardiac requirements are often combined with space distance fusion in online query processing or internet search environment. These requirements present new challenges as well as more effective processing space distance fusion query. In this article, we first introduce an effective k distance fusion algorithm using space index such as R tree.
We have proposed a new method designed to deal with separate queries within the measurement space. users provide weight for positive examples; our system "learn" the concept and returns similar objects. our method is different from existing relevant feedback methods, based on Euclidean or Mahalanobis measurement, as it promotes learning even separate, separate models within the vector space, as well as voluntary measurement space.
Traditionally, the optimizer is “programming” to optimize the query according to a set of building programs. However, the optimizer should firmly make its changing environment to produce the most suitable query implementation plan. In order to adaptability, we propose and design an adaptation optimizer with two functions. First, the search space and search strategy optimizer can be adjusted by parameters, allowing the optimizer to choose the most suitable one in the optimization process. Second, the optimizer has the “learning” ability to query, allowing the existing plan to be “optimized” by continuous replacement.
This article describes problems and solutions related to the creation of a corporate product information database and uses the database as the basis for the implementation of an electronic directory. Today, product information is usually managed in the document composition system and communicated on paper. In the new cable world, these processes are undergoing fundamental changes to address the market pressure and the precise, complete and structured presentation of product information.
In this article, we introduce the effective image access system based on the content, using the image's color, structure and shape information to promote the process of access. For effective characteristics extract, we automatically extract the image's color, structure and shape characteristics, using marginal detection, widely used for signal processing and image compression.
In this article, we consider a dynamic self-adjustment method to reorganize a shared system. We introduced a new index-based method to make the data fast and efficient migration easy. Our solutions include a global highly balanced structure and load tracking at different levels of accuracy. We conducted a wide range of performance studies and implemented methods on Fujitsu AP3000 machines.
The paper describes an effective optimistic competition control system used for distributed databases in which objects are hidden and manipulated on the client machine, while permanent storage and transaction support is provided by the server. The system provides both sequence and external consistency for transactions; it uses smoothly synchronized clock to global sequence. it saves only a single version of each object and avoids keeping any competition control information on the object basis; instead, it tracks the latest faults on a client basis, a method with low memory space and no object disk top.
The aim of model management is to reduce the programming required for the development of a data-intensive applications. We introduce the first complete prototype of a genetic model management system, in which advanced operators are used for map operations between models and models. We define the key conceptual structures: models, models and selectors, and describe their use and implementation. We define the known model management operators to apply to these structures, presented new and developed new algorithms to implement individual operators.
In this article, we carefully defined a query cost framework that includes options and cost estimates.We developed an algorithm called predictive migration and proved it to be the best plan for query by expensive methods.We then described the implementation of our predictive migration in the business relationship database system Illustra and discussed the actual issues affecting our previous assumptions.We will compare predictive migration with a variety of simplified optimization technologies and prove that predictive migration is the best overall solution so far.
Active database systems are now being widely used. However, in these systems the use of the launchers is difficult because of the complex interaction between the launchers, transactions and applications. Rules of repeated calculations may lead to the cost of underestimated calculations in the rule conditions and actions. In this article, we focus on the active relationship database system that supports the SQL launchers. In this context, we offer a powerful and comprehensive solution to eliminate the underestimated calculations of the SQL launchers when they are expensive. We define a model to describe the program, the rules and their interaction.
The subject of the paper is to promote non-synchronous transactions, we discuss our experience of synchronous transactions on the large distribution production systems of Boeing, due to the poor performance of synchronous transactions in our environment, it encourages the exploration of non-synchronous transactions as an alternative solution, which introduces the requirements and benefits/limits of non-synchronous transactions.
The response time is a key point of exclusion between the e-commerce (e-commerce) sites. at special events or summaries, the main site’s ski and slow decline indicate an increase in the size of the e-commerce sites. this slow response time and decline time can cause devastating to the e-commerce sites, as Zona Research noted in a recent study, the relationship between the time of downloading of the web and the user’s departure rate. the study shows that only 2% of users will leave a site (i.e. the departure rate) if the download time is less than 7 seconds.
Our paper solves this problem by submitting MV3R trees, using multi-version B trees and 3D trees’ concepts, with a wide range of experiments demonstrating that MV3R trees with specialized structures are favorable to time and interval window queries, both time and space requirements.
In this introduction, we will describe a series of new object relationship features that have been added to the IBM’s DB2 Universal Database (UDB) system. The features to describe include supporting structured types, object references and charts and signs. These features will be covered from the point of view of the database designer or end user. In addition to introducing the features currently available in DB2 UDB V5.2, we will also discuss the expected progress and impact of this technology.
“Peer-to-peer” systems such as Napster and Gnutella have recently become popular for information sharing. In this article, we study the design of extensible P2P systems related problems and vulnerabilities. We focus on a subgroup called “mixed” P2P systems, some of which are still concentrated. (in Napster, for example, index concentrated, file exchange distributed.) We simulate a file sharing application, develop a probability model to describe the query behavior and expected query results size. We also develop a analytical model to describe the system performance using the experimental data collected from running, openly available mixed P2P systems, we verified two models.
We describe DataSplash, a direct operating system to create a series of graphs (relative) of data.DataSplash contributes to the construction of three key areas of these views. First, DataSplash helps the user graphically specify the visual appearance of the object group. Second, the system helps the user visually programming the appearance of the object group as the user browses the view.
In this article, we introduce DynaMat, a system that manages the dynamic collection of substantial integrated views in the database. During the query, DynaMat uses a special disc space to store computing integrations, which are more dedicated to answering new queries. queries are independently performed, or can be combined in multiple queries expressions. In the latter, we introduce a implementation mechanism, using the dependence between queries and substantial settings to further optimize its implementation.
It processes semi-structure data, XML graphic language and traditional database models. The system is based on a "model" method, in this sense, it knows a set of form structures and allows to define the model by involving form structures.
In this article, we describe an open market using the workflow technology and the current data exchange and generic data representation standards that appear on the network. In this market architecture, e-commerce is achieved through the adaptable workflow template provided to its users. There is a workflow template for the e-commerce process, the result is based on the architecture of the components in which the components can be an agent (buy and sell) as well as the existing applications referred to by the workflow.
In Semantic Web, the data necessarily comes from many different ontologies, information processing in ontologies is impossible, knowing between them Semantic maps. Manual finding such maps is boring, wrong, obviously impossible Web scale. Therefore, the development tool to help in ontology map process is key to success in Semantic Web. We describe GLUE, a system using machine learning technology to find such maps.
The majority of SQL-based XML providers are supported by interaction rather than integration. One of the reasons is that XML is the internal sequence and SQL is the assumption no. This paper shows how ANSI SQL with its relative Carthage product model can naturally perform complete and flexible sequence query processing. With this ANSI SQL internal sequence processing ability, original XML data can be fully and smoothly integrated into SQL processing and run in a complete sequence layer. This paper will describe the basic stages of involvement in this sequence of SQL processing: sequence data model, sequence work group created, and sequence sequence product processing. These processes make a complete sequence of XML, heritage and data integrity, while the integrity of SIAN data can even be fully integrated into SQL processing.
The bubble pool administrator is the core component of ADABAS, a high-performance expandable database system that is used for OLTP processing. The bubble pool administrator’s high efficiency and scalability is necessary on all supported platforms. For maximum parallelity, without dealing with the risk of death, use multiple versions of the lock method. The division of the central data structure is another key to performance. The variable page size allows flexible adjustment, but makes the bubble pool logic more complex, especially in the parallel aspect.
One option is to use TCPdump to monitor network ports and user-level applications to process data. although this method is very flexible, it is not fast enough to process gigabit speed on cheap devices. another method is to use network monitoring devices. although these devices are able to monitor high speed, they are unflexible because the combination of monitoring tasks is predefined. adding new features is expensive, there is a long lead time. similar method is to use monitoring tools built on routers such as SNMP, RMON, or NetFlow. these tools have similar features --- fast but unflexible. other problems with all these tools are their lack of query interface.
Graphic databases have caused a huge interest in the past few years because of their wide range of potential applications (e.g., social networks, biomedical networks, data from the networks). However, many published data suffer quality problems, graphic data is not an exception. In this article, we study the processing of graphic databases quality information issues in the time of query. A framework is provided to enable it to introduce confusing quality preferences Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic
Despite the importance of space networks in real-world applications, most space databases literature focuses on the OCR space. In this paper, we presented an integrated network and OCR information building, capturing pragmatic boundaries. Based on this building, we developed an OCR restriction and network extension framework, using location and connectivity to effectively scan search space. These frameworks successfully apply to the most popular space queries, i.e. the closest neighbors, range search, the closest couples and electronic distance connection, in the background of the space network database.
Ultra high storage density up to 1 Tb/in2. or more can be written, read, and deleted data in a very thin polymer film. The thermal scan is based on the test data storage concept, internally called "millipede", combined with ultra high density, small shape factor and high data rate. High data rate is achieved by the same operation with thousands of micromolecules / nanomolecules / suggestion, which can be composed of silicone surface micration technology.
We propose and evaluate two indexing plans to improve the efficiency of data collection in high-dimensional databases that are new, search keys may contain the lack of properties. the first is a multi-dimensional indexing structure known as Bitstring Increased R Tree (BR Tree), while the second includes a multi-dimensional one property (MOSAIC) index. our results show that the two programs can exceed the comprehensive search.
The multi-dimensional cross-trade association rules extend the traditional association rules to describe the more common associations in projects with multi-properties in transactions. “After the opening of a branch of McDonald’s and King Berg, the KFC will open a branch in two months, a mile away” is an example of such rules. Because the potential cross-trade association rules are often very large, the mining cross-trade association is more challenging than the mining traditional cross-trade association. In order to make this association rules mining really practical and calculable, in this study we present a template model to help users announce interesting cross-trade associations for mining.
We introduced a new algorithm (BOAT) for the decision-making tree building, improving the previous algorithms in terms of performance and functionality.BOAT built several levels of the tree in only two scans on the training database, resulting in an average improvement of 300% in performance than in previous work.
We offer a continuously adapted, continuous query (CACQ) implementation, based on a close query processing framework, we show our design provides a significant performance advantage of continuous query assessment of existing methods, not only due to its adaptability, but due to the work and space of aggressive cross query sharing, it allows.
Many DBMS suppliers have implemented the ANSI standard SQL isolation level of transaction processing, which creates a gap between the database practice and the transaction processing text accounts, which simply match the isolation with regularity.
In this article, we present the visualization of the World Wide Web Document Network Parts, we describe how we use the Hy+ visualization system to visualize the World Wide Web Parts that are explored during the browsing, when the user browsing, the web browser transmits the URL and title of each document and all the tracks contained in the document.
In online data exchange, the view of XML on relationship data usually requires compliance with the predefined DTD. The presence of return in DTD and non-defined make it difficult to produce DTD-oriented, effective conversion. Our framework provides a language to define the view, ensuring that it complies with DTD, as well as the intermediate software to evaluate these views. It is based on a new concept of properties translation (ATG). an ATG through SQL query to expand a DTD.
Workload information has been proven to be an important component of the database management tasks, as well as the analysis of the query logs to understand the user's behavior and system use. These tasks require the ability to summarize the large SQL workload. In this article, we identify many important workload summarization tasks important primary factors. These primary factors also seem useful in various practical scenarios beyond the workload summarization. Today SQL is not enough to easily express these primary factors.
In this article, we introduce a new online reorganization algorithm that shows binary index updates and converts it to transactions with users. In addition to a significant decrease in total I/O costs, the algorithm also ensures that almost all databases are available at any time and reorganizable and reorganizable.
This article focuses on an object-oriented database (ODB), providing a method designed to support the designers in building the right ODB chart. The first necessary condition for the chart is the lack of contradictions. The second reason for the chart is due to the structural repeated type of existence, when defined in certain levels of chart, leading to the end of the heritage process. In the chart, the formal definition of a correct chart, two chart theoretical methods, designed to verify the accurate analysis of the ODB chart. Although the first method is intuitive, but not effective, the second allows the chart accuracy in the multi-time, in the chart size to be checked.
In the relative model, the order of the query data will not affect the accuracy of the query. This flexibility is used in the query optimization through static rearrangement of data access. However, once the query is optimized, it is performed in a fixed order in most systems, the result is that the query data is performed in a fixed order. Only a limited running time rearrangement form can be provided by the lower-level equipment administrator. More aggressive rearrangement strategy is essential in the scenario, data objects access delays are broad and dynamic, as in third-party devices.
The future has enormous challenges to real-world systems: understanding and improving the performance of the DSS, Web and other applications workload, optimizing response time, dealing with VLDB, main memory databases and other extreme environments, dealing with the automatic parallelity of available hardware, dealing with NUMA, group and other new hardware architectures, and ultimately building self-configured high-performance servers.
Database systems have achieved a huge market as they serve many applications very well - trading processing starts and then decision-making supports. Today, in some parts of e-commerce over 200% of accumulated growth rates, it is clear that this new category of applications will be a powerful driver of the database growth, commercially, as well as from the research perspective.
In this article, we suggest APEX, an adaptive route index for XML data.APEX does not keep all the routes from the root and uses the common route to improve the query performance.APEX also has a good feature that can be constantly updated according to the changes in the query work load.
In this article, we discussed a path of index configuration, we first summarized some basic concepts and introduced the concept of a path of index configuration, then we introduced the cost formula to assess the cost of different configurations, and finally, we introduced the algorithm to determine the best configuration and show its accuracy.
In this article, we propose a problem-solving technology by making a single query on the whole input section, therefore, according to the query and data set characteristics, the cost may decrease due to the size of the order, we also propose an analytical model to the expected production, as well as the cost of query processing, and extend the technology to a variety of changes in the problem.
I am pleased to write to you for the first time in the press release as the chairman of TCN and I wish to take advantage of this opportunity to introduce you to the newly elected officials from 2016 to 2018.
Since the Web was originally not designed to support these applications, the Web application development is becoming increasingly a limit to the basic Web infrastructure, and if the Web will be the basis of complex business applications, it must provide the same universal capacity that the OMA provides (although these applications may need to adapt to the more open, more flexible Web nature, and the specific requirements of the Web application).
The development of the site does not require any code writing activity: based on the results of the design process, the system automatically creates the application to implement the site, static and/or dynamic, if necessary; in addition, the system does not depend on any specific tool or language: it has a modular architecture, integrating the external server for a specific task; ultimately, the system supports the site administrators for various maintenance activities that may involve the change of the site at different levels.
The performance of the data parallel algorithm is used for space operations, using the data parallel variables of the Buck PMR four trees, R trees, and R+ trees space data structure is comparable. The operation of the study is the data structure built, diversified, and space combined in an application field, composed of the parallel line division data. The algorithm is implemented by scanning the parallel calculation model on the hypercube architecture of the connector. The experimental results show that the Buck PMR four trees exceed the R trees and R+ trees. This is mainly because the Buck PMR four trees generate a regular split space, while the Rtree and the Rtree do not.
In this speech we will describe the availability challenges faced by large distribution companies and we will also discuss what the IBM’s DB2 universal database is doing to solve these complex problems.
Multi-layer infrastructure has become a common practice for the implementation of high-capacity websites. These infrastructures usually include TCP load balancers, HTTP servers, application servers, transaction processing displays and databases. Caching has been widely used in different layers of infrastructure to increase the size and response time of e-commerce applications. Most existing caching mechanisms are only aimed at static HTML pages or page fragments. However, as web applications become more dynamic by increasing personalization, these caching technologies become less useful.
General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General General
The amazing clean and beautiful program puts the "top" query to optimize the world I assume is the only possibility. in fact, this article is about questioning the suggestive assumption behind the classic query optimization.
This assumption is correct, but only to one point: the size structure of the different databases in different organizations may (and should be) different. but at the same time, developers should understand the methodological principles of certain size models, these principles are common and do not differ from the project to the project and the company to the company.
We have implemented our algorithms and tested it on synthetic and real-world data. Reported some experimental results showing the effectiveness of our algorithms in terms of maximizing performance, and also showing that in specific circumstances of accurate encryption, our algorithms work as well as previous algorithms.
In this article, we consider a very common form of semi-structure data based on labels, oriented charts. We show that these data can use the largest fixed point score of unit data programming. We introduce an algorithm of close-range charts of semi-structure data. We determined that finding the best charts of the overall problem is NP - hard, but proposed some based on classification structures and techniques, allowing effective and almost the best treatment of problems. We also introduce some initial experimental results.
In comparison, we consider expanding the existing interactive binary integration operators alternative to deal with more than two entries. to this goal, we completed a multi-way integration operators prototype implementation, we call it "MJoin" operators, and explore its performance. Our results show that in many cases, MJoin produces output than any other binary operators.
Fibonacci is an objectively-oriented database programming language, characterized by static and powerful writing, as well as new database simulation mechanisms for objective roles, categories and associations.
Our goal is to understand recovery.We define an installation chart operations in an execution, an order significantly weak than the conflict of orders from the competitor control.The installation chart explains the recovery system state, in which operations are considered to be installed in the sense.This interpretation and recovery during the operations are composed of an unchanged, which is normal operation and recovery between the contract.It regulates how to coordinate the system components of changes, such as state, logs and storage.We also describe how the widespread use of recovery technology is simulated in our theory, why they successfully provide recovery.
The real estate management database system is being developed in Northern Ireland’s Housing Administration (NIHE) is a large relationship database system. The application system has a high expected trading processing rate of approximately 37000 transactions per day (the majority of which access mixed tables) from approximately 250 online users.
In many database applications, a common query is to find a query element close distance matching from the collection of data elements. For example, considering the image database, you may want to get all images similar to the data query image. Distance-based index structure applies to data fields of high size, or the distance function used to calculate the data object is not Euclidean. In this article, we introduce a distance-based index structure, called a multi-benefit point (mvp) tree, used for similarity query on a high size measurement space.
In this article, we show a rich market-specific service, such as automatic detection of the required services, comparison of purchases and negotiations, can be provided to market participants by introducing a market as an eCo business. For this purpose, a previously developed market, namely MOPPET, becomes eCo suitable.
An important contributor to the SIGMOD and PODS Awards Honour Database, the prize was submitted at the SIGMOD/PODS annual meeting, for those who missed this introduction at the 2001 meeting, this article reports the winners of this year.
In this article, we extend these results to the influence of including updates.We first presented several alternative updates models and reviewed the basic gap that occurs between the data currency and performance.Then we proposed and analyzed the mechanisms of implementing these different models.Performance results show that even in a model, the performance of the broadcasting disc technology can be achieved quite rich by using simple technologies to spread and predict data elements.
We refer to the need for water labelling database relationships to prevent data theft, identify the characteristics of the water labelling, and describe the characteristics required for a water labelling system's relationship data. We then introduce an effective water labelling technology that is related to data. This technology ensures that certain characteristics of the point location contain specific values.
As a database system magazine, it is committed to the advanced academic contribution to the international publication of the information system architecture, the impact of emerging technologies on the information system, as well as the development of new applications. it has made significant advances in the design, implementation and evaluation of the database and other information collection systems its range ranges from the development of dedicated hardware, the design of innovative software methods, the integrated system architecture, the design analysis of the system and performance evaluation to the introduction and capture of new technologies.
The database application engineering laboratory (LIBD) is committed to developing models, technologies, methods and tools to support all engineering activities related to the database and its applications. It also develops materials and activities that transfer data base knowledge to the industry. The report describes the main activities of the laboratory in the past decade. It first discusses the overall resources and processes that make up the basis of other research activities. The latter will be divided into reverse engineering, interactiveness, advanced processes and CASE technologies.
The main objective of the paper is to suppress this deficiency whileining the solid accuracy of the 2LSR implementation, we recommend accurate defining the value dependency concepts and managing them in order to avoid imposing LDP assets.
In this article, we introduce the application Manifold system, which aims to simplify the integration and customization of e-commerce applications. The work range of this paper is limited to the network-available e-commerce applications. We do not support the integration / customization of ownership / heritage applications. The application as a network service packaging is our work supplement. Based on the emerging Web data standards, XML and application simulation standards, UML, the system provides a new statement specification language to describe integrated / customized tasks, supporting a modular method where new applications can be added and integrated with the minimum effort.
In a image database capturing a sequence, space, temporary and evolutionary sequence, we introduced a sequence data model that simulates the user's conceptual views of image content, providing a pre-treated framework and guidelines to extract image characteristics. Based on the model structure, it provides the ability to directly manipulate the image object space evolutionary query language (SEQL). through the sequence information captured in the model, space evolutionary query is effectively answered.
Wireless and mobile computing have made significant progress over the last decade. In particular, the challenges we are now facing are the spontaneous creation of wireless self-organized networks such as ad hoc, interference tolerance, sensors and wireless networks. These spontaneous self-organized networks have been the focus of intense research activities over the past few years. Spontaneous networks come from the collaboration of mobile devices, in an ad hoc mode, without the need for previous infrastructure. In this context, the key point for dual research and reality applications is to understand how mobility (device, user and application) influences the practical aspects of the network.
The contribution claims that the electronic market can be as a powerful mechanism that allows suppliers to identify their customer base and provide customer-oriented, high-quality and economic services, and guide customers with more focus and price awareness behavior. The paper claims that it should be true to provide and access scientific literature, where the tradition has so far been mainly free customer access and untransparent cost accounting and services to purchase the university library. We report a project to develop a technical network infrastructure, allowing more cost transparent access to the university literature users, and try to add a competitive element library service.
The paper is based on the extension of 2PL, which allows the rule to be more relaxed than 2PL, but the extension of 2PL also imposes some rules that allow certain valid timetables (existing in VSR and CSR) passed by AL, which proposes a multi-version of AL to solve this problem, the report also discusses the relationship or comparison between different protocols, such as MAL and MV2PL, MAL and AL, MAL and 2PL, etc.
Our algorithms (hms use several models pa.rameters for each triple memory device, so designed to move through a variety of tert,ia.ry memory devices and da,tnhase t,ypes.
In this article, we first discuss the current practices and trends of component-based e-commerce based on international component-based e-commerce seminars.
In matching the problem, taking into account a pattern, a set of data objects and a distance measurement, we find the distance between the pattern and one or more data objects. in finding the problem, according to the contrast of the objects, taking into account a object, a measurement, and a distance, we find a pattern that matches many of these objects in a given distance.
In the IoT era, many things are connected effectively to integrate the physical world into the information space, one of the applications of the city’s IoT is the intelligent network, smart buildings, smart houses, vehicles, electric networks, power plants, and other components to connect to each other in an efficient way to provide services.
This paper describes XDB-IPG, an open and extensible database architecture that supports effective and flexible integrated unusual and distributed information resources.XDB-IPG provides a new "no-programmed" database method using a document-centered XML database map that allows structured, non-structured and semi-structured information to be integrated without the need for document charts or translations.
Gifford’s basic Quorum Consensus data copying algorithm is universal to contain flow transactions and transactions failures (abortion), the formal description of the algorithm is using the new Lynch-Merritt input output automatic model for flow transactions.
In this article, we model and verify a data administrator, whose algorithm is based on ARIES. Work using I/O automation method as a formal model, the accuracy definition is defined on the interface between the calendar and the data administrator.
Regular space queries are usually meaningless in a dynamic environment, as their results may be damaged after queries or data objects move. In this paper, we have developed two new types of queries, time parameters and continuous queries that apply to these environments.
The Internet’s fast-growing and interactive protocol supports increasing the number of network-accessible sources, WebSources. The current Wrapper intermediate architecture must expand to a Wrapper cost model (WCM) for WebSources, which can estimate response time (delayed) access sources and other related statistics. In this article, we introduce a Web forecast tool (WebPT), a learning-based tool using query response from WebSources.
In this article, our research goal is to develop a database-virtualized ions technology that allows data analysts or other data mining methods to use all the virtual databases on the Internet in their work as they are recognized as a single database, thereby helping to reduce their workload, such as data collection and data cleaning work.
In this article, we will describe the new teaching time management techniques, covering a variety of topics, from meeting planning advice, e-mail, writing scholarship proposals and teaching.
In a "shared no" parallel computer, each processor has its own memory and disk and processor to communicate through interactive connection. Many academic researchers and some suppliers believe that sharing is no parallel DBMS "consensus" architecture. This assumption consensus is used as a reason for simulating models, algorithms, research prototypes and even marketing activities. We believe that sharing is no longer a consensus hardware architecture, and hardware resource sharing is a bad basis for classing parallel DBMS software architecture, if someone wants to compare parallel DBMS product performance characteristics.
One of the main factors in the expenditure of the query is the length of the conversion list by the query term, which increases with the size of the document collected and is often within a range of many megabyte. To solve this problem, IR and database researchers proposed the calculation or agreement based on the time ranking function without having to scan the complete conversion list.
The ability to optimize queries is considered to be one of the key technologies in relation to the database, is an active field of research and development, and it continues to work on new transformations in SQL, and the problem of building an extended queries optimizer is still attracting a lot of attention, not only in academic research, but also in business development.
The STRUDEL project aims to extend these concepts to the issue of site management, taking into account several tasks required by a site administrator, site administrators often want to manage a single site database, but based on the type of user access to the site, such as external or internal, experts or beginners, the site’s different browsing “view”. In addition, administrators may want to modify the database by editing simple text files or by updating external databases, by manipulating the chart to reorganize the page structure, or by editing HTML files or using WYSIWYHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH
Programming Committee W. Anheier (DE) S. Aunet (NO) M. Baláž (SK) S. Bernard (FR) T. Borejko (PL) D. Borrione (FR) A. Bosio (FR) G. Carlsson (SE) M. Daněk (CZ) A. Dąbrowski (PL) M. Dietrich (DE) R. Domhsler (DE) M. Drutarovský (SK) P. Ellervee (EE) G. Fey (DE) J. Figueras (ES) P. Fišer (CZ) P. Földesy (HU) Függer (AT)
In this article, we explore the performance effects of two options, using two business relations database systems for local implementation, and for a special purpose to convert the list engine. Our performance studies show that although RDBMS is generally not suitable for such queries, but under the conditions they can exceed the list engine. Our analysis further determines two significant reasons for distinguishing the IR and RDBMS implementation performance: using the combined algorithm and hardware storage use. Our results show that, unlike most expected, some modifications in the RDBMS implementation can be more effective in supporting this query class.
In this paper, we introduce a method that supports queries on the Semantic Web.Semantic Associations captures the complex relationships between entities involving the prophecy series, as well as the collection of prophecy series that interact in a complex way.Detection of these associations is the core of many research and analysis activities, which is crucial for the application of national security and business intelligence.This is combined with improving the ability to identify entities in the document, as part of the automatic Semantic Annotation, providing a very powerful ability to analyze a lot of unusual content.
A storage library is a shared database about engineering objects. an object-oriented storage library has many of the same functions as the object-oriented database: properties, relationships and versions. However, the two technologies are different for two reasons. First, the storage library system has built-in information models, these database charts or object models cover general and tool-specific information types. Second, the storage library features are often more functional than similar functions supported by the object-oriented database.
The budget of the Health Promotion and Disease Prevention Office is so small that few health promotion professionals meet directly during their career.The Center for Disease Control and Prevention (CDC) is responsible for coordinating government’s health promotion efforts, but in addition to some of the funding and very important programs in the field of tobacco control, although many employees are strong interests, the CDC does little in promoting health promotion.
GTE SuperPages' yellow page service enables Web users to flexibly connect to more than 17,000 categories of 11 million enterprises, to the required flexibility, it uses the information recycling (IR) engine through complex list of objects search, the object itself is stored in the object database, using the IR engine allows us to create an index that covers all the components of a complex object.
We believe the space time database supports the constantly changing location and range of space objects, known as the mobile object database. We formally define the data base models, which include complex evolutionary space structures, such as linear networks or multi-component areas and holes. Data models as data types and operations collected, can be inserted as property types of any DBMS data models (e.g., relative, or object-oriented), to obtain a complete model and query language. A specific new concept is a sliding representative, representing a time development as a group of units where space types and other data types represent some "simple" time features.
The Software Research and Development Center was established by the Turkish Science and Technology Research Committee (TUBITAK) in October 1991 at the METU Computer Engineering Department. The center's goal is to double: lead large-scale software research and development projects and promote international cooperation. SRDC participates in a series of research and development projects supported by the government, industrial companies and international organizations. Although the SRDC project also covers other fields of computer science, the main focus is on database systems.
In this article, the HMM-based indexing technology was proposed. The new indexing is using the HMM and a new search algorithm to provide the variants of the triangular data structure that match near distance. Each nod in the tree contains a handwritten letter, each letter is represented by HMM. The branch in the triangular based HMM is provided in the game ranking. The new indexing algorithm is parametrated so that through a time-based budget to control the matching quality of the search process. The indexing significantly improves the search time in a handwritten word database. Because this work is aimed at the diversity of the platform, from the personal digital assistant to the desktop computer, we implemented two main memory and disk systems.
In June 1997, a international seminar on the Federal Database Systems Engineering was held in Barcelona, in cooperation with the 9th Conference on Advanced Information Systems Engineering (CAiSE'97).
In this paper, we introduce the formal study of the dynamic multi-dimensional histogram structure of continuous data flow. The core of our recommendation is to use a dynamic summary of the data structure (completely different from the histogram) to keep the data distributed closely. according to the demand, a accurate histogram comes from this dynamic data structure.
Two known indexing methods are reversed files and signature files. We made a detailed comparison in the background of text index, with special attention to query assessment speed and space requirements. We conducted their relative performance review through experimental and sophisticated methods to simulate signature files and show that reversed files are better than signature files. We can not only use reversed files to evaluate the typical query time without signature files, but reversed files require less space and provide greater features.
This article introduces a set of integrated algorithms for the very large compressed databases of OLAPs, these algorithms run directly on the compressed databases, without having to first compress them, these algorithms apply to the data base compressed using various data compressed methods, these algorithms have different performance behaviors, as the function of the database parameters, the output size and the main memory availability, analysis and experimental results show that the algorithms perform better than the traditional integrated algorithms.
In this article, we reviewed presentations in three different fields (weight, medium and light) and provided buffer management and access control algorithms for three fields, we proposed two improvements (slide and dynamic adjustments) for the weight presentations.
SSM uses a global data structure to abstract information available in a multi-database system. This abstract table allows users to use their terms when accessing data (not accurate queries) instead of being forced to use the system-defined terms. The system uses a global data structure to match the user’s terms to the most intimate system terms.
ACRP's Association Trust Committee Chairman tells him how he left the production and research and development a decade ago to serve as a clinical research manager, which is a move that allows him to enter a role he rarely knows because he did not take part in clinical research in advance, if it sounds like a familiar experience to others, the lessons shared in this column emphasizes the importance of the individual's willingness to continue learning, as well as the organization supports learning.
In this column, we reviewed these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answering a question that many of us may ask; and what we can do. Comments by Shoshana Marcus. 2. Parallel calculation elements, by Eric Aubanel. on this important and exciting field of calculation basics. Comments by Michele Amoretti. 3. The 21st Century Theory: Volume 1, by Bogdan Grechuk. background, background, and many important (available) mathematical theory statements in the past twenty years. Comments by William Gasarch.
This paper studies five strategies for storing XML files, including one leaving files in the file system, three using the relative database system, and the other using the object manager.We implement and evaluate each method through a series of XQuery queries.
Conjwzctiue queries are about a relationship database queries and are at the core of a relationship queries language, such as SQL. queries content (and levels) tests appear as part of many advanced queries optimization features, for example, using rnateriahzed view, processing related queries, semantic queries optimization, and global queries optimization.
In the 21st century, the cost-efficiency of computing and the integration of network products, methods and services ultimately allowed more researchers and practitioners than ever to explore the innovative ways of using computer technology management and improving the teaching and learning experience. Recognizing the importance of these trends, this special part requires submission belonging to one or all three major learning fields, i.e. content, methods and technologies, dealing with the above integration in related issues, for example, openness (e.g., source, access and educational resources), online or mixed and collective teaching methods, adapting to learning, data and social information security and information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information information
To solve this problem, we first introduce a collective calculation model, called a division containing division (DIP) calculation model, which is the formal basis of our method. Based on this model, we then introduce a channel collective algorithm. This algorithm calculates using a channel bubble size, which is the lowest bubble size needs to ensure a disk access for each page. We prove our collective algorithm is the best about a channel bubble size under our collective calculation model.
Here we introduce a new vertical mining algorithm called VIPER, which is a common purpose, without any special requirements for the base of data.VIPER storage data in the compressed BitVicts called the "Slang", and integrates a series of new optimizations for effective serpent production, cross, calculation and storage.We analyze the performance of VIPER for a series of synthetic database work load.Our experimental results show significant performance profits, especially for large databases, beyond the previously proposed vertical and horizontal mining algorithm.
We are witnessing the quantity and complexity of digital information that people want to access and analyze. Many of these data are “multimedia” such as images, videos, audio and files. Many other types of complex and semi-structured data are also important, including geographical objects, chemical and biological structures, mathematical entities such as matrix and equations, and financial data such as time sequence. Databaae systems must effectively support the dates of such a rich structure queries, otherwise they will soon become a “route in information superfast”.
The Billkent University’s real-time database system (RTDBSs) project involves all aspects of VAR’s transaction planning in RTDBS. In Darmstadt’s REACH (real-time, active and unusual systems) project, we have been on real-time DBMS engineering with a focus on the ability to transfer predictions and be able to transfer RT protocols to the basic RTOS calendar. We are also struggling to combine active and real-time features.
Exact answers are not always required: DSS applications are usually exploratory: Early feedback helps to identify the “interested” area Combination queries: accuracy to “the last number” does not require, for example, “What is the total sales of the product X in NJ?”
Map to a dimension of value, then using a one-dimensional indexation method has been proposed as a way of indexing multi-dimensional data. Most previously related work using the Z command curve, but recently the Hilbert curve has been considered because it has superior classification characteristics. However, any method is only practical value if there is an effective method to do the range and partially match the query.
The use of views to answer queries is an effective way to answer queries by using a set of previously substantiated views on a database, rather than access to the database relationship issues have recently received major concerns due to their relevance to various data management issues, such as queries optimization, physical data independence maintenance, data integration and data storage.
Global land coverage 2000 databases (GLC2000; Global land coverage 2000 databases, 2003, the European Commission, joint research center; resolution of 1 km) the land use category was later defined with Switzerland data set to collect a piece of land distribution data set across Europe and integrate it into the 7 km COSMO-ART network.
In this framework, the owner publishes a single data example, partially encrypted, and implementes all access control policies. Our contribution includes a declaration of the language access policy, and solves these policies with a logical “protection model” to protect a XML tree key. the data owner carries out a access control policy, by giving the user the key. the model is quite powerful, allowing the data owner to describe the complex access scenarios, and also quite elegant, allowing logical optimization to be described as re-writing rules.
We develop new query processing algorithms that operate directly on the relative table of wavelet-coefficient synopses, allowing us to arbitrarily process complex queries completely within the wavelet-coefficient domain. This guarantees a very fast response time as our near-query execution engine can complete its processing collection in the complex set of wavelet coefficients, basically delayed expansion to the relative double until the final result of the query. We also proposed a new wavelet decomposition algorithm that can build these synopses in an I/O efficiency way.
Because the main DBMS now supports all the expensive user definitions for Boolean queries, we believe that such support for ranking queries will be more important: ranking first queries often require a model of user specific concepts of preferences, relevance, or similarity, which requires dynamic user definitions.
The modern computers are increasingly powerful and continuously opening new applications for advanced data processing, such as engineering and knowledge-based applications. In order to meet their requirements, advanced data management concepts have been researched over the past decade, especially in the field of objective orientation. In recent years, based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based
In the name of the Faculty of Engineering, the University of Alexandria, the computer department of EGYPT, I want to take advantage of this opportunity to acknowledge that during the 2001 SIGMOD, the help of the Santa Barbara and the SIGMOD community, I especially acknowledge the contribution of the SIGMOD members, as well as the donations of the publishers.
On this issue, we briefly discussed the continued insurgents of the NSF, ARPA and HPCC, as well as more clear information about the U.S. National Information Infrastructure Program, and then we described the financing opportunities from the NSF, ARPA, the National Security Agency, the National Automatic Information Research Center, the Air Force and NASA.
In this work, we use the knowledge of the range of relationships of multiple databases to identify that will return to the same results of the site. Then, we proposed a new way to optimize the query, using the conflict of the program in search of the minimum implementation cost. We achieved the goal, first dividing the different program conflict into different types. In the conflict of the program, the cost of performing the same relationship operations is assessed and the weight is allocated to each case to reflect the complexity of performing operations. Because this method only involves the simple weight calculation and query execution time savings can be dramatic, here the method of development can be considered an effective way to optimize the query processing of the multi-databases environment.
However, on the background of memory search, we found that considering the traditional concept of good orbit split by attempting to reduce the amount of MBBs to reduce the index excessively not suitable for improving the performance of memory distance search. Another finding is that calculating good orbit split to time search index trees and time processing candidate group orbit split may not be beneficial to consider big data sets. GPU is an attractive technology for remote border search as the internal data parallelity involves the calculation of the double polymers between mobile distances; however, the challenge generated from the SIMD programming model and limited GPU memory.
What is English Query?Microsoft English Query (EQ) allows users to submit database queries in clear English.To do this, developers only need to define a database sequence and actually build a database concept model.EQ provides an authorized tool that allows developers to define the entities and relationships in the database, as well as the database objects that are relevant.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
While this method has succeeded, the arrival of the Internet and the need for queries through the circulation sources require different methods, since for the circulation input, queries may be unknown or may even unknown (such as in the case of unlimited flow).
In this extension, we briefly describe EMC’s information sharing technology, allowing UNIX and NT systems to have direct access to the MVS main box data sets and how to use the technology to have direct access to the MVS DB2 database.
Extension supports new data types, such as points, cycles, etc., as well as features such as errors, intervals, text content, etc. Give the table policy (Politics ID, name, address, location, vehicle type,.. ) and claim (Politics ID, claim labels, accident date, accident location, accident report,.. ) represents a part of the chart containing SQL’92 and user-defined data types (UDTs).
First Part: Basics: Concepts and Business Models Chapter 1 Background, Terms, Opportunities and Challenges Chapter 2 Enterprise and Consumer Data Storage Chapter 3 Consumer and Consumer Data Storage Chapter 4 Enterprise and Enterprise Data Storage Chapter 5 Electronic Management and Data Storage Chapter 6 Enterprise and Employee Models and Data Storage Chapter 2 Building Block, Challenges and Solutions Chapter 7 Core Technology and Building Block Chapter 8 Electronic Commerce Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information Information
In this article, we show how to effectively use this task of probability graphic models as a precise and compact approach to frequency distribution of multiple properties across multiple relationships. probability relative models (PRMs) are a recent development of extended graphic statistics models, such as the Bayesi network to the relationship field. They represent statistical dependency between properties in a table, and properties between cross-border critical combinations. We provide an effective algorithm to build a PRM prior to a database and show how a PRM can be used to calculate selective estimates of a wide range of queries.
The project MENTAS (Motor Development Assistant) - aimed at achieving an interconnected, engineering-oriented development environment for faster design and comparison of the engine. To this goal, it provides an integrated access to multi-sales DBs. In our exhibition, we show how interconnected abnormal DBs work in MENTAS.
XML attempts to make the natural language documentation ("text") that the database has been in possession for decades: clear structure, its properties can be known; data and structure independence reports (we foreigners call it "formatting"; various isolations; and so on.
Content Table I1 The Fourth World Research Integrity Conference Process: 1. National Systems and Policy to Promote Research Integrity CS01.1 Second: Research Intensive University Implementation and Integration of Research Policy and Practice Responsible Review Susan Patricia O'BrienCS01.2 Promote Research Integrity Measures: A Asian University Case Danny Chan, Frederick Leung2. Examples of Research Integrity Education Programs in Different Countries CS02.1 South Korea's "Network Education Research Ethics Program" Development Eun Jung Ko, Jin Sun Kwak, TaeHwan Gwon, Ji Min Lee, Min-Ho Lee.
Previous studies have shown that in the Multimedia Database Management System (MMDBMS) some data elements can actually be stored by storing, which means that they are stored as a sequence of editing operations.The existing method of content base recycling (CBR) in the MMDBMS usually assumes that data elements are stored as big, binary objects.The result is that the MMDBMS cannot use the existing method without losing the space savings actually obtained by storing data elements.In our presentation, we introduce a prototype CBR system's virtual image to avoid this problem by using sequence information in editing operations.
Workflow Management System (WfMS) is a software platform that allows to define, execute, monitor and manage business processes.WfMS records every event that occurs during the process execution.Therefore, workflow records include a lot of information that can be used to analyze the process execution, understand the reasons for the high - and low quality process execution, and evaluate the performance of internal resources and business partners.In this article, we introduce a packed data storage solution, with the process manager, collect and analyze the workflow execution data.
We first deal with perfectly unlocked queries, i.e. a fixed proportion of queries to obtain answers from each disk, we show that the part of the data set must be assigned to each disk is affected by the relative speed and capacity of the disk. In addition, the majority of the distributed systems are a sequential structure in which the disk group is placed on the server, which causes further complications due to variations.
In this article, we describe a new web query processing method and learning ability. In this method, the user query is in the form of keywords, the search engine is used to find the URLs of the site may contain the necessary information. The first URLs are introduced to the user browsing. At the same time, the query processor learns the information required by the user and the user through hyperlinks browsing the way to find this information.
One of the most challenging issues in the digital library is finding relevant information. information findings are the focus of the research of the Stanford component of the CS-TR project sponsored by ARPA and work has been one of the main drivers of the Stanford Integrated Digital Library project. In this article, we discuss some information finding issues such as text database findings, effective information dissemination, copy detection and deletion.
In this article, we introduce a image access system using the image's color and space information to promote the access process. The basic unit used in our technology is a single-colored group that connects the color in the image to the same area. The two image groups are similar, if they are the same color and covered in the image space. The number of groups extracted from the image can be very large and affect the accuracy of the access. We study the impact of the number of groups on the access efficiency to determine the appropriate value of the "best" performance. In order to promote the effective access, we also proposed a multi-layer indicator mechanism called multi-layer score (Multi-Attribute TATTree).
With the growth of the Internet and e-commerce, online search has become the core of the consumer information search and economic decision-making process. However, due to the size and format of the information accessible by the search engine as well as the abnormal nature of man, it is difficult to determine what information is used and how exactly through the search engine is used by the consumer to promote decision-making activities in the online market.
The paper describes the ARANEUS Wel-Base Management System (L, 5, 4, 61, a system developed at Universitb di Roma Tre, which represents the recommendation for a new type of database that is designed to manage the database style. We call the WebBase an unusual data collection, more specifically: (i) high-structure data, such as usually stored in the relative or object database system; (G) semi-structure data, in the web style.
In data mining, classification is useful to detect groups and identify interesting distribution in the basic data. Traditional classification algorithms are both favorable to classification with regional shapes and similar sizes, or very vulnerable in the presence of classification. We propose a new classification algorithm called CURE, which is stronger for classification and identifies classification with non-regional shapes and large sizes. CURE achieves this by representing a certain number of fixed points of each classification, these classifications by choosing the dispersed points from the classification, then shorten them to the center of classification, and by classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification classification
The selection of views is one of the most important decisions in the design of the database. In this article, we introduce the framework of the analysis of the choice of views issues to the best combination of good query performance and low-view maintenance.
Therefore, a precise classification is an essential component of the hypertext database. the hyperlink raises new problems that are not solved in a wide range of text classification literature. the link clearly contains high-quality grammatical suggestions that are lost in a pure term based on classification, but the use of the link information is harmless because it is noise. the naïve use of the term in the link near the file can even reduce accuracy. our contribution is to propose strong statistical models and relaxing the label technology to better classify, using the link information around a small area of the file.
We think the data is semi-structured when there is no specified or known plan, when the data may be incomplete or irregular. For example, the HTML file in the world usually contains a certain structure, but often the data is irregular or in addition, the data from a variety of unusual information sources is often semi-structured. storage and query of the data submits quite different issues and requirements, while the traditional database where data storage and query processing depends on structured data.
VQBD project solves the following questions: What is the best way to explore unknown structures and content of XML documents? We focus on XML documents, these documents are too big to browse in its entirety, even with the help of quality printing software (e.g., multi-bit or larger XML documents). In this context, we use the term "data exploration" to refer to a user gathering the information required to use data for specific purposes (e.g., creating a report, writing queries, building user interface, writing applications). in the relative or object database, graphs (e.g., graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs, graphs
The network is a emerging platform to support the demand of the “virtual organization” to coordinate the sharing of resources and solving problems globally. the application’s power is a large-scale scientific effort, while the scale and complex scientific data poses the challenge of the database. the network begins to take advantage of the development of the technology for Web services and to its potential it also stands beneficial to the Semantic Web technology; on the contrary, the network and its scientific users provide the application’s power to benefit from the Semantic Web.
This chapter reveals that so far, network intermediate software focuses mainly on the basic issues of storage, calculation and resource management, which need to make information and tools available to the global scientific community.
The objective of the i3 project is to improve the multi-dimensional database products through a series of advanced operators to automatize the data analysis tasks that are currently processed by manual exploration.The majority of the OLAP products are quite simple and depend on the user's intuition to manually drive the detection process.This ad hoc user-driven exploration becomes boring and wrong because the size and size of the data are increasing.We first study how analysts are currently and why they are exploring the database, and then automatize them using advanced operators, which can be submitted as interactively as the existing simple operators.
In this article, we have developed algorithms for the following operators: half, opposite half, outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside.
In writing multimedia scenes, especially with users interacting scenes where the sequence and time of interaction occurs are not scheduled, it is difficult to guarantee the consistency of the results of the scenes. therefore, the execution scenes may lead to unexpected behavior or unconsistent use of the media. This article proposes a method to check the full time of interactive multimedia file (IMD) scenes in writing time at different levels.
We presented a new indexing structure called XTrie, which supports the effective filtering of XML files based on XPath expressions. Our XTrie indexing structure provides several new features that make it attractive in large-scale publishing/abonnement systems. First, XTrie is designed to support the effective filtering of complex XPath expressions (with simple one-way specifications opposite). Second, our XTrie structure and algorithms are designed to support the order and inaccurate matching of XML data.
The World Wide Web is emerging rapidly as an important medium for trading and the dissemination of information related to a variety of topics (e.g., business, government, leisure). according to most predictions, most human information will be available on the network within 10 years.
The paper reports the experience acquired when designing, implementing and using a multi-parameter query interface to an object-oriented database.The specific systems developed allow the use of text, form and graphic-oriented notes to express the same data-acquired tasks and support the automatic translation of query between these three parameters.The motivation behind this interface development is presented, just as a software architecture that supports multi-parameter functions.
We introduce an effective method of implementation of time integrity limits proposed in the past time logic. Although the limits can refer to the past state of the database, their check does not require the entire database history of storage.
Searching for the nearest neighbor in a high-end space is an interesting and important issue, meaningful for a variety of new databases applications. However, recent results show that this issue is very popular, not only in performance issues, but also in quality issues. In this article, we discussed the quality issues and defined the new universal concept of the nearest neighbor as the relevant issues in the high-end space. Unlike the previous methods, we treat the new concept of the nearest neighbor search unevenly all dimensions, but use the quality standards to choose the relevant dimension (prognosis) related to the specific query.
DTL’s DataSpot is an advanced, unprogrammed tool that allows web designers and database developers to automatically publish their databases to access the Web browser.DataSpot allows non-tech terminal users to explore the databases using free graphic language queries along with supertext navigation, similar to using search engines, such as Alta Vista in search for text files on the Internet.
In order to overcome these problems, we have invented five new network views that will generalize the traditional display. Two views show a full network, while the other three focus on a larger network part, defined as connected to a particular nod. Our new visual image retains many known advantages of traditional network maps, while using three-dimensional graphics to solve some basic problems, limiting the scalability of the two-dimensional display.
This dynamic, data-centered approach opens a personalized opportunity: each user can map to a separate supertext site (the so-called site view), business rules can be used to change the site view, both static and dynamic. We believe that personalized web access (also known as a web delivery) is a natural support of the recommended data-oriented method, which is our recognition of supporting ESPRIT project 28771 W3I3, MURST project Interdata, CNR-CESTIA, and HP Internet philosophy initiative.
Star queries are the most common type of queries in data storage, OLAP and business intelligence applications, therefore, it is necessary to effectively process star queries. To this goal, a new category of facts organization has appeared, using the route-based exchange keys to categorize a star plan facts data (DRSN98, MRB99, KS01) in a series. In the background of these new organizations, star queries processing has occurred a thorough change. In this article, we presented a complete abstract processing plan, capturing all the necessary steps in evaluating these queries based on a series of facts.
The digital content is used to copy: quotation, review, copy, and file sharing all the creation of copy. Document fingerprint involves accurate identification of copy, including small parts of copy, in a large number of documents. We introduce the category of local document fingerprint algorithms, which seem to catch any essential properties of fingerprint technology, ensuring detection of copy. We have demonstrated a new low line in any local algorithm performance. We also developed a winner, an effective local fingerprint algorithm, and show the winner's performance within 33% of the low line.
The relative model of data contains the basic claims of entity integrity and reference integrity. Recently, these so-called relatively unchanged are precisely defined by the new SQL2 standard. therefore, they must be guaranteed by their user's relative DBMS, therefore, the parameters of all issues and implementation become very important.
Multimedia databases usually process a large amount of data, requiring an indexing structure to provide effective data access. R-Tree with its variables is a common indexing method. In this article, we proposed an improvement of the closest neighboring search algorithm on R-Tree and its variables.
EOS uses multiple versions of two-stage lock protocols, allowing many readers and one writer to access the same item at the same time. Conversion to simple 2PL options are also available. EOS uses a written re-logging program, providing short logs, fast recovery of system failures, and unlocked checkpoint. In addition, the profile is provided, can be edited by the user to customize and adjust the EOS performance.
We are developing a precise SemaSQL synthesis and grammar in a way to expand the traditional SQL synthesis and grammar and show the following content.(1) SemaSQL retains the taste of SQL while supporting data and meta data queries.(2) It can be used to represent the data in a database structure, which is fundamentally different from the original database, in which data and meta data can be exchanged.(3) It also allows to create a view, its chart is dynamically dependent on the input case content.(4) Although SQL’s integration is limited to the value that occurs in a column, SemaSQL allows “level” integration and even integration of more “information” overall.
The data integrated system provides access to multiple data sources through a single mediation program. One key point in building these systems is the sophisticated manual construction of a serial map between the source program and the mediation program. We describe the LSD, a system that uses and expands existing machine learning technologies to semi-automaticly find these maps.
In this article, we study how to find such a best timetable. in particular, we consider two optimization criteria: (i) one is based on the maximum number of broken clips, while (ii) the other is based on the maximum impact on the bubble space. We show that the best timetable under the first standard is equal to the maximum match in the properly defined bilateral chart, while under the second standard, the best timetable is equal to the maximum match in the properly defined weight bilateral chart.
In this article, we discuss the benefits of workflow automation and show why workflow is the key technology to build an ebusiness base, and we will put HP Changengine in the core of its e-commerce platform by introducing an example of a very successful ebusiness startup.
During the panel discussion most but not completely focused on the application perspective of the active database research issue, there were nine panel workers, each panel worker was asked to prepare short answers for a set of questions, all participants discussed the answers and eventually discussed a series of more general questions.
Over the past decade, the Internet has revolutionized many aspects of life, scientific publications are just one of the many companies influenced by the worldwide broadband and high-band connectivity, and now most scientific magazines have a network existing, that is, it is still a surprising degree that ACM generally and TODSin especially embrace the unique ability of the network to help spread knowledge.
Structural queries are based on a special form of access to content, the user specifies a set of space limits in the query variables and requires all the actual objects that meet these limits (all or part). processing such queries can be considered a general form of space connection, i.e. instead of objects, the result is composed of n objects, where n is the number of query variables. In this article, we describe a flexible framework that allows the configuration of representativity at different resolution levels and supports the similarity measurement of automatic derivatives.
The central development of the database field involves tools that allow non-expert users to understand and easily extract information from the database.The fourth generation of query language, although non-programmatic, is not friendly enough for a random user, they must know the logical structure of the database as well as the synthesis and grammar of the DBMS query language.
Currently, Parallel Object Relationship Database Technology has set a direction for the future of data management. The core improvement in Object Relationship Database Technology is the ability to perform the volunteer user definition features in SQL Statements. We show the boundaries of this method and propose the user to define the desktop operator as a new concept that allows the volunteer to define and implement the N-ary database operator, which can be programmed through SQL or embedded SQL (there are some extensions).
Model-based engineering technology provides a prospective approach to addressing the complexity of the platform and expression field concepts in the third generation of languages.
First, the XML Keyword Search Question is not always returning the full file, but can return the deep XML elements, containing the required keywords. Second, the XML Painting Structure means the ranking concept is no longer in the document marginality, but in the XML element marginality. Finally, the keyword approach concept is more complex in the XML data model. In this article, we introduce the XRANK system, which is designed to handle these new keywords search results.
ACRP's Association Trust Committee Chairman tells him how he left the production and research and development a decade ago to serve as a clinical research manager, which is a move that allows him to enter a role he rarely knows because he did not take part in clinical research in advance, if it sounds like a familiar experience to others, the lessons shared in this column emphasizes the importance of the individual's willingness to continue learning, as well as the organization supports learning.
When processing similar semi-structure data available on the web, it is important to list the internal structures, whether the user (e.g., promoting queries) or the system (e.g., optimizing access). in this article, we consider the problem of identifying certain infrastructure in large semi-structure data sets.
The database is the core operator of data storage and OLAP, its effective calculation, maintenance and use to respond to queries and advanced analysis, has become the subject of many research, however, for many applications, the database has a huge scale of limiting its availability as a means of user's emotional exploration.
We the performance of the NOW-Sort, a collection of classified implementations on the workstation network (NOW).We found that the parallel classification on the NOW was competitive for the classification of large-scale small enterprises.In 64 nodes, we classified 6.0 GB in less than a minute, while 32 nodes in 2.41 seconds.
Data analysis technologies and technologies are widely used in the business industry, allowing organizations to make more informed business decisions, scientists and researchers can verify or deny scientific models, theories and assumptions. According to specific applications, information may be historical records or new data, which has been processed in real time, or it may be the result of mixed data channels.
In order to find a static type system that supports the database language, we need to express the most common type of program involving the database operations, which can be achieved by expanding to the field of capture of the various countries of the ML type system, as well as the technology of converting the relationship operator into voluntary data structure.
This paper introduces a method to develop a practical driven objective database (DOOD) system to integrate a logical query language with a compulsory programming language on the background of an object-oriented data model. This method is new, one of which is officially defined data model has been used as the starting point of the development of two languages.
We study the main memory indexing technologies, including hash index, binary search trees, T trees, B+ trees, interval search, and binary search backgrounds, our main concerns are the search time and occupy the indexing structure of space. Our goal is to provide faster search time than binary search, by focusing on reference location and cache behavior, without using significant additional space. We propose a new indexing technology called cache sensitive search trees (CSS tree).
Traditional statistical methods deal with the assumptions confirmed on the data body. however, the assumption itself is intuitive and genius. it is clearly impossible to test all the assumptions on the database, millions of records and hundreds of assumptions. through data mining, there is a try to bridge this gap.
In the DBCache project, we incorporate the database storage feature into the DB2 UDB by modifying the engine code and using the existing federal database feature, which allows us to store the database using DB2’s sophisticated query processing power.
In this article, we introduce and carefully evaluate a new category of query optimization algorithms, which are based on a principle that we call iterative dynamic programming, or IDP in short.
In this paper, we introduce a solution to the problem that works simultaneously on the R tree, a dynamic access structure that can store multi-dimensional space data. We describe the R chain tree, a variation of the R tree, adding brother indicators to the nodes, a technology first deployed on the B chain tree, a variation of the R tree, adding brother indicators to the nodes, a technology first deployed on the B chain tree to compensate for current structural changes.
As we enter the information age, the use of electronic information is spreading to the various sectors of society, whether nationally or internationally.Therefore, business organizations, educational institutions and government agencies believe it is necessary to be connected by networks around the world, while the use of the business internet is growing rapidly.
Providing content-based video queries, access and browsing are the most important objectives of the video database management system (VDBMS). video data is unique not only in terms of its spatial and time characteristics, but also unique in the graphics associations expressed by the entities that exist in the video. This paper introduces a new video data model, known as the logical super-video data model. In addition to multi-level video abstraction, the model can also represent the user's interest in the video entities (defined as hot objects) as well as its graphics associations with other logical video abstracts, including the hot objects themselves.
The paper describes the architecture of OPERA, a general platform to build distributed systems in separate applications. The main contribution to the research effort. It is t,o proposed a “core” system, providing a “basic” distributed processing and showing the key role of database technology in supporting this function. These include a powerful process management environment. Created as a general workflow idea and includes transaction concepts, such as isolation, atomicit.y, and a durable and a transaction engine implementing accuracy based on a based and multi-level model. It also includes a toolkit, providing external database functions, allowing the physical database to design an extraordinary database.
In this article, we study how to update a local copy of an independent source of data to keep the copy up to date. As the size of the data increases, keeping the copy becomes more difficult, "to make the copy efficiently synchronized is essential. We defined two freshness measurements, change the model of the basic data, and synchronize the policy. We analyzed how effective the various policies are. We also experimented to verify our analysis, based on the data collected from 270 sites for more than 4 months, and show our new policy improved the freshness" with the current policy in use very significantly.
The transaction concept in computing returns to the early days of computing data processing, it has been developed and developed for years, both in the form of theoretical and practical applications, this evolution process is mainly driven by applications that need to trade similar properties, emerging applications include several roles involving people in time-dependent, these applications require human involvement in the processing is generating new system-level challenges, as well as these needs to be presented from a theoretical perspective and opportunities.
Weight samples are used to save the density of the original data. The density balance samples naturally include a single sample as a special case. A memory efficiency algorithm is recommended to approach the density balance samples through a single scan data. We experimentally evaluate the density balance samples using a synthetic data set, showing the distribution of different group sizes to find a single sample of up to six improved factors.
The TSQL2 language, created for a temporary database, has a limited information concept(14). therefore, formalism and temporary query languages provide some solutions to represent and manipulate uncertain temporary objects. The main objective of this paper is to show a formalism, providing a definition of representation and manipulation of uncertain temporary objects in uncertain temporary database (ITDB).
In this article, we focus on obtaining a set of interesting answers from the database, called the string. Considering a set of points, the string includes other points uncontrolled. The other point is dominant if it is good or better in all dimensions and at least better in one dimension. We introduce two new algorithms, the bitmap and the index to calculate a set of points. Unlike most existing algorithms, it takes at least one to return the most interesting points through the database, our algorithms gradually return to the interesting points because they are identified.
The multimedia description standard MPEG-7 is an international standard since February 2002, it defines a huge combination of multimedia content, its creation and communication description classes. This article studies the MPEG-7 on the multimedia database system (MMDBS) and vice versa.
The new generation of e-commerce applications require a continuously developing data chart and a few inhabitants. Traditional vertical series chart cannot meet these requirements. We represent objects stored in vertical format as a set of boxes. Each boxes consists of an object detector and specified name value objects. Chart evolution is now easy. However, writing for this format queries becomes obscure. We create a logical vertical chart chart and convert queries to vertical chart. We introduce the alternative implementation and performance results, showing the effectiveness of vertical chart for data spread. We also identify the additional facilities required in the database system to support these applications.
In this paper, we presented DynaMat, a system with multi-level energy to meet the demand (work load), but also taking into account storage maintenance restrictions, such as time limit for updating view and space availability.DynaMat unites view selection and view maintenance issues under one framework, using a new "good" measurement for materialized view.DynaMat continually monitores access to queries and materializes the best view sets, subject to space limitation.
We introduced a new algorithm to dynamically calculate the number of input and deletion operations. algorithm monitoring operations andining a simple, small space representative (based on random subgroups or RSS) of data distribution. using these RSSs, we can quickly estimate, no need to access the data, all the quantities, each guarantee is accurate, within the accuracy of the user specified.
This short paper is a slight introduction to the theory, with the focus on the most relevant results of the database theory. Interested readers are quoted to a single map of Downey and Fellow to learn more about the theory of parameters complexity. The paper is organized as follows: In section 2, we describe in an informal way two simple fixed parameters traceable algorithms.
I think of the two aspects of the scientific publication that seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem seem
This paper provides a formal advanced operational, for a complex value of OQL-like query language, which can create fresh database objects and query external methods.We define a type of system our query language and prove an important sound feature.We define a simple effect literature to determine the calculation effect in our query.We prove this effect system is correct and show how it can be used to detect undefined cases and determine the correct query optimization.
METU Object-Oriented DBMS1 includes a database core, an object-oriented SQL similar language and a graphic user interface implementation. core features are divided into a SQL interpreter and a C++ compiler. therefore, the interpretation of the function is avoided by increasing the efficiency of the system. C++ compiled function is used by the system through the function manager. the system is implemented on the Exodus Storage Manager (ESM), thus using some core features provided by ESM easily.
Sustainable Application Systems (PASs) have an increasingly important social and economic significance. They have a long-living potential, simultaneously accessible, and consists of big data and programs. Typical PAS examples are CAD/CAM systems, office automation, CASE tools, software engineering environments, and patient care support systems in hospitals. Orthodox Sustainable Object Systems are designed to provide better support for the design, construction, maintenance and operation of the PAS. Sustainable Abstract allows the way to create and process data, independent of its life, thus integrating the database view with the programming language view.
Our GC uses a new synchronization mechanism (a mechanism that allows GC to work simultaneously with the normal user of the database), called the CC consistent cutting.GC consistent cutting is a combination of the virtual copies of the database page.
This tutorial deals with the construction of web applications of data centers, with a focus on the process modeling and integration with web services.
Today’s children grow up, have information and communication technologies (ICT) as an essential and natural part of their daily life, and as they grow, they are expected to become active and self-oriented members of their own local community and information society, the technology creates information acquisition and creation, for self-expression, as well as diversity of opportunities for communication and interaction with others from around the world.
DataSpot of DTL is a database publishing tool that allows non-tech end users to explore a database, using free form of graphic language queries combined with supertext navigation.DataSpot is based on a new data representation, with an irregular semi-structure graphic called supertext.DataSpot publishers take one or more possible unusual database, predefined knowledge bank, such as a thesaurus, and undefined association, and create supertext.DataSpot search server to search and navigate, against supertext, return the user's answers, whether in the HTML page or through the object API.
From this lucky point of view, we can focus on spreading good science quickly and ef® wisdom. But, to do this, we need good reviewers' continuous support. The next time, one of the big brown packages appear in your mailbox, it takes a few minutes to think what is asked to you, not only from a overworking professional point of view, who just got another task, but also from the author and editor's point of view, who rely on your expertise, see the best scientific interest services.
The Tht3 paper shows support for transactions in ADSTAR Dhtributed Stomge Manager, (A DSiU) system. For a user, ADSM provides a storage and file service in an unusual client server environment. It also serves as a file transfer mposito~ In some Unix environment. As a silent administrator, ADSM server is a large silent system (MSS) managing silent system (MSS) an arbitrary depth, where all the activities are at the bottom of the transaction. its system targets include many computers in phomts#, pm# to Glya@metlya@metlya@metlya@metlya@metlya.
This paper proposes a dynamic bubble allocation scheme that requires the user to allocate the smallest size of the bubble to a partial load state as well as a full load state. In the dynamic bubble allocation scheme, the internal difficulty of determining the bubble size is that the current allocation of the bubble size depends on the number and size of the bubble allocated during the next service. We solve this problem by predicting and reinforcing the strategy, where we predict the amount and size of the future bubble, based on ineffective assumptions, and during the running time implementing these assumptions.
Finding by a choice of predicted rotation mode of all events in a XML file multi-elements is effective to evaluate the core operation of the XML query. The overall rotation combination algorithm has recently been proposed as a best solution when the rotation mode only involves the ancestors and retrorelationship. In this article, we solved the problem of the overall rotation combined in all / part indexed XML files. In particular, we proposed an algorithm using the available indicators of the elements group. Although it can analytically prove that the proposed algorithm is effective in the existing state and the art of the algorithm in the worst case I / O and CPU costs, the different results of the experimental data indicate that the index is based on the overall rotation combination in all / part indexed XML files, in particular, we
We think the answer to a question from a database may be incomplete, if some details may lack a certain relationship, and each part of the relationship is known to be complete. This problem occurs in multiple contexts. For example, the system that provides a variety of incomplete information sources will often encounter incomplete sources. We ask the question is to determine whether the answer to a particular question is incomplete, even if the database is incomplete. We introduce a new voice and complete algorithm of answer-complete question, by putting it with the problem's independence question from update. We also show an important question of independence question (so the answer-complete question) can be solved in time, where the best case of the algorithm is complete.
In this article, we propose a monitoring service that these database servers can provide and provide their implementation algorithms. Unlike the published view maintenance algorithms, we do not assume that the server has access to the original materialization when the calculation difference view changes are notified.
Discussing the geographical information system GIS issue, starting with the definition of this term, there are countless definitions of GIS, each based on the user and the type of application field. The more common definition will be "a digital information system, its recording is to some extent geographical reference." For more accurate definition, it can emphasize its functionality (e.g., GIS capture and processing space data) or support applications (link defined to the type of problem solving). Others emphasize that GIS is ultimately used for decision-making support tools.
We consider the existence of query optimization in the data access pattern (i.e., when a person has to provide value for a relationship property to get double). we show that in the existence of a limited access pattern, we have to search for a space for the registration query program where the query description input must be given to the plan. we describe a theoretical and experimental analysis of the result of the search space and a new query optimization algorithm designed to perform well under different conditions.
In this article, we reviewed the rules of the mining association in a big database of sales transactions. The rules of the mining association can map to find the problems of the big projects, one of which is a big project group that appears in a sufficient amount of transactions. Find the big project group problems can be identified by building a candidate project group, first, then, in this candidate project group, that project group, meets the requirements of the big project group.
This paper presents the dependency criteria for the OLTP system, which dependency criteria uses the workload of the TPC performance criteria, and describes the measures and all the steps required to evaluate the performance and critical dependency characteristics of the OLTP system, with a focus on availability.
We describe a "complete speed" backup, which is only easy to connect to the backup administrator, so similar to the current online backup, but effective overall logic operation. This requires additional login to the backup object to ensure the media recovery. We then show how login can be significantly reduced when log operation has a limited form, but provides a very useful additional login efficiency of the database system.
Our work focuses on the integration of data mining and OLAP technology, as well as the development of extensive, integrated and multi-data mining features.Data mining systems, DBMiner, have developed multi-level knowledge of interactive mining in large relations database and database.The system implements a wide range of data mining features, including description, comparison, combination, classification, prediction and classification.
Since the multimedia acquisition is based on the similarity of the grammatics and the media-based search, the accurate matching is unpredictable. We will consider the query a multi-media database as a combination of IR, image matching and traditional database query processing, it should be carried out in a way of permanent query reform to obtain target results. In this article, we introduce a mixed multi-media database system that uses a serial database statistical structure, both query optimization and reform analysis, without increasing the additional query processing costs.
We describe the design and implementation of the Glue-Nail-driven database system. The nail is a pure statement query language; Glue is a programming language for non-query activities. The combination of the two languages is enough to write a complete application. The nail and the Glue code are written to the target language IGlue. The nail compiler uses the magical combination of variables and supports good models. The Glue compiler’s static optimizer uses peephole technology and data flow analysis to improve the code.
In this article, we presented a new “Transitional Execution Model” (TEM) with the wooden index opposite user queries. Our models use the internal parallelity of the index scanning and dynamically divide each query into a divided “query query”. TEM will integrate the idea of forecast and sharing query into a new framework, suitable for a dynamic multi-user environment. It supports time limit in the programming of these queries and introduces the concept of data flow to the stable progress of all queries.
Using non-synchronous video interviews (AVI) and AI-based decision-making agents can be more effective than traditional synchronous video interviews (SVI). However, the social impact of using synchronous and AI decision-making agents in video interviews has not yet been investigated. Painting media wealth theory and social interface theory, the study adopts a new experimental design to compare human ratings and employment applicants' reaction behavior to the SVI and AVI settings, and compare employment applicants' fair perception of the AVI settings and AVI settings using AI decision-making agents (AVI-AI).
At the University of Illinois in Urbana-Champaign, the computer engineering course is provided by the major, research-oriented electrical and computer engineering department.The course has a solid foundation in electrical engineering, the proper balance of computer hardware and software topics, a series of increasingly complex design experiences, as well as a variety of choices.
In this case, the SQL engine provides access to external data using the SQL DML statement as an intermediate software server and combines external data with SQL tables in an unusual query.
The British National Database Conference is a forum for the British Database Practitioners, which has been the focus of the database research since 1981.In recent years, the interest in this conference series has far exceeded the UK.In 2001, the 18th session of the series took place from 9 to 11 July 2001 at the CLRC Rutherford Appleton Laboratory (RAL).
The paper describes the technology of accelerating online analysis processing or OLAP queries, which allows users to quickly get the answers to complex business queries, quickly answering these queries, these queries integrate a lot of data, requiring a variety of specialized technologies.
In this article, we propose HMAP (this term is the translation of the ancient Greek poem, meaning "day"), a time data model that extends the ability to define effective time, different sizes and/or uncertainty. in HMAP, the absolute interval is clearly representing their beginning, end and duration: thus, we can represent effective time, such as "in December 1998, 5 hours", "from July 15, 1995, "from March 15, 1997 to October 15, from 6:30 p.m.
We proposed a universal parallel method called TOPAZ. different forms of parallelization are used to get the maximum speed, combined with the minimum resource consumption. the necessary abstract operating characteristics and system architecture are provided by a cost-based, top-level search engine use rules. based on the global analysis of multi-stage spread effectively guide the search process, thereby significantly reducing complexity and achieving optimized performance.
With the massive development of the Internet, access to information becomes difficult and unreliable. user interests and needs are different in every moment. to improve the search experience, several personalized search technologies are proposed. using information about the users, their history and query behavior results will be repeated. this type of query is known as personalized search technology. Today, information on the server is a semi-structured way. XML (eXtended Markup Language) search helps to improve the access efficiency in semi-structured data. the system proposes a new personalized search in the XML framework.
Database support data mining has become an important research topic. especially for big data, the comprehensive support from the database side is necessary. In this article, we identified the data integrated big data in all possible low-size projections (such as estimated low-size history), occurring in multiple data mining technologies. Second, we show that the existing OLAP SQL extension is not enough for high-size data, and proposed a new SQL operator, which is misleadingly matching the existing OLAP group by the operator.
Technically, however, there is a broad misunderstanding that Semantic Web is mainly existing artificial intelligence and databases work, focused on encoding knowledge representing formalism in labelling languages such as RDF(S), DAML+OIL or OWL. Kashyap, Bussler and Moran try to spread this concept by introducing this emerging Semantic Web’s wide size and multidisciplinary basis such as machine learning, information re-evaluation, service-oriented architecture and computing, thereby achieving the potential and potential of the computer perspective.
We believe that video support on mobile systems will actually make many new interesting applications possible. However, providing mobile video is an unusual task and requires a lot of work before the practical system is widely available. In this short note, we discuss mobile multimedia issues from the perspective of the practitioners. We notice what software and hardware is currently available on the market to support mobile multimedia and point out some disadvantages. We also discuss some communication and data management research issues that need to be solved to solve these disadvantages.
The programming query implementation program is a complex problem in a series parallel system, each site is shared by a collection of local time (e.g., CPUs (or discs) and space sharing resources (e.g., memory) and communicating through messages with remote websites.We develop a general method of question, capturing the complete complexity of the multi-dimensional resource unit of programming distributed, all types of parallel between query and operator.We present a variety of forms of problems, some of which may be near the best.
We introduce a general solution to the problem, which is an unpredictable worst case performance characteristic in the core of a wide multi-dimensional index design: those using a repeated division of data space. then we show how this solution can produce a change of design with the completely predictable and controllable worst case characteristics. in particular, we show how to repeat the division of the n-dimensional data space can be described in a way so that the one-dimensional B tree characteristics areined in the n-dimensional, within the possible range, i.e. a representative, ensuring logistics access and update time, while ensuring a minimum of one-third of the data and index nodes.
The availability of high-performance networks triggered the research interest in the RDMA capacity of the hardware. The one-sided RDMA element, in particular, produces significant excitement due to the ability to access remote memory directly from the application without involving TCP/IP pipe or remote CPU. This article discusses how to use RDMA to improve the analytical performance of the parallel database system.
Many real-time databases applications appear in electronic financial services, security key facilities and military systems where implementation of security is critical for business success. We have studied the multi-level secret performance effects of applications in real-time databases systems from the point of view of loss transactions. In particular, we focus on the issue of bubble management. Our main contribution is as follows. First, we determine the importance and difficulty of providing secure bubble management in a real-time databases environment. Second, we introduce SABRE, a new bubble management algorithm that provides hidden channels free security.
Workflow history administrators maintain the information necessary for workflow monitoring and data mining as well as restoration and authorization purposes. Certain characteristics of workflow systems such as the need to run these systems in an unusual, independent and distributed environment and data nature, preventing workflow history management from being processed by classic data management techniques, such as distributed DBMS. We further indicate that multi-database query processing techniques are also not suitable for the problem. In this article, we describe history management, that is, history structure and history query, in the fully distributed workflow architecture, according to the OMG object management architecture (OMG).
This paper suggests the use of repeated broadcast as a way to increase the memory layer of the customer in an invisible communication environment, we describe a new technology called the "Radio Disc" to structure the broadcast disc in a way to provide uneven access to data and improve performance, the broadcast disc rotates multiple disc on a single broadcast channel at different speeds, thus creating a voluntary subtle memory layer.
The chapter presents the Hamming standard as a basic interest operation.The Hamming standard formalizes the idea used throughout the data processing.When applied to a single class, the Hamming standard provides the number of different elements present in the data stream, which is very interesting statistics in the database.When applied to a pair of streams, the Hamming standard provides (dis) important measurement of similarity: the number of inequal elements is calculated in two streams.The Hamming standard has many uses in comparative data streams.
We proposed a new index structure, A tree (approximately a tree), for similarity search high-size data.The basic idea of a tree is to introduce a virtual marginal straight corner (VBR), which includes and approaches MBR and data objects.VBR can be quite small, thereby affecting the configuration and quality of the tree. First, because the tree thread can be installed a lot of VBR entries, the nodes become larger, thereby leading to rapid search.
The paper proposes two extended R trees, allowing the data area to grow index, while allowing the internal marginal area to grow. the internal marginal area can be triangular and rectangular. the algorithm of the index structure provides new illusions. therefore, the death space and the super layer, now also the time function, decreases. performance studies show that the best extension index is usually 3 to 5 times faster than the existing R tree base index.
Joining is one of the most common operations. some fast joining algorithms have been developed and widely studied; these can be classified as type-based mixed and index-based algorithms. although all three types of algorithms show excellent performance over the majority of data, improved performance decline in the existing mixed has been investigated only for mixed algorithms.
In this article, we presented an effective way to rebuild a B+ tree index. This method has been implemented in Sibase Adaptive Server Enterprise (ASE) version 12.0. It provides high competitiveness, performs the minimum number of logs, and does not end with other index operations. It copies the index line to the new assigned page in a key order so that good space use and classification is achieved. The old page is assigned in the process. Our algorithm differs from the previously published online index rebuilding algorithm in two ways.
Many organizations have developed a very large database to maintain customer transactions. New applications, for example, the web recommendation system, presented the requirements for processing similarity queries in the market database. In this article, we presented a new similarity queries in the database data program. We developed a new representative method, contrary to the existing methods, has been proven to provide the right results. New algorithms were proposed to deal with similarity queries.
XML is widely considered to be a prospective means of data representative integration and exchange. As the company operates on the Internet, the sensitive nature of information must be selective, using accurate access control specifications to provide information. Use the specifications directly determine whether the user has access to a specific XML data project, so it can be very inefficient. Alternative fully materialized, for each data project, the user authorized access it can be spatial inefficient. In this article, we presented a space and time efficiency solution for access control problems XML data. Our solution is based on a new concept of compressed accessibility map (CAM), it recognizes the user’s access to XML data project, using the local accessibility structure of data.
The collection of synthetic XML documents can be useful in many applications, such as reference labels (e.g., Xmark, XOO7) and algorithm testing and evaluation. We introduce ToXgene, a template-based tool to produce a large, consistent XML document collection. ToXgene designed the following requirements: it should be declared to accelerate the production of data; it should be sufficiently common to produce quite complex XML content, it should be strong enough to catch the most common types of limitations in popular reference labels.
Database computing is one of the most important but most expensive operations in data storage. The latter is represented by two algorithms: BUC and H-Cubing, calculating the bottom of the ice sheet and convenient Apriori printing. BUC explores rapid classification and division technology; while H-Cubing explores a data structure, H-Tree, sharing calculations.
Previous studies have demonstrated how de-industrialization provides an unclean prospect for individuals and their communities: long-term unemployment, poverty and corrosion of previously critical areas.What can people do to alleviate the impact of the recession industry that has once employed several generations of workers?More importantly, how collective actions can help society multiple, rather than just a few, strictly defined interests?Jeremy Brechell's "United: Bress Valley Economic Democracy" shares the very necessary reward of this effort in the Naugatuck Valley, West Connecticut since the 18th century.
This is an ancient joke: what is black and red? a newspaper. but why? because we assume nothing can really be black and red, we get the “red” should be heard as “read”. in the great philosophy tradition, even humour atheism, I want to solve this assumption. my paper is that you can see two objects in black and white, while seeing one of them better than the other.
In this article, we explore new accuracy standards and planning methods, capturing temporary transaction dependency, and belonging to a wide area between these two extreme methods.We introduce the concept of result dependency and time dependency, and define accuracy criteria, even if the temporary dependency between transactions is retained, we also propose a timetable to perform at any time, which can ensure that the transaction is carried out in accordance with its time limit.
This view mechanism allows users to work transparently with the data in the object database, just as it is stored in the object view (OO) database. the object view queries are translated into one or more queries of the relative database. the results of these queries are then processed as answers to the initial queries. this query method is not limited to the "clean" object view mechanism of the relative data, as the object view can also store their own data and methods.
In this work, we studied a series of queries and developed a query mechanism that connects a web query based on a signature file. Our algorithms deal with a series of queries and cross-relationships between a query and the corresponding query project. We also developed a query replacement strategy to deal with the different sizes and contribution of the query project when providing some query answers. We report the experimental results and show how the query mechanism is achieved in the knowledge brokerage system.
The paper proposes an extensible framework for capturing and querying metadata properties in the semi-structure data model. These properties, such as the temporary aspects of data, are considered with data access related prices, with data related quality ratings and data access restrictions. In particular, the paper defines an extensible data model and accompanied query language, providing new facilities for matching, sliding, collapse and query properties.
In this article, we study the problem of continuous maintenance of the materialist views in the database, we consider the views defined by the operator of the relationship algorithm and the integrated function, we show that the materialist views can beined by materializing andining additional relationships without access to the views themselves, these relationships are generated from the middle result of the point of view calculation, we first provide an algorithm to determine what additional relationships require materialization to maintain the continuity of the materialist views.
The fast-growing management health care industry needs to effectively coordinate human and automated tasks and information flows in multiple enterprises.One of the most important applications in management care is national immunity tracking, which can significant recent effects if supported by appropriate workflow technologies.In this paper, we discuss a comprehensive and realistic application to support Connecticut's children's immunity tracking, working closely with CHREFl.
Question optimization creates a plan to obtain the requested data. Question rewrite, which is the first step in this process, rewrite a query expression to a fair form to prepare it to be planned. COKO-KOLA introduced a new method, query rewrite, allowing query rewrite formal verification, using an automatic theoretical tester. KOLA is a language to express the term rewrite rules, can be "rewrite" on query expression. COKO is a language to express query rewrite conversion, to express the simple KOLA rules are too complex.
Oracle RDBMS recently introduced an innovative compression technology to reduce the size of the relative table.Oracle is able to compress data more effectively than the standard compression technology by using a compression algorithm designed specifically for relative data.More significantly, unlike other compression technologies, Oracle has almost no performance penalty for SQL requests access to the compression table.In fact, Oracle compresses can access a large amount of data for requests and certain data management operations such as backup and recovery.
We set up a multi-database system to support financial applications, storing traders using history data to identify market trends. The application has an update rate (only added) 500 entries per second, and there is also a second question response requirement. A typical query request is between 100-1000 records. In this article, we define the application’s characteristics, we use multi-database systems to support the application and we make the expansion in the application to the required features and performance.
The storage view needs to be updated when the source data changes. Due to the constant increase in size and rapidly changing speed of the storage, there is more and more pressure to reduce the time to update the storage view. In this article, we focus on reducing this “update window” by reducing the work required to calculate and install a set of updates. In literature, a variety of strategies are proposed to update a single storage view. These algorithms are usually not extended to update the entire view strategy. We develop an effective algorithm to choose the best update strategy for any storage view.
In this article, we presented an effective algorithm mining association rules that are essentially different from known algorithms. Compared to previous algorithms, our algorithms not only significantly decrease the top of I/O, but also lower CPU top in most cases. We conducted a wide range of experiments and compared the performance of our algorithms with one of the best existing algorithms.
In this article, we have proposed two updates of broadcasting strategies to improve the freshness. both use instant broadcasting: updating to the original copies, once they are detected in the main nodes, without waiting for the promise of updating the transaction, they will spread to a slave nodes.
The paper is a review of the Stanford Information Filter Service (SIFT), which has been processing more than 40,000 global subscriptions and more than 80,000 daily documents since April 1996, which describes some of the indexing mechanisms developed for the SIFT, as well as the evaluation carried out to select a plan to be implemented, which also describes the implementation of the SIFT, as well as the experimental results of the actual system.
As we rely on the growth of the information systems, there are threats that are broken by cyber attacks to become a very tensive reality. We witnessed recent attacks, seriously broken and organizations. Unfortunately, this trend is just increasing. For a while now, some research teams are doing research on oil data mining technologies that may help solve the challenges these attacks present. This special issue is a attempt to gather some of these people together and spread some results among the SIGMOD audience and may cause the community's interest in this emerging field.
Knowledge-based applications require language, terms and ontological resources. These applications are used to complete a series of tasks, such as grammatical indexing, extracting knowledge from the text, obtaining information, etc. Using these resources and combining them to the same application is a boring task with different levels of complexity. This requires their representativity in a common language, extracting the necessary knowledge, and design effective large-scale storage structures to provide operator resources management.
Question Optimization is a powerful process of calculation, especially in the current data storage and mining applications. Question Optimization internal superquestion is made up of a new question usually optimized to the fact that there is no chance to overcome these superquestion priority optimization. Although the current business question optimizer for the previous question generated implementation plan (such as "Save Output" in Oracle 9i), question matching is very limited - only if the upcoming question has a close similarity to the text of a storage question is related to the plan to perform the new question.
In this article, we introduce Probabilistic Wavelet Synopses, the first on the basis of wavelet data reduction technology, which ensures that the individual is close to the accuracy of the answer. Although the previous method depends on deterministic boundaries to choose a set of "good" wavelet equations, our technology is based on a new, probabilistic boundary scheme, each equation isined according to its importance and then transfers the currency to the rebuilding of individual data values and then chooses the merger.
We first consider a cost model, calculating a score in a physical plan, and showing a search space, ensuring that it includes a best rewrite, if the query has a rewrite point of view. We also develop an effective algorithm to find the minimum number of rewrite points. We then consider a cost model, calculating a physical plan's intermediate relationship size, not giving up any properties, and providing a search space to find the best rewrite. Our final cost model allows the properties to be abandoned in the intermediate relationship.
Recently, a technology called a quantum bubble was proposed for a data bubble’s overview structure, retaining its grammar, with online exploration and visual applications. The authors show that the quantum bubble can be very effectively built, leading to a significant decrease in the quantum size. Although this is an interesting suggestion that this paper leaves many problems unresolved. First, a quantum bubble’s direct representative is not as compact as possible, so it still waste space. Second, while the quantum bubble can be used in principle to answer queries, no specific algorithms are given on paper.
Bioinformatics, the disciplines related to bioinformatics management are essential in the post-genic era, where the complexity of data processing allows modern multi-level studies, including genetic levels, genetically transmitted levels, protein levels, metabolic levels, as well as the integration of these - Omic studies to gain understanding of systemic levels of biology.
Information communication applications are increasingly popular due to significant improvements in communication bandwidth and visibility.The clarity of available data requires the use of selective communication methods to avoid the user's excessive use of unnecessary information.The existing mechanism of selective communication is usually dependent on simple keyword matching or "boxes" information accessing technology.XML as the appearance of the information exchange standard, the XML data development query language allows the development of more complex filter mechanisms, taking into account structural information.We have developed multiple indexing organizations and search algorithms to effectively filter XML files for large-scale information transmission systems.
The database is copied to improve performance and availability. The commonly used accuracy concept is through transaction sharing, possibly copied, the feasibility of data. However, the feasibility may be useless in high-performance applications because it imposes too strict competitive restrictions. When the feasibility relieves, the integrity restrictions describe the data may be violated. by allowing limited violation restrictions, however, we can increase the feasibility of transactions performed in the copying environment. In this article, we introduce an unknown transaction concept, which is a transaction that may ignore the majority of transactions, one of which the result of the transaction may be ignorant.
Can. Dep. Agric., Sci. Serv., Entomol. Div., Publ. No. 932. DANKS. H. V. (Edited) 1979. Canada and its insect fauna. Mem. Entomol. Soc. Can. 108. FAIN, A., N. J. KOK, F. S. LUKOSCHUS, and F. W. CLULOW. 197 1. Notes on the hypopial nymphs foretic on mammals in Canada with description of a new species (Acarina: Sarcoptiformes). Can. J. Zool. 49: 1518. GLENNY, F. H. 1951.
In addition to the study of the chemical properties of natural aromatic compounds (NACs), which lead to perception of smell and smell, several studies report their potential application to human health due to their antioxidant, anti-inflammatory, anti-cancer and anti-obesity properties. In addition, the consumer demand shows a tendency to natural products; most industry and academic fields of research focus on commercial-related NACs of bio-production, especially microbial production through de novo synthesis or bio-transformation using enzymes or whole cells in traditional water media.
Here we explore the sensitivity and geological stability guesses proposed by the participants of the 2018 IAS emerging theme seminar, and we try to explore these guesses and all the progress in the relevant examples, although they cannot cover all of this.
This article describes the new architecture that supports Teradata Business VLDB, and in a number of new operating environments, we started an overview of the Teradata database software architecture, especially the NCR Unix OS and the Database Software Parallel Database Extension (PDE).
In this article, we presented an effective algorithm called FastXMiner, discovering the common XML query pattern, we developed theory to prove that only a group of candidates produced need to pass the expensive wood container test, we also showed how the common query pattern can be used to improve query performance, the experimental results show that FastXMiner is effective and extensible, the result of the common query pattern significantly improves the query response time.
We abstract a GUI application as an event processor combination. Each event processor can be conceived as a transition from the old screen / program state to the new screen / program state. We use the screen / program state of the data center view (i.e., each entity on the screen is equivalent to the project date) and express each event processor as a problem-dependent update, but a complex. To express these complex updates, we use Logic++.
The XEWA-00 seminar was held in December 2000, sponsored by the IEEE Computer Association, aimed at gathering members of the bioinformatics community to determine whether XML can simplify access to the network-based data sources in large, unusual, distributed collections. a series of interrupted and collective discussions are the starting point of the proposed text editing, describing how through its web interface data sources are consulted. as the result of these discussions, the methods are verified, edited and generated multiple reference implementations in the ongoing efforts.
Recovery Oriented Computing (ROC) is a joint research effort between the University of Stanford and the University of California, Berkeley.ROC takes the views, hardware errors, software errors and operating errors are to be dealt with rather than the problem to be solved.This view is supported by historical evidence and recent research in the production system's main sources of failures.Through focusing on reducing intermediate time recovery (MTTR) rather than increasing intermediate time failure (MTTF), ROC reduces recovery time, thereby providing higher availability.We describe the principles and philosophy behind the joint Stanford/Berkeley ROC and efforts to list some of its fields and current projects.
In this paper, a solid technique is recommended to integrate non-related information to effectively detectable search areas. a decision-making surface is defined as dividing proper space into relevant and non-related areas. the decision-making surface consists of a super layer, each of which is normal from a non-related point to the relevant point of the conversion of the minimum distance vector.
In order to fully exploit and manage the value of biological resources, society must have intellectual tools to store, obtain, collect, analyze and synthesize information on the biological and ecological scale, but it is currently difficult to find, access and use the data of biological diversity, because of the "bottom layer" of scientific biological diversity information evolution history, the differences between the distribution of biological diversity itself and the distribution of data, most importantly, the essence of biological diversity and the complexity of ecological data.
In this text, we collected 331 new texts, most of which were published between 1996 and 1997, some of which were published in 1995, some of which were published in 1997 or 1998.
Java language as a popular programming language is rapidly growing. for many applications, it requires effective provision of database facilities. here, we report a specific method to provide such facilities, called "the right durability". durability allows data to have an unlimited life from transition to (we can get the best approach). it is the right durability, if the durability is the same for all types of data.
The multi-dimensional calculation in the database is very important.Dwarf is a very compressed calculation structure, storing data circles, which can be fully materialized.In the construction process, each closed nodes are stored on the disk, while all the calculation units need access to the closed nodes in the disk often.
We are developing a mobile passenger guide system for public transport. Passengers can create their travel plans and use the Internet to purchase the necessary electronic tickets. During the journey, the mobile terminal, also as an electronic ticket, will be stored the travel plans to compare with the actual activities of the passengers and provide the appropriate guide information. To perform this task, the mobile terminal collects a variety of information about the travel field (routes, flights, regional maps, stations maps, operating schedules, schedules, stations and vehicle facilities, etc.) using multi-channel data communication.
We consider an optimizing technology of assumptions and relationship databases. Optimizing technology is an extended magic template rewrite, it can improve the performance of query assessment by not achieving extended middle view. Standard relationship technology, such as the conversion embedded view definition, is not applicable to re-defined view, so alternative technology is necessary. We prove our re-writing accuracy. We define a class of "not repeated" view definition, and show that for certain queries, our re-writing performance is at least and the magic template is not repeated view, and often better.
In this article, we proposed a case-based method to solve the question of rewrite the query. According to this method, we use case memory rather than static views, i.e., the views are submitted in advance. Therefore, the mediation program is dynamically updated, the query submitted by the consumer is strongly overthrown. This method allows the mediator to face the system, the consumer may change their customized needs, the source of information may become inaccessible, can be added, or may change their program.
In this article, we will discuss the impact of the application of traditional trading management technologies in a multi-layer architecture in a distributed environment, we will show the performance costs related to distributed transactions, and discuss how really manage their distributed data to avoid this performance breakthrough, our goal is to share our experience with the database research and the supplier community to create easier to use and expandable designs.
The paper provides an overview of the IRO-DB architecture and describes in detail the cost evaluator of the next version of the distributed query optimizer. The cost model consists of a set of mathematical formulas, with an estimated ratio of the cost of the search operator. The ratio is extracted from an object-based database, composed of the relevant object collection. Run a adjustment application in each place to adjust the clothing formula and adjust the ratio. We report the adjustment of O2 and ObjectStore. We show that the estimates are quite accurate pathways on the OO7 reference mark at the top of the ObjectStore.
Currently there is a significant interest in the development of a multimedia digital library. However, it is clear that the existing architecture of the management system does not support the specific requirements of the continuous media type. This is an important field of service support quality. In this email, we discussed the quality of the service within the digital library and proposed a reference architecture that can support certain aspects of quality.
In this context, as part of the European project vision, we have established an interactive culture and telecommunications guide for the Sardinian region. The culture expert team browsed the network to get a lot of Internet resources. The system is used to manage these data using emerging Internet technologies, such as XML language packages and its applications.
In a big data storage environment, it is often beneficial to provide quick, close-range answers based on a complete statistical summary of complex collective queries. In this paper, we show the difficulties of using basic relations statistics (especially samples) to provide good close-range answers. We recommend merger as an effective solution to this problem and show how pre-calculating only one merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger merger
We describe the architecture of a relative database of hyperlinked resource detection system. This system can strongly answer the question of the combination of page content, generic data and hyperlinked structure, such as the number of links from the environmental page to the page about oil and gas in the past year. A key question in the population database in this system is to find the web resources related to these issues. We believe that the search based on the keyword "find similar" is based on a huge full-purpose browser that does not need and does not fit for resource detection.
The paper is drawn out of the basic platform, rather than taking into account the requirements of CRM solutions for various communication channels, to develop a unified enterprise data architecture to the entire channel’s customer view to maximize the value of business customers in customer retention and customer center analysis.
In the past few months, we have spent most of this column reviewing the meaning of the word "standard", updating your long-awaited third-generation SQL standard status (formerly known as SQL3 is now known as SQL:1999), and introducing your two-three parts of SQLJ specifications. This month, we will continue to look at the future by reviewing some new components of the SQL standard currently being developed.
DEVise is a data exploration system that allows users to easily develop, browse and share large table data sets (which may include or cite multimedia objects) with visual presentations.
In order to accelerate the multi-dimensional data analysis, the database system is frequently budget aggregated in certain subgroups of size and their corresponding sequence. This increases the time of query response. However, determining what and how much budget is a difficult one. It is more complex facts that the budget in the existing sequence can lead to an unconscious great increase in the storage requirements of the database. Therefore, interesting and useful estimates of the storage impact, which will be by a proposed sequence of budget, and not practically calculating them. We proposed three strategies of this question: one based on sample, one based on mathematical approach, one based on probability calculation. We study these unique algorithms of accuracy in the database estimates and the database estimates, because it is based on the database estimates and the database is
In this article, we developed a BBS (Filial and Connected Branch), an advanced algorithm also based on the closest neighbouring search, which is the best of IO, that is, it performs only one single access, these R Tree nodes may include Filial Filial Filial Filial Filial Filial Filial Filial Filial Filial.
In this article, we describe a method to introduce and maintain ontology-based knowledge management applications with knowledge processes and knowledge meta processes focused, while the first process surrounds the use of ontologies, the second process guides their initial settings.
Our approach depends on randomized technology, calculating the small "slide" summary, and then can be used to provide close-range answers to collect queries with reliable guarantees of approximate errors. We also show how existing statistical information base data (e.g., history maps) can be used in the proposed framework to improve our algorithm provides approximate quality.
Their appearance is based on making the network browsing welcomed mechanism: a transparent network foundation establishment, the existence of broadly distributed communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication
This success is not only attributed to the suitability of the target, but in fact, some new queries optimize ideas, such as “priority” techniques, which make the coalition rules mining work much faster than expected.
We introduce a general technology to effectively carry out large simple conversion or query operations of external memory databases. It significantly reduces the number of conducted disc access and searches, through maximum data access temporary location, and organizes the majority of the necessary disc access to long-series read or recorded data, repeatedly used in memory. This technology is based on our experience, from building a functional complete and fully operating web search engine called Yuntis. Therefore, it is suitable for the majority of data processing tasks in a modern web search engine and used throughout Yuntis. The key idea of this technology is to coordinate the allocation of relevant data tables and the corresponding allocation and delay of the translation and query operations of data processing and data processing and data processing and data processing and data processing.
Open a series of specific work to follow, this visual paper identifies, motivates, and abstract model management issues. It suggests supporting “model” and their “map” as a high-end structure, advanced algorithm operating manipulation. In the winter of 2000, I was at the Primary College of UIUC, this paper inspired me for countless moments when I had to create my own research agenda. I have always been interested in information integration on a variety of topics, such as query translation and data maps.
The system addresses these three questions: Is my protein series already patented? what is the previous art? how to expand my patent requirements? the system provides the following information to answer these questions: it can find a similar patent series to you, it can try to identify your series of super families, it can also find a similar patent series to some of the same super families, and it can also find a similar series of non-patent members of the same super family.
The field of research and development of database systems has achieved tremendous success over its 30 years of history, it has triggered a billion-dollar industry, installed a base that touches almost every big company in the world, manages a large amount of valuable information,ins operating, without the support of the business database management system (DBMS).
High breakthrough series (HTS) progress leads to an abnormal increase in the number of data produced by series experiments, increased complexity of bioinformational reports and increased data types produced. The number of these data, increased diversity and complexity, their analysis reveals the need for a structured and standardized report template.
ACTA is a comprehensive trading framework that facilitates the formal description of the characteristics of the expanding trading model. Specifically, using ACTA, it is possible to identify and interpret the interaction between (1) the impact of the transaction on the object and (2) the transaction. This article introduces ACTA as a synthetic tool for the expanding trading model, supporting the development and analysis of the new expanding trading model. Here, this is by triggering a new trading definition (1) by modifying the specification of the existing trading model (2) by combining the specification of the existing model, and (3) starting from the first principle.
To solve these problems, we have developed a visual AM "analysis" tool to support the AM design and implementation process. It is based on the AM building GST (General Search Tree, [HNP95]) framework that provides the designers with an abstract view of the wood AM and the AM implementing mechanical aspects of factors such as tree cross, competition control and recovery.
The checkpoint is an important mechanism to limit the time of accident recovery. This article describes a new checkpoint algorithm, implemented in Oracle 8.0. This algorithm efJiciently JWS bubble needs to write to the checkpoint and easily expand to the very big bubble storage size: it has tested the bubble storage size greater than six million bubble.
As a database system magazine, it is committed to the progress of the information system architecture, the impact of emerging technologies on the information system, as well as the academic contribution to the development of new applications, it has made significant progress in the design, implementation and evaluation of the database and other information collection systems, its range ranges from the development of dedicated hardware, the design of innovative software methods, the integrated system architecture, the design analysis and performance evaluation of the new technologies to the information introduction and capture.
The idea of building a database as a central data collection provides broad recognition for the company’s decision-making support applications. The specific design and management of the database, from a technological and organizational point of view, however, is far from small, but requires sophisticated and time-consuming efforts. The DMDW seminar took place at the CAiSE’99 conference held in Heidelberg on June 14 to 15.
The literature of information-integrated databases quietly assumes that the data in each database can be disclosed to other databases. However, there is an increasing need to share information in an independent entity so that no information except the answer to the query is disclosed. We formalized the concept of sharing the smallest information in the private databases and developed cross, cross, cross size and cross size protocols. We also show how to use the proposed protocols to build new applications.
However, in parallel query processing literature, integrated processing gained a surprising small amount of attention; in addition, for each traditional parallel integrated algorithm, there is a range of combination options, the algorithm performs poorly. In this work, we propose new algorithms to dynamically adapt, in query evaluation time, in response to observed combination options. By analyzing the simulation and the implementation of a workstation group, performance analysis shows that the proposed algorithm can perform well for all combination options.
With the emergence of GIS, multimedia and storage technologies, database systems began to focus on storing and accessing multi-dimensional data, such as space, OLAP, image, audio and video properties. As a step in this direction, Oracle% launched intermedia products to support space and image data, as well as materialized view (MV) to support storage applications. Although 2D space data is indexed effectively through OracleBi space and high-dimensional image data, using a combination of bitmap indexing and visual information recycling (VIR) products, it still requires effective indexing mechanisms to support intermediate data, such as OLAP and CAD/CAM applications. In this document, we describe a new application, which uses OracleBi space and high-dimensional data to support image tracking.
A warehouse is an integrated storage of information extracted from a remote data source. Because a warehouse effectively implements the materialized view, we must keep the view as the data source is updated. This view maintenance problem differs from the traditional view definition and the basic data is now disintegrated. We show that this disintegration can lead to abnormalities if the traditional algorithms are applied. We introduce a new algorithm, ECA (for "Eager compensation algorithm"), eliminating abnormalities.
ACRP's Association Trust Committee Chairman tells him how he left the production and research and development a decade ago to serve as a clinical research manager, which is a move that allows him to enter a role he rarely knows because he did not take part in clinical research in advance, if it sounds like a familiar experience to others, the lessons shared in this column emphasizes the importance of the individual's willingness to continue learning, as well as the organization supports learning.
DISIMA (Distributed Image Database Management System) is a research project developed by the University of Alberta, DISIMA implementes a database method to develop an image database system, image content adopts an object-oriented paradigm for modeling, while a statement query language and corresponding visual query language allows query image synthesis and grammar characteristics.
We introduce a variable center method to manage a Web WareHouse XML data. The starting point is the screenshots of the XML files we get from the Web. By running an algorithm, we calculate the changes between two consecutive versions. We then represent the series, using the completed deltas and the permanent identi ers of new representation. We introduce the logical representation of the basis and some aspects of the physical storage policy. The work presented here is on the background of the Xyleme project developed the huge XML storage data from the Web. It has been implemented and tested.
An infrared generator, where one sperm reflector has a source rich in infrared, in one of which the focus. The end of the reflector is combined with one sperm reflector in another focus so that the focus of the one reflector is matched with the focus of the previous one. The other sperm reflector can be inserted between the first one reflector, the one focus is matched with the other focus of the previous sperm reflector, the other focus is matched with the focus of the sperm reflector.
In this article, we introduce an enhanced tool to explore OLAP data, adapting to a user’s data to the prophet. The tool continues to track a user’s access to the cube part. These dispersed access to the cube part of the information gathers together to form a model of the user’s expected value in the unaccessed part. This model’s mathematical basis is provided by the classic Maximum Entropy principle. At any time, the user can ask the most surprising unaccessed part of the cube. The most surprising value is denied for those who know the user will bring new expected value closer to the actual value.
The real-time application of the database system must meet the time limits related to the transaction whileining data consistency. In addition to the real-time requirements, security is usually necessary in many applications. Multi-level security requirements introduce a new dimension of transaction processing in the real-time database system. In this article, we believe that due to the complexity involved, it is necessary to trade between security and timing. We briefly introduce the safe two-stage lock protocol and discuss a adaptation method to support the security of the transaction as timing, depending on the current state of the system.
We have implemented a compressor (XMilI) and compressor (XDemill) for XML data, used for data exchange and archive, which can be downloaded from http://www.research.att.com/sw/tools/xmill. XMill compressed about twice, such as gzip, about the same speed. it does not require DTD to compress and keep the input XML file faithful, including element order, properties order, PI, comments, DTD, etc.
Traditional protocols for distributed databases have high information, lock or restrict access to resources when implementing protocols, and may not be practical for certain circumstances, such as real-time systems and very large distributed databases.In this article, we introduce distributed protocols; it overcomes these issues by using clear linear algorithms to limit consistency as accuracy standards.
In fact, a software system must be processed and reacted from many sources (e.g., sensors) instead of human operators, requiring a re-thinking of the basic architecture of the DBMS in this application field. In this article, we introduce Aurora, a new DBMS, which is currently in Brands University, Brown University and M.I.T. We describe the basic system architecture, flow operators, optimization strategies, and real-time operations support.
The paper describes Paradise’s design and implementation, a database system designed to deal with applications of the GIS type.The current version of Paradise uses the client server architecture and provides extended relationship data models for GIS applications modeling.Paradise supports ~ SQL’s extended version and provides a graphic user interface for database queries and browsing.
Today, the widespread acceptance of the "Business Rules Independence" is necessary for information systems to better and faster adapt to the changes in the business environment, the paper tries how logically based on the database systems provide the appropriate technology for better "Business Rules Independence" these systems do so by exceeding "data independence" and providing "knowledge independence".
This article describes the use of the Object Database Management System (ODBMS) for storing high-energy (HEP) data.
We solved the query re-writing problem for TSL, a language query semi-structure data. We developed and introduced an algorithm, based on a semi-structure query Q and a set of semi-structure view V, found re-writing query, that is query access view and produced the same result Q. Our algorithm is based on the appropriate general query map, query and query composition - technology, developed to structured, relationship data. We also developed an algorithm to equally check the TSL query. We show the algorithm is sound and complete TSL, that is, it always finds every non-tested TSL re-writing Q and discuss its complexity to use some structural algorithm, we find some structural algorithm.
In this paper, we introduce the theory and algorithms required to create alternative evaluation commands to optimize queries containing external content, our results include a complete set of transformation rules, suitable for the new generation, transformation-based optimizers, as well as the combination list algorithms used with traditional optimizers.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
Abstract method describes each object in a symbolic image, using a vector consisting of the values of its specific characteristics (e.g., shape, gender, etc.).The method is different from the way the answers are calculated.In the classification method, the image is recycled based on whether they contain the same classification of the objects in the query.On the other hand, in the abstract method, recycling is based on the similarity of the characteristic vector values of these objects.
Therefore, the index structure can be easily used for queries. a typical example is a combination course that consumes two basic courses output. however, most of our work is not focused on the area of the relationship of the databases, but mainly refers to space and time data. for space databases, for example, we offer several implementing space combination algorithms.
The current state of internal database technology is "a size suitable for all" methods, whether the database is used to solve an OLTP problem or a DSS problem. all the leading database manufacturers think they have the right solution. this will be true if an OLTP application needs similar to a DSS application. but in fact. their needs are very different, the physical database's internal architecture designed for OLTP is very different from one designed for DSS optimization, not only these two architectures are different, they will also oppose.
In a New York Times article on May 21, 2020, poet and paper writer Cathy Park Hong complained that "nine works of Asian actors were cut or cancelled due to Covid-19 in New York City"1 Of course, the epidemic caused works around the world to be cancelled or delayed, including two major theatre meetings and festivals, will focus on performances in Asia Canada and Asia America: one will be held in Toronto and the other will be held in Hawaii, respectively during May and August 2020.
Decisions support applications to produce complex forecast queries.We show how complex queries expressed factors reveal important opportunities to use available indicators.We also introduce a new idea to relax forecast under complex conditions to create the possibility of factors.Our algorithm is designed to easily integrate with existing queries optimizers and support multiple optimization levels, providing different transactions between planning complexity and optimization time.
While sequential capture of databases consistency requirements and transaction accuracy properties through a concept, recent research tries to find accuracy standards that these two types of requirements are observed independently. Searching for more flexible accuracy standards is partly motivated by introducing new trading patterns, expanding traditional atomic trading patterns. These extensions occur because atomic trading patterns and sequentiality are found to be very limited when used in advanced applications (e.g., design databases) operate in the distribution, cooperation and abnormal environments. In this article, we developed a different accuracy standards, focusing on databases consistency requirements and transaction accuracy properties from different sizes. These extensions allow us to use the same standard and sequential trading patterns are found to be very limited in different applications.
This article presents the three seminar reports organized by Brian Cooper, newly edited as the seminar reports and technical notes.The first article summarizes the activities and discussions of the EDBT Summer School on XML and the database, contributed by Riccardo Torlone and Paul Atzeni.The second article by Ioana Manolescu and Tannis Papakonstantinou, provides an overview of the seminar on the implementation, experience and prospects of XQuery, held this year in Paris, combined with the ACM SIGMOD conference.
In this column, we reviewed these three books: 1. The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr. Answering a question that many of us may ask; and what we can do. Comments by Shoshana Marcus. 2. Parallel calculation elements, by Eric Aubanel. on this important and exciting field of calculation basics. Comments by Michele Amoretti. 3. The 21st Century Theory: Volume 1, by Bogdan Grechuk. background, background, and many important (available) mathematical theory statements in the past twenty years. Comments by William Gasarch.
Here we have studied Pearl Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Basic Bas
Wood pattern is the core of XML queries. Wood pattern in XML queries usually contains discounts, especially when a wide integrity limit (ICs) is present and considered. Clearly, the minimisation of the wood pattern has a great significance for effective XML queries processing. Despite the proposed various minimisation programs/algorithms, no one can take advantage of the wide range of ICS completely minimisation of the wood pattern in XML queries. The study aims to develop a innovative minimisation program and provide a new implementation algorithm. Design/methode/methode – queries increase/extension is taken as a necessary first step, most of the previous methods to obtain the minimisation pattern under some ICs.
I describe the need to build infrastructure for this field, the main research dynamics need to improve its work practice, as well as the research field that can contribute to the progress in the field. I emphasize that biodiversity science is essentially an information science, which is worthy of special attention from the computer and information science community, due to its scale and the characteristics of social and technological complexity.
Due to organizational or operational limits, the various data sources used by the company are usually not completely copied or fully consolidated under a single database, so the demand for data exchange and federal access is increasing.IBM is doing information integration technology work, allowing integrated, real-time access to traditional and emerging data sources, converting information to meet the needs of business analysts, and managing data positioning to performance, currency and availability, leading to the customer's e-business solutions fast, stable, easy to access.IBM's information integration infrastructure today supports SQL - mature, powerful query language - and supports multiple SQL extensions.
The Department of Health and Human Services of North Carolina, through the Department of Public Health and the Department of Elderly and Adult Services, passed a proof-based self-management course known as living health in NC, using toward learning to improve the ability of people to manage their diseases, including diabetes, and to prevent or slow progression of chronic diseases.The program is based on the University of Stanford's Chronic Disease Self-management Program (CDSMP) and in North Carolina through broad and diverse partnerships, within and between multiple systems.
Text models focus on the manipulation of text data. They describe the structure of the text, the operation on the text, as well as the limitations of the structure and operation. In this article, the common characteristics of machine-readable text are generally listed.
Our work focuses on the design and the creation of a dynamic data allocation system, which is continuously stored, i.e., the data delivered must retain the related continuous requirements (users specified connectivity in tolerable uncertainty) and resist the failure of the system. for this purpose, we consider a system in which a set of warehouses cooperate with each other and the source, forming an opposite network.
However, when the VE is too big to the main memory, the frame rate may become unacceptable. In this article, we combine tracks and database technologies such as indexing, querying and forecasting to improve a very large VE tracks performance. We implement a prototype tracks system called REVIEW (real-time virtual tracks) and evaluate its performance on a synthetic data set of 1 GB to simulate a large city range.
In this article, we focus on a key issue of the quality of business processes: analysis, forecasting and prevention of the appearance of exceptions, i.e. deviation from the necessary or acceptable behavior.We describe the problem and propose a solution based on data storage and mining.We then describe the architecture and implementation of a toolkit that allows the analysis, forecasting and prevention of exceptions.
In these environments, the best access performance can only be achieved if the data is classified by all the searchable event properties on the third layer of storage. Due to the high number of these properties, the basic data management facilities must be able to simultaneously process very large data capacity and very high data size. The proposed indexing technology is designed to promote the classification and high-size data on the third layer of storage. The structure uses the original space allocation program, which has many benefits to other space allocation technologies.
With the popularity of the Internet and the World Wide Web (Web), the demand for access to the database management system (DBMS) from the network is growing rapidly, here we describe the technology we invented with the bridge HTML, the standard label language and SQL, using the standard query language access to the DBMS. We proposed a flexible general purpose variable alternative mechanism, providing cross-language variables alternating between HTML input and SQL query chain and SQL result lines and HTML output, thereby allowing application developers to use the full ability of HTML to create query tables and reports, as well as SQL query and updates.
The TimesTen Performance Software’s Front-Tier product is an application layer database that interacts with disk-based relationship database management systems (RDBMSs) to innovative response time and breakthroughness, transaction load scalability, high availability, management and deployability. Front-Tier accounts often use a subgroup of the corporate database on multiple servers of the application layer and support SQL queries and updates to the account.
Decision-supporting applications are increasingly popular because more business data is kept online. These applications usually include complex SQL queries, which can test the ability of queries optimizer to produce an effective access plan. Many access plan strategies use the data provided by index or classification of physical orders. Classification is a costly operation, however. Therefore, it is essential that the classification is optimized in some way or avoiding all combinations. To this goal, this paper describes new optimization techniques to push the classification in the attachment, to try to reduce the number of classifications, and to detect the classification can be avoided because of predictions, critical or index.
This article describes a system, WebSemantics, completing the above tasks. We describe a building of the publication and discovery of scientific data sources, which is an extended world wide network architecture and protocol. We support the directory containing data sources of certain applications. We define a language of discovery sources and query their data. We then describe the prototype of WebSemantics.
The multilateral characteristics of these interactions have attracted significant attention to the language and vocal community. This article provides our current understanding of the manual and vocal forms and functions, the principles of functional interaction between vocal and vocal help communicate, communicating meaning and vocal production. In addition, we introduce a study over the sync of temporary vocal and vocal, including the special role of vocal and vocal.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
This paper introduces DEDALE, a space database system designed to represent and manipulate a space object by providing an abstract and non-special data model and language, DEDALE relies on a linear-based logical model, generalizing a limited database model [KKR90].
We offer a client-side encryption and replacement model in a client server database system, and compare this method to page encryption and double encryption strategy. Our encryption model is based on, and from its advantages, three key ideas. First, the client retains a similar description of the data in its encryption, which makes a similar description, as a remaining query, the double needs to answer a query, not present in encryption. Second, using the alternative policy to keep in a way of application similar areas, with the collection of double encryption.
The objective of the COKO-KOLA project is to express the rules of the rule-based optimizer in a way that allows to be verified through theoretical proof. In this paper, we consider the expression of the rules of re-writing too specific questions of conversion. These conversions require re-writing rules to complement the gramatical conditions to protect the gramatical shooting. This paper takes into account this translation expression, using the terms of re-writing rules, as well as the interpretation of the conclusion rules to guide the optimizer to determine whether the gramatical conditions remain.
An underestimated aspect of the search structure is the importance of high-performance search within the B tree nodes. Many attention is focused on improving the nodes, thus attempting to reduce the height of the tree [BU77,LL86]. [GG97,Lo98] discusses the importance of the size of the B tree page. A recent article [GL2001] discusses the internal nod architecture, but the topic is buried in a part of the paper. In this short note, I want to describe the good internal nod architecture and the long-term evolution of the technology, including understanding what problems have been solved in each nodes step, leading to the improvement of the nodes organization.
Testing the reliability of the high-performance tlansaetion processing system poses many difficult challenges that are not fully solved by traditional testing technologies. We discuss a new test paradigm that is dynamic and exploratory, and discuss its ability to address these challenges. We describe an implementation of the thii paradigm in the product, helping to effectively test reliable, high-performance trading processing systems at T~adem Computers Inc.
The cost function is a parameter to properly adapt to the various hardware characteristics. Combined with the basic model, we can describe the storage access mode of the database operations. The cost function of the database operations can automatically be derived from the corresponding combination of the basic model of the cost function. To verify our method, we conducted experiments using our DBMS prototype currency. The results presented here confirm the accuracy of our cost model, different operations. In addition to being used for query optimization, our model provides insight, not only in a main storage DBMS, but in a disk-based DBMS with a major storage stock.
The specific use of the entity relationship model is expanded to meet the requirements of the conceptual simulation of the geographical application, called the geographical ER model, proposed. processing with objects related properties, not because of the nature of the object, but because of the location of the object, requires processing - at the simulation level - with the object of space, location and size, space relationship, space dependency properties, as well as the size and universal representation. to this, within the framework of the ER and its derivatives, we introduce special entity collections, relationships, and add new structures.
We study the search engine and the index service query process. in particular, we study the search engine should prepare the result pages in the query processing phase. search engine users have been observed to browse very few results pages they submit query. this user's behavior indicates that predicting many results when dealing with an initial query is ineffective, as most predicted results will not be required by the user to start searching.
The digital library should be based on images or text display? who will better serve the user? experience and experiments show that the user can be employed and that each format has a technical advantage.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
We develop a simple but powerful MDBS classification, based on integrity limitations and the nature of the transaction process. For each confirmed model, we show how consistency can be by ensuring that the implementation is double-level sequentiality (2LSR). 2LSR is the accuracy standard of the MDBS environment rather than sequentiality. Our method is exciting to make sure that the 2LSR in the MDBS environment is relatively simple and the protocol ensures that the 2LSR allows a high level of competition.
Object Relationship Database Systems, i.e. the "universal server" is emerging as the next generation of big products of the business database system technology. including IBM, InfoMix, Oracle, UniSQL and other relative DBMS suppliers, today include the Object Relationship feature, all major suppliers seem to be able to provide full Object Relationship support in their products in the coming years.
The client server-oriented database management system is significantly different from the traditional centralized system in its architecture and application perspective. In this article, we introduce the architecture of the client server storage manager, we describe the competitor control and recovery mechanisms it uses. EOS provides a semi-optimistic lock program, based on two-level versions of two-stage lock protocols.
Many papers have reviewed how to effectively export a materialized view, but to our knowledge, no one has studied how to effectively import one. To import a view, that is, to install a update stream, the database system has to process new updates in time to keep the database "fresh", but at the same time has to process transactions and ensure that they meet their time limits.
The limitation provides a flexible and unified way to represent a variety of data capture space time behavior, complex simulation requirements, partial and incomplete information, and has been used for a variety of applications. The limitation of the database has recently appeared deep-integrated data capture restrictions in the database. The paper reports the first limitation of the object-oriented database system, CCUBE, and describes its specifications, design and implementation. The CCUBE system is designed to implement and optimize the high-level limitation of the object-oriented query language, as well as to directly build a software system that requires widespread use of the limitation of the database functionality. CCUBE data operating language, Constra Calculation Compreculation, is the integration of the first limitation of the object-oriented database system, CCUBE
Index and access methods have been a stage of database research - in fact, computer science in general - for decades. look at this year’s SIGMOD and PODS program content shows another bubble grown index file. considering hundreds of index files published in database literature, a break reflection seems to be in order. from a scientific point of view, it’s natural to ask why the final index solution has missed us so many years. what is the huge challenge of the index? what is the basic complexity or detail is this great laboratory? what will make up a successful completion of this research agenda, and what steps will move us better in this direction? or what is the problem, the spatial branch in many ways, we should constantly expect to solve the problem from a scientific point of view, it’s a natural question why it’s a final challenge?
They can reduce the need to scan the tables in the annexes. from the results of the customer reference label show its usefulness. We also describe the hacking collection, eliminating the following collection of the type forming group, the hacking collection allows for queries in the reference label, which was previously impossible. maintenance decision supporting the database (updated, added index, its physical layout changes) must be carried out regularly, and in these operations, the availability of the database becomes better. For this purpose, Tandem is introducing a series of new online data management operations, including data split added, drop, split and moving, as well as the index creation. We describe the part of the implementation as an example. The basic idea is "transfer to a basic idea and transfer data to a similar application, and then transfer data to a similar application.
The series is a common and important data category. Currently, the database system does not provide the appropriate series support: the series cannot be easily defined or easily manipulated. In addition, the series manipulation is not optimized. This article describes a language called the series manipulation language (AML), expresses the series manipulation, and collects the optimization of the AML expression technology. In the AML framework of the series manipulation, any external definition features can be applied to the structure of the series. AML can be adapted to different application fields, by choosing the appropriate external functional definitions. The file focuses on the series occurring in digital images, such as satellite or medical images in the database. the series can be called the series manipulation and can be re-selled as the result in the AML framework, can be optimized.
Data sources about the environment, energy and natural resources are very many worldwide. Unfortunately, users often face several problems when searching for and using relevant information. In the Ecobase project, we solve these problems in the context of multiple environmental applications in Brazil and Europe. We proposed the Distributed Environmental Information System (EIS) architecture based on the INRIA-based Le Select intermediate software. In this article, we present this architecture and its capabilities and discuss the learning and open issues.
The Diploma thesis covers the information system SAP R3 for the period of 1972.For decades, it has been improved and spread to almost all data processing fields of the company.Through different modules, the system covers human resources, production planning, material management, factory maintenance, project systems, etc.The basic navigation in SAP R3 is carried out through a lack of programs, known as transactions.The business application system is divided into standard and customer applications.The standard application is complex, broad, general, therefore, the convenient application is usually the customer program development, covers only the smaller areas of the company's needs.
The 1998 Nagano Olympics data management requirements are stronger than any previous Olympics in history, this speech will bring you behind the stage to talk about technical challenges and architecture, allowing you to process 4.5 million data and support 6.5 million web requests, reaching the top of 103K per minute, we will discuss the overall structure of the most comprehensive and widely used Internet technology applications in history, many products are involved in hardware and software, but this speech will focus on the database and web challenges, which will allow you to support this huge workload.
The system is designed to support the design, concept modeling and rapid prototyping of data intensity applications based on object-oriented databases (OODBS), which are modeled through a graphic user interface and coded the model produced to TQL++, the design language based on Mosaico.
So far developed XML filtering solutions focus on matching documents to a large number of queries, but do not deal with the output customization required for the emerging distribution information infrastructure. Supporting this customization can significantly increase the complexity of the filtering process. In this article, we show how to use an effective shared route matching engine to extract specific XML elements that need to produce customized output in the XML mail broker.
Our expansion committee chairman Karen Marschke-Tobier is considering this possibility, her group will be assisted by the Planning Committee, and they will also organize a seminar for mental health professionals held in Boston, after reviewing the subsidy proposals we received, one of the New York Psychoanalysis Institute and one of the Chicago Psychoanalysis Institute, the Executive Committee voted for approval of two applications.
News Editor Ramin Rahmani Information on this issue Report ASME Liquid Engineering 3 FED Technical Committee Report Liquid Applications and Systems Technology Committee 4 Micro and Nano-Scale Liquid Dynamic Technology Committee 5 Multi-Step Technical Committee Calculating Liquid Dynamic Technology Committee 7 Liquid Measurement and Instruments Technology Committee 8 Liquid Mechanical Technology Committee 9 FED Awards Honor and Awards 10 Liquid Engineering Awards 10 Robert T. Knapp Awards 11 Lewis F. Moody Awards 11 S. Gopalaknan-Flowserve Pump Technology Awards 12 Freeman Scholar Technical Articles
Disc-based database systems benefit from the competition between transactions - usually there is a marginal limit. for the main memory database system, however, the lock limit may have a serious impact on the performance. This article presents the SP, the transaction in the main memory system performs a series of protocols and evaluates its performance on the rigorous two-stage lock. SP news is that using a timetable and conversion, making a transaction begins its pioneer commitment records written on the disk, while ensuring no commitment to transaction reading unordered data. We showed seven times and twice the maximum by reading and updating the intensive work load, accordingly, we showed ten times and twice the same response time conversion.
Interesting patterns often occur at different levels of support. Based on the unified minimum support of the classical mining association, such as Apriori, whether missing the interesting low support patterns or the flaws from the project group’s bottle labels. A better solution is to use the support limitations, describing what project group’s minimum support is necessary, so only the necessary project group is generated.
Connectivity products can eventually be used to provide a "highway" between computers that contain data. IBM has provided a powerful verification concept with their "information warehouse."DBMS suppliers provide portals to their products, SQL in many older DBMS, making it easier to access the data of the standard 4GL products and application development systems.The next step required for data integration is to provide (1) a common data dictionary with a concept program in data to cover the many differences that occur when the database is independently developed and (2) a server can access and integrate the data base using the data dictionary information in this article, we discuss InterViso, one of the first commercial federal data base products.
Active Object-Oriented Database Management System (AODBMS) is looking for more and more applications in different applications, especially cooperation and long-term activity management. In this paper, we presented a competition control mechanism to conduct open transactions in AODBMS. It uses the transaction grammar to control cooperation and competition between transactions. Atomic AODBMS transactions are considered as the basis transactions. A complex transaction type consists of basic and complex transactions, a set of separate ECA rules and a country transition model.
The multidimensional database is a database that supports the effective implementation of complex business decision queries. the queries can be significantly improved by storing the appropriate collection of materialized views. these views are selected from the multidimensional light, its elements represent the problem solution space. the past proposed several techniques to make the choice of the materialized views of the database, reduced the amount of size. when the size and complexity increase, the recommended technology will not expand well.
In this article, we study a simple SQL extension that allows the query author to clearly limit the cardinability of the query results. We review its impact on the relationship DBMS query optimization and running time execution components, introducing two methods - conservative methods and aggressive methods - using cardinary limitations in the relationship query program.
SHORE (Scalable Heterogeneous Object REpository) is a stable object system that is being developed by the University of Wisconsin. SHORE represents an integration of object-oriented databases and file system technologies. In this article, we provide the goal and motivation for SHORE and describe how SHORE provides the features of both technologies. We also describe some of the new aspects of SHORE architecture, including interactive peer-to-peer server architecture, server customization by adding value to server facilities, and multiprocessor system extensibility support.
We explore two types of drop drop drop drop drop drop drop drop drop drop drop drop drop drop drop drop drop drop drop drop
We presented a framework that allows users to access and process data in a uniform way, whether it lives in a database or file system (or both). a key issue is the performance of the system. We show that the text index, combined with the new development of optimization technology, can be used to provide an effective high-level interface stored in the file. In addition, using these technologies, some queries can be evaluated faster than the standard database.
In this article, we introduce a unified framework called rainforest classification tree building, which separates the classification aspects of the algorithm from building the core characteristics of the tree, determining the quality of the tree. The general algorithm is easy from literature (including C4.5, CART, CHAID, FACT, ID3 and extension, SLIQ, SPRINT and QUEST). In addition to its universality, in which it provides a wide range of classification algorithm versions, our method also provides performance improvement of more than one factor over the three SPRINT algorithm, the fastest classification algorithm proposed before.
We describe a system that supports voluntarily complex SQL queries with “uncertain” forecasts. queries are based on a probability model, result ranking, like in information recycling. our main focus is on queries evaluation. we describe an optimized algorithm that can accurately calculate most queries. however, we show that some queries are data complexity is #P- complete, which means these queries do not recognize any accurate evaluation method. for these queries, we describe an approximate algorithm and a Mont Carlo simulation algorithm.
In this paper, we presented a prototype of Earth Science Data Management, which is new, it requires a central view of the task. Our prototype - known as "BigSur" - is shown in the background of its use by two geographically distributed scientific groups requiring data storage and processing requirements. BigSur currently stores 1 data, about one thousand EOSDIS must be stored. We claim that the design principles contained in BigSur provide sufficient flexibility to the task to the Earth's difficult scientific and technological goals.
MITRE provides technical assistance, system engineering and purchase support to large organizations, especially U.S. government agencies, we help our customers plan complex systems based on emerging technologies and implement systems based on non-commercial products. In MITRE’s research program, instead of emphasizing the concerns of the DBMS or CASE suppliers, our research emphasizes the need to use these products to organize issues. For example, we are favorable to us to build business products rather than change them inside.
In this article, we presented a new limitation definition called XFD, which captures structural and sexual information. We presented a set of re-writing rules of XFD and using them to design a multi-time algorithm, based on a input group of XFD, calculating a reduced group of XFD. Based on this algorithm, we presented a redundancy remove storage map from XML to the relationship called RRXS.
In many emerging applications, Ad hoc queries and/or interconnections also require data processing before or during interconnection. For these applications, we have developed a system that processes Ad hoc and continuous queries as well as data processing and queries, allowing new queries to be applied to old data and new data to the old queries.
In this article, we describe JurWordNet’s vocabulary resources and core law Ontology as the application of the descriptive vocabulary in the simulation field of law, which can be seen as a grammar component of the digital government’s globally standardized framework, and the content descriptive model provides a structured storage of knowledge designed to support the grammar interaction between the public administration and the communication process to citizens.
Xyleme is a dynamic warehouse of Web XML data that supports query assessment, change control and data integration.We briefly present our motivation, overall architecture and some aspects of Xyleme.The project we describe here was completed at the end of 2000.The prototype is now being converted by a startup also known as Xyleme to a product.
Text is a transparent information type, many applications need to ask for text sources, in addition to structured data, this paper studies the problem of query processing in a system, it easily integrates an expandable database system and text access system. We focus on a category of link query, which includes the connection between text and structured data, in addition to the choice of these two data. We adjusted the technology of distributed query processing and introduced a new category of test-based connection method, which is useful for connection with text system, we introduce the cost model of various alternative query processing methods.
Classification, which includes finding the rules, classification of a particular da.ta set as a separate group, is a category of data mining issues. So far, the method for mining classification for the big database is mainly based on the decision-making symbolic learning method. Based on the neura.l network connectivity method is considered not suitable for data mining. One of the main reasons referred to is that the knowledge generated by the nerve network is not clearly represented in the form of the rules suitable for human verification or interpretation. This paper reviewed this problem.
In the real world, many database applications are no longer built on the top of a separate database system. On the contrary, the general (standard) application system is used, where the database system is an integrated component. SAP is the market leader of the integrated business management system, its SAP R/3 product is a comprehensive software system, integrated finance, material management, sales and distribution, etc. From a building point of view, SAP R/3 is a client/server application system, with a relative database system as a background.

The performance of the object-oriented database system (OODB) remains a problem for today's designers and users. Therefore, this paper aims to present a universal difference incident random simulation model, known as VOODB, to evaluate the overall OODB performance, as well as optimize the performance of methods such as classification, in particular. This optimization method has undoubtedly improved the performance of OODB. However, they are also always causing an advantage to the system. Therefore, it is important to evaluate their accurate impact on the overall performance. VOODB has been designed for a universal difference incident random simulation simulation model, using simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation simulation
SkyServer provides Internet access to the Sloan Digital Sky Survey (SDSS) data for astronomers and scientists, which describes the SkyServer's goals and architecture, it also describes our experience operating on the Internet, the SDSS data is public and has good documentation, so it is a good test platform to study database algorithms and performance.
The behavior of scientific and engineering research is becoming critical to the effective management of scientific and engineering data and technical information. The rapid progress of scientific tools, computers and communication technologies enables scientists to collect, produce, process and share unprecedented amounts of data. For example, the mission of Earth Observation Systems Data and Information Systems (EOSDIS) is to manage the data from NASA's Earth Scientific Research satellite and field measurement programmes, as well as other necessary data to explain these measurements support global change research. In addition to being able to process a traffic of 1 trabat data daily to 2000, EOSDIS also needs to provide transparent access to immaterial data stored in the files of several U.S. government agencies and national organizations.
One of the key issues of the development and integration of medical research experts is the problem of data integration. In most cases, the general information about the patient and the data about the conducted research process exists, as part of multiple separate information systems, each using its own program to introduce and store information. The paper proposed a solution to integrate research and patient data in medical facilities using a formal projection mechanism, allowing the unified data to be extracted from different data sources.
This paper involves finding output (exceptions) in the multi-dimensional data set. the identification of output can lead to finding truly unexpected knowledge in the fields of e-commerce, credit card fraud, etc., and even analyze the performance statistics of professional athletes. The existing method we see in the big data set to find output can only effectively process the two dimensions/properties of the data set. Here, we study the concepts of most DB (distance base) output. Although we provide form and experimental evidence to show the usefulness of the DB output, we focus on developing algorithms to calculate these output. First, we provide two simple algorithms, both with complex O(base), not based on N(base) numbers, not based on N(base) numbers, not based on N(
In this article, we reviewed the development problems of information visual systems and presented the framework for their construction. the framework deals with the components that must be considered in providing effective visual aspects. the framework uses the declaration-oriented language to determine; the result of the object model can be maped to the various graphic user interface development platforms. This provides general support for the developers of visual systems. a prototype system exists, allowing the study of alternative visual data sources.
Integrated access to information, spreading multiple, distribution, and abnormal sources is an important problem in many scientific and business fields. Although many work has been under the cost standards of query processing and selection of a plan, we know very little about the important issues that include the information quality aspects of query planning. In this article, we describe a framework of multi-database query processing, fully including the quality of information in many aspects, such as integrity, time accuracy, accuracy, etc. We have no doubt incorporating the information quality into a multi-database query processor based on a viewed editing mechanism.
In 10 years, people will have hundreds of interconnected computing devices around them. many connections will be wireless, many devices will store data among them. a hundred million or more mobile users have a hundred devices, we face a very large (IP) network, a very large (100B) small and hidden or embedded database, as well as "always" connected to the network of all types of databases. how to solve, for example, security, copying and durability challenges? some answers are already visible in today's mobile phones and internet networks but many answers even possible questions are not clear.
We found a surprising law that regulates the selectivity of spatial fusion in two sets. such spatial fusion examples are "to find a library, in 10 miles of school." Our law stipulates that the number of these qualified couples follows a power law, whose agents we call "double calculation exposer" (PC). We show that this law also possesses the self-spatial fusion of 10% of fast communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication communication
Red Brick WarehouseTM is a business relationship database management system designed specifically for queries, decision support and database applications. Red Brick Warehouse is a software single system that provides ANSI SQL support in an open client server environment. Red Brick Warehouse is an optimized architecture from traditional RDBMS products to provide high-performance most, high-intensity queries applications. In these applications, the workload is much higher than the complex SQL SELECT operations, but does not update the database.
This paper provides an overview of the 1995 International Temporary Database Seminar, which summarizes the technical paperwork and related discussions, as well as three panels: “What is TSQL3?”, “Temporary Data Management in Financial Applications” and “Temporary Data Management Infrastructure and Beyond”.
Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation Interpretation
Thor is an object-oriented database system, designed to be used in an unusual distributed environment. It provides very reliable and very available permanent storage objects, and supports two areas that these objects are safe sharing by other object-oriented databases written applications. Security unusual sharing long-lived objects need to be included: The system must ensure that the application only interact with the object by reference method. Although security issues are important, most object-oriented databases through security to avoid paying the related performance costs. This paper provides an overview of Thor's design and implementation. We focus on two areas that these objects are safe sharing with other object-oriented databases written applications. First, we discuss the technology to ensure that it is safe to discuss the application and not to share the application.
Due to the recent growth of the World Wide Web, many space time applications can obtain the information they need from a network source. In this demonstration, we showed the WorldInfo Assistant, an application that extract and integrates space, time and other information about the world regions. The application also provides data integrated with the satellite images of the world regions.
Reusable ontologies are increasingly important, such as information integration, knowledge level interaction and knowledge base development. We have developed a range of tools and services to support the process of consensus by geographically distributed groups on common ontologies. These tools use the world wide network, allowing broad access and provide users to be able to publish, browse, create and edit ontologies stored on the ontology server. Users can quickly collect a new ontology from a module library. We discuss how our system is built, how it uses existing protocols and browsing tools, our experience supports hundreds of users.
The paper provides an overview of the current database research activities within the Intelligent Information Systems Group of the Department of Computer Science and Engineering of the Arizona State University, with our research focus on the integration of data and knowledge management issues, with a specific focus on multimedia systems, objective-oriented databases, active databases, driven databases and unusually distributed databases environments.
We discussed the design and implementation of the object-oriented database viewing mechanism, which allows the structure and behavior of the objects stored in the database to be restored. with the view extension data model is awarded and then the implementation of the prototype function is introduced. the paper focuses on the implementation requirements of the object-oriented viewing mechanism, from the viewing description language to the search and updating of the viewing strategy optimization, such as the viewing materialization and consistency maintenance.
The prophecy of teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching is about teaching.
The popularity of data in RDF format has led to the appearance of a variety of specialized management systems. While adapting to the complexity of SPARQL queries - taking into account its internal diversity - is critical, the current approach is not large-scale, facing quite complex non-selective connections, leading to an unusual growth of execution time. In this example, we introduce H2 RDF+, a RDF store, effectively perform distributed Merge and Sort-Merge connections, using two multi-index systems HBase index. Through an enthusiastic planner, which includes our cost model, it is suitable to command a single or multi-machine queries to perform based on the complexity of connections. In this text, we allow its current scientific and real-time contribution to the real-time connections of H2 RDF and
Today’s industrial control systems store a large number of monitoring sensors data to optimize industrial processes. Over the past few decades, architects have designed these systems primarily assuming that they are closed, the factory side IT infrastructure has no vertical scalability. Cloud technology can be used in this context to save local IT costs and allow higher scalability, but their maturity for high responsibility and intensity requirements of industrial applications has not yet been understood.
Currently, the majority of business intelligence data (BIDs) are distributed in incompatible tools such as RDBMS, OLAP engines and disclosure panels. The bridges allow the exchange of data and data availability, thereby providing a small degree of BID sharing, but due to the lack of data standards, this sharing is limited to a specific c installation. This creates an unacceptable situation in the analysts cannot get a complete image of the business. A solution is necessary, collaborative analysis processing (CAP) solution, it imposes a very close integration of tools between, so (meta) data exchange, change management, scale and data availability is good, as in RDBMS, and the performance of the APOL query is good, as in any special APOL engines.
In this article, we discuss the need to automatically divide text files into thematic basics, such as ACM Digital Library and Yahoo! existing local methods to build a class of each class of thematic basics. However, local methods do not solve classes close to the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the basics of the bas
This presentation will combine active DB technology in an open, unusual environment with the presence of the web page of the Norwegian user, which is described through the user and the Internet available vehicle configuration. a scene is being developed to provide useful features such as tools to adjust, maintain and diagnose information processing with the corresponding workflow, as well as convenient features such as location-dependent language translation support and traffic information.
In the storage database (ODB) system, the database owners publish their data through multiple remote servers, which aims to allow customers to access and request data on the network margins more effectively to do analytical assessments, because the server may be unreliable or may be damaged, query verification becomes an essential component of the ODB system, the existing solution of this problem is mainly focused on the static scene and based on the ideal characteristics of some encrypted primary material. In this work, first, we defined the various basic and practical cost measurements related to the ODB system. Then, we analyzed some different methods in finding a solution that is best suitable for all the important measurement measurement measurement measurement measurement measurement measurement measurement measurement measurement measurement
The Bavarian network is an appropriate tool to deal with the uncertainty of the real application. The Bavarian network architecture represents statistical dependence between different variables. In the data mining field, the rules of association and mutual relationships can be interpreted, as well as expressed statistical dependence relationships. K2 is a well-known algorithm capable of learning the Bavarian network. In this paper, we introduce two extension K2 called K2-Lift and K2-X2 using the two parameters usually defined with the rules of mutual relationships and mutual relationships.
Caching has been presented (and implemented) by the OLAP system to reduce the response time to a multi-dimensional query. The previous work of such query has been taken into account for desktop-level query and query-level query. desktop-level query is more suitable for static charts. On the other hand, query-level query can be used in dynamic charts, but for "big" query results are too tensive. query-level query has further disadvantages of smaller query results because it is effective only when a new query is submitted to a previous query query. In this article, we submit query-level query for small areas called "query". query-level query allows detailed query, query-level query, query-level query, query-level query
In the digital library system, the document is available in digital form, so easier to copy, its copyright is easier to be violated, which is a very serious problem as it prevents the owner of valuable information with the authorized user to share it, there are two main philosophies to solve this problem: prevention and detection, the first actually makes the unauthorized use of the document difficult or impossible, and the latter makes it easier to find such activity. In this article, we recommend a system to record the document and then detect a copy, whether a complete copy or a part of a copy. We describe such detection algorithms, as well as assess the measurement required for the detection mechanism (covering accuracy, efficiency and security). We also describe a work prototype called COPS, implementing the problem and providing the appropriate number of references to the
Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules Alliance Rules
Relational database systems have traditionally optimized for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM), which significantly improves cache performance by grouping all values of each attribute within each page.
In the digital library image recycling queries can be based on the similarity of objects, using several characteristics properties, such as shape, structure, color or text. These multifunctional queries return a ranking result setting rather than accurate game. In addition, users want to see only k top objects. We introduce a new algorithm called Quick-Combine (European patent delay, EP 00102651.7) combining a multifunctional result list, ensuring the correct recycling of k top ranking results. To evaluate the collection of almost any combination of features can be used, including weight queries. Compared to the Fagin algorithm, we have developed an improved end state, combined with a heat control flow to allocate a specific factor we can display data through the computer and different sizes of data through the computer.
The date of January 31, 2018 marks the adoption of a three-stage law in Hong Kong that prohibits the Eagle trade that enters into force on December 31, 2021, which follows the decision of the Continental China to prohibit this practice from December 31, 2017, these new rules, derived from the International Convention (CITES), are applicable to these places as they represent (with Japan) the world’s main destination of Eagle, legally and illegally, and since the 1950s.
In this article, we introduce a method of integrity maintenance, by automatically generating the integrity of the rules of production implemented. the limitations are expressed as a specific formula of the field relationship calculation; they are automatically translated into a set of repair actions, encoded into an active database system of the production rules. the production rules can be reversed (they perform the same limitations in different ways) and conflicts (because the repair of one limitation can lead to the violation of another limitation of the rules. therefore, it is necessary to develop the technology, to analyze the characteristics of the combination of the active rules, and to ensure any calculation of the production rules after any wrong transaction ends, and to produce a unified national database in these guidelines, we describe a specific structure structure structure structure structure structure structure structure structure structure structure structure
The database supports the analysis of historical data. This often involves integration within a period of time. In addition, the data is usually included in the growing order of time properties, for example, the date of sale or the time of temperature measurement. In this article, we propose a framework that uses this attachment only to update the nature, because of a time properties. The framework allows us to integrate a lot of new data into the warehouse and effectively produce historical summary.
Based on the data transmission model, the client server database system can use the customer's memory resources by cutting a copy of the data component on the transaction border. cutting reduces the need for data from a server or other websites on the network. to ensure that this cutting does not lead to transaction sequence violations, it requires a transaction cutting consistency maintenance algorithm. Many such algorithms have been proposed in literature, because all of which provide the same features, performance is the choice of one of the main issues. In this article, we presented an algorithm that describes the design of the different cutting space consistency maintenance algorithm, and shows how the proposed algorithm is similar to another.
The reference mark belongs to the standard list of tools deployed in the database development. evaluating the ability of the system to analyze the actual and potential bottles, of course, comparing the advantages and disadvantages of the different system architecture has become an indispensable task as the database management system has grown in complexity and capacity. In the development of the XML database requires a reference framework to become increasingly clear: in the past proposed many different ways to store XML data, each with its real advantages, disadvantages and consequences, spread through the complex database system level, needs to be carefully considered. different storage programs make the data characteristics variable. However, there is no conclusion methods to evaluate the differences of these data. In this reference framework, we have proposed many different ways to store XML data, with each of its real
In this article, we describe the data supply input support in AsterixDB, a open source big data management system (BDMS), providing a platform for storage and analysis of large quantities of semi-structure data. Data supply is a mechanism that continues to reach a BDMS from an external source and continues to populate a permanent data set and related indicators. The need to stick and index the "fast flow" of high-speed data (and support ad hoc analysis queries) is unlimited. However, today's art state involves "integration" of different systems. AsterixDB is different, as a unified system with "national support" data supply.
We introduced a new algorithm to calculate the spatial merger of two or more spatial data sets when the indicators are not available on them. Size separated space merger (S<3J<) imposes a relatively simple data space breakdown, contrary to the previous method, no need to repeat the entity from the input data sets. Therefore, its execution time depends only on the size of the existing opposite data sets. We described S<3J< and proposed an analytical evaluation of its I/O and processor requirements compared to the previously proposed algorithm. We showed S<3J< has a relatively simple cost estimation formula that can be used by optimizing the query. S<3J< can effectively apply the software on the existing opposite data sets size We have proposed S<3J
Based on the hash extended distributed data structure (SDDS), such as LH* and DDH, for connected computers (multicomputer) network has been proven to open a new perspective of file management. We propose a family order SDDS, known as RP*, providing orders and dynamic files on multiple computers, thus more effectively processing the range of queries and orders of file channels. The basic algorithm called RP*N, building files with the same key space division as a B tree, but by using multiple computers, avoiding indicators.
We introduce the issue of the rules of the mining association into a big relationship table, which contains quantity and category properties. An example of such association may be "10% of the people between the ages of 50 and 60 have at least 2 vehicles."We deal with quantity properties, by accurate allocation of the properties, and then combine the attached divisions according to the need.We introduce some integrity measures to quantitate the information lost due to the divisions.The direct application of this technology can produce too many similar rules.We solve this problem using a "greater than expected value" rate to determine the interesting rules in the export.
The link between the projects in the mining in the big transaction database is a core problem in the field of knowledge discovery. When the database is distributed between several shared unparalleled machines, the problem can be solved by the distributed data mining algorithm. A such algorithm, called CD, was proposed by Agrawal and Shafer [1] later improved by the FDM algorithm by Cheung, Han et al. [5]. The main problem of these algorithms is that they are not large-scale and the number of divisions differ. Therefore, they are useless to use in the modern distributed environment, such as the opposite system, in which hundreds or thousands of computers can interact. In this paper, we will implement a new algorithm, which is distributed by the algorithm, which is distributed by the alg
The automatic database design system contains the knowledge of the database design process. however, their knowledge of the database is developing is not significantly limited to their usefulness. the overall knowledge of the database design business method has been developed and implemented in a system called common sense of business, which collects the facts about the application field and organizes it to a base of knowledge, background-based. this knowledge is used to provide users with intelligent advice on the entities, properties and relationships, including in the database design. a remote functional method is used to integrate the specific facts obtained from the personal design meeting to the knowledge base (learning) and application knowledge to subsequent design issues.
In the second part, we present a wide range of classification methods and divide them into three groups - model and optimized methods, link and density-based methods, and mixed methods. a detailed comparison shows the strengths and weaknesses of existing technologies and reveals the potential for further improvement. in the next two parts, we discuss the database technology, suggesting improving the efficiency and efficiency of the classification process. the four main categories of technologies can be used for this purpose are based on model and optimized methods, link and density-based methods, and a detailed comparison shows the strengths and weaknesses of existing technologies and reveals the potential for further improvement.
The growth rate of the database size and response time requirements has made a breakthrough in processors and mass storage technologies. One way to meet the demand for processing power and input/output bandwidth in the database application is to have multiple processors, smoothly or closely connected while serving the database demand. The technology developed over the past decade has made commercial parallel database systems a reality, which has become the strength of traditionally based on the main frame big database applications. This paper describes the DB2® parallel product, the prototype development developed by the IBM Institute, is now being developed together with the IBM Toronto Laboratory.
Half-structured files (e.g., magazine art, idols, e-mail, television programs, mail orders directory,..) a.re often not clearly written; the only available t,ype information is suggested structure. a clear t,ype however, it is necessary to a.pply the objective technology, such as the type of specific method. in this article, we introduce a.n experimental vector space cla.ssifier to determine the type of semi-structured file. our goal is to design a. high-performa.nce classified in t, accuracy (reaction and accuracy), speed and scalability.
The author's view of databases mining is the combination of mechanical learning technology and the performance of databases technology is introduced.Three categories of databases mining problems involve classification, association and sequence are described.These problems can be considered uniformly as the need to find the rules embedded in a large amount of data.A model and some basic operations rules of the process of detection are described.It is shown how databases mining problems are considered this model map, and how can be solved by using the basic operations.An example is the classification algorithm obtained by combining the basic rules of the operation.This algorithm is effective to find the rules and accuracy with the current ID3.
XML has appeared in a standard data exchange format for Internet-based business applications. This creates the need to publish existing business data, stored in a relationship database, such as XML. The general way to publish relationship data, such as XML, is to provide XML view of relationship data and allows business partners to use XML query language to query these views. In this article, we solved the problem of evaluating XML query, the view of XML query relationship data.
We describe a new method to classify and apply it to analyze and mining categories of data "category data" we refer to the fields of tables that cannot be commanded naturally - for example, the name of the car manufacturer, or the product name provided by the manufacturer.
The paper introduces a new effective merger algorithm to improve the speed of the merger relationship operations.Using dividing and conquering strategies, in the new merger algorithm order-oriented filter technology as quickly as possible to filter unnecessary merger, while currently there is no existing merger algorithm using any filter concept.Other merger algorithm may carry unnecessary merger to the last moment of merger properties comparison.The four merger algorithm are described and discussed in this paper: merger algorithm, merger algorithm, merger algorithm, merger algorithm, merger algorithm, merger algorithm, merger algorithm, merger algorithm, merger algorithm, merger algorithm,
The next generation of decision-making support applications, in addition to being able to process a lot of data, needs to be able to integrate and reasonable data from multiple, unusual data sources. often, these data sources will significantly reduce the costs of query processing in various aspects, such as their data models, they support query language, and their network protocols. in addition, usually, they spread across a wide geographical area. processing decision-making support query costs in this case are quite high. however, the processing of these queries transition often involves conversion, such as repeated access to the same data sources and repeated execution of similar data sources. In this article, we propose a complex processing solution, including multiple query, multiple query, multiple query, multiple query, multiple query, multiple query, multiple query, multiple query, multiple query
Three types of series locks suited to the B tree index are designed to study in detail and compare their advantages and disadvantages. Traditional series include indicators, sheets, key range or key values. On the contrary, the locks on the separator key on the internal B tree page can protect different sizes of key columns. Finally, for the key consisting of multiple columns, different sizes of key budget allow the third form of series locks. Each method of these records requires appropriate implementation of the technology. Here the technology to explore includes nod divisions and mergers, locks divisions and locks divisions, as well as the changes in the online locks divisions. These techniques are the first design that enables in a series a series a series a series a series a series a series a series a series a series a series a series
Previous Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post
We extend the relationship data model to include some orders in the data field, which we call the relationship model of orders. In the extended model, we define the relationship algorithm of some orders (PORA), allowing the order forecast to be used in the formula of the operator (σ).
A new type of algorithm, known as AlphaSort, shows that the commodity processor and disc can handle the commercial package work load.Using the commodity processor, memory, and a series of SCSI disks, AlphaSort runs the standard type of reference in seven seconds.This hit the best release record on the 32CPU32 disk Hypercube of 8:1 on another reference label, AlphaSort classifies more than a billion bats in a minute.AlphaSort is a cache-sensitive, memory-intensive type algorithm.
This traditional method is effective to try to reduce the number of I/O operations, but also the top source compared to the memory system. To avoid this top, the memory database system thus completely leaves the bubble management, which makes the processing of the data set bigger than the main memory, very difficult. In this work, we review this basic score and design a new storage manager to optimize modern hardware. Our assessment, based on TPC-C and micro reference labels, shows our method compared to the pure memory system, when all the data live in the main memory while, as the traditional bubble management, it can fully manage the data set bigger than the main memory.
Daytona #8482; data management system is used by AT&T to solve a wide range of data management problems. For example, Daytona manages a four-trabit database, its largest table contains more than a billion lines. Daytona's structure is based on its advanced query language Cymbal (including SQL as a subgroup) completely to C, then C is written to the object code. The structure generated by the system is fast, powerful, easy to use and managed, reliable and open to UNIX#8482; tools.
We propose a new paradigm, namely, Ratio Rules, which are quantifiable in that we can measure even the “goodness” of a set of discovered rules. We also propose the “guessing error” as a measure of the “goodness” a big data error, that is, the root-mean-square error of the data matrix (e.g., customers $times$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
“... making a computer so confusing, so suitable, so natural that we use it, not even thinking about it.” • “Virtual reality is almost the opposite of virtual reality, virtual reality puts people into a world created by computers, and virtual computers force computers to live with people in this world.
The latest work in the field involves the processing of multi-level transactions, expanding the logic of the MLS query language, and using the MLS principle in the e-commerce field. However, there is a basic error in the MLS logic that prevents the processing of unchanged collective query and physical entity related query, in which some information in the database can be obtained from the external world.
The dramatic growth of the Internet created a new problem for the user: the location of the related file source. This article introduces a framework (and experimental analysis of a solution) of the problem that we call the text source finding the problem. Our method consists of two stages. First, each text source exports its content to a centralized service. Second, the user provides a query to the service, returns a order list of future text source. This article describes the GlOSS, the server word, has two versions: bGlOSS, provides a Boolean query recycling model, and vGlOSS, provides a vector-space recycling model. We also introduce the hGlOSS, provides a distributed version of the system.
We introduce an algorithm to answer these questions in a multi-dimensional database, using the choice of crossing a multi-resolution combination (MRA) tree storage point data. Our method provides 100% confidence of the value of combination and work unusual, with improved quality of answers, to some error requirements are met or time limit, such as achieving. using the same technology, we can also answer the combination question accurately, our experiments show that even the accurate answer of the data structure and the algorithm is very fast.
In 1998, a small group of people started a similar project designed to convert our product, NonStop SQL/MX, to an active RDBMS. This project tries to integrate the functionality of the transaction exchange system with relative tables and SQL, using simple extension to SQL synthesis and ensuring clear definition of queries and transaction exchanges. The result is the first commercially available RDBMS, which includes flow.
Data storage includes technology and industrial practices to systematically integrate data from multiple distributed data sources, and use these data in recording and integrated forms to support business decisions and business management. Although many databases technologies have been reviewed or newly developed in the background of data storage, such as visual maintenance and OLAP, some business has paid little attention to the design, management and high-quality service management.
Based on generalization, we propose a smaller permissible window size, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method, general matching method,
XML is popular for data exchange and data publication on the network, but it involves errors and incompatibilities related to real-world data. Therefore, it requires XML data cleansing, which requires the solution to solve the confusion of copying detection in XML. The sequence and semi-structure characteristics of XML are very different from ∞at and structural relationship models, so far we have gained a major focus in copying detection. We take into account the four major challenges of copying detection to develop effective, electronic science and extensible solutions.
This second edition systematically introduces the concepts of ontologies to non-expert readers and details how to apply this concept framework to improve the access to enterprise information and knowledge, as well as internet-based e-commerce. It also describes ontology languages (XML, RDF and OWL) and ontology tools, as well as the application of ontology.
With the enormous popularity of the network, the world is witnessing the unprecedented demand for data services. At the same time, the Internet is developing to an information super highway, constantly adapting to the broadcast content mixed with existing and emerging communication technologies, including wireless, mobile and mixed networks. Using these new technologies, we are proposing a mixed system that effectively combines the broadcast to large-scale data dissemination and unique personal data delivery. In this article; we describe a technology using the broadcast media to store the frequently required data, as well as an algorithm, constantly adapting to the broadcast content to match the data base hot points. We show that hot points can be observed directly through this paper; we describe a technology using the broadcast technology to display it effectively and it is very effective on its system
Two new space fusion operations, the distance fusion and the distance semi-fusion, are introduced, where the fusion output is the fusion of the distance command between the space properties. Additional algorithms are submitted to calculate these operations, which can be used in the pipe mode, thus ignoring the need to wait for their completion when only a few fusion is necessary. The algorithms can be used with a large level of space data structure and random space data type in any size.
This paper introduces a regional split strategy for the physical database design of the two best physical database design is to determine a regional split strategy for the best configuration of the physical file for a specific interval query. Recently, many multi-dimensional file organizations support multiple file access interval ratio has been presented in literature. However, no effort for the physical database proportion design. First, we show that the query processing performance is by a regional split different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different forms different
For years, SQL standards have beenined and enhanced by the NCITS Technical Committee H2 in the Database in the U.S. and ISO/IEC JTC 1/SC32/WG3 Database Language Working Group internationally. In SQL:1992 after the release, the Group began to publish SQL as a basic file (SQL/Foundation) and several separate parts. These parts began with SQL/CLI (Call Level Interface) and SQL/PSM (Sustainable Storage Module) in 1995 and 1996, respectively. Interest in XML has been in the past years in the software provider, user company all sizes, and grows within the standard range. NCITS H2 and SC32 both approved a project for a new part, SQL, part XML, part XML, part SQL, part SQL, part SQL, part SQL, part SQL
E-Commerce applications pose new challenges to the database systems. E-Commerce applications such as portals, market sites and online stores (Amazon.com, eBay.com) often face the problem of rapidly integrating new catalogues from different sources into existing catalogues (the "master" catalogues). E-Commerce applications allow the database to support the content of the database directly from the internet. However, the most popular web interface is the Google-style search box and the queries submitted from this interface may not include the name or unit.
HTTPHypertext Transfer Protocol Requests, Java Virtual Machine embedded in the Web browser running application, or remote program call protocol), researchers and practitioners also realize that any information service technology should face the problem of interactivity, that is, the application’s ability to exchange data and activate data manipulation function, using its domain model.
The remote-covered vision provides the basic data of the remote-visual policy for the conduct of visual policy, as well as the changes in time information, to monitor sustainable land management practice. In this paper, the current use of remote-visual specific to create the ideal of sustainable land management is reviewed and explores the potential of the future (new) satellite systems to promote the potential of sustainable development. Other factors can be compared to successful sustainable development (i.e., collecting good policies and participation methods) and compared to the information requirements. The visual policy of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of the visual technology of
LeSelect is a mediator system that allows scientists to publish their resources (data and programs) so that they can access transparently. Scientists can usually send queries, these queries perform techniques, by fully exploiting the distributed mediator system structure and involving the execution of expensive functions (compatible with the program). In addition, queries may involve large objects, such as images (such as the archive of meteorological satellite data). In this context, the transfer of large objects costs, and cited expensive functions are the control of the execution of the time. In this article, we first propose three queries performing techniques to reduce these costs by fully exploiting the distributed mediator system structure, such as LeSelect and then we distribute strategies that include expensive functions.
The definition of the database is used to manage structured data, combining the Internet to the extension range of terminal users, triggering new data access scenarios, this paper first studies how to fundamentally integrate the database ranking. Users often want to express non-traditional confusion queries with soft standards, instead, with Boolean queries, and explore what options are available in the database, and how they analyze these processes are not in accordance with the queries standards. The traditional database management system (DBMS s) has become increasingly inappropriate for such a new scenario. To allow data access, this paper first studies how to fundamentally integrate the database ranking. We established a RankSQL, a DBMS system that supports the principle of ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking ranking
Web storage libraries, such as Stanford WebBase storage libraries, manage a large abnormal collection of Web pages and related indicators. For effective analysis and mining, these storage libraries must provide a statement query interface, supporting complex expressive Web queries. These queries have two key features: (i) they view Web storage libraries at the same time as a collection of text files, as a navigable oriented chart, and as a set of relative chart storage Web page properties (length, URL, title, etc.) (ii) query using the application specific rankings and order relationships pages and links to filter and obtain only the "best" query results in this file, our Web storage template "Web" relations describe a complex web storage.
The Institute of Technology of India, Mumbai is one of the leading universities in India. Located in a suburbs of the vibrant city of Bowie, Mumbai (will return to its original name, Mumbai), which is a landscape campus along the shore of Bowie Lake extending more than 500 acres. The Institute has a faculty intensity of about 400, with about 2,500 students. The Faculty of Computer Science has a faculty intensity of 25, about 150 graduates and 70 graduates. The Database Group of the Faculty of Computer Science and Engineering is the largest database group in India. The group currently has four academic members, D. B. Phatak, N. L. Sarda, S. Seshadri and Sudarshan. The group currently has three graduates, ten graduates, ten graduates and engineers.
The traditional method of resource allocation of the database management system (DBMS) includes the manual determination and allocation of each resource. the database resources are usually used to the top performance of the known and predictable work load. As the database becomes more common on the built-in systems and the internet, it is impossible to predict that the work load will need to be dealt with. From online transaction processing (OLTP) to online analytical processing (OLAP), the DBMS must be able to provide the top performance, not only different types of work load, but multiple work load on a single system.
In the past few years, many active database models have been introduced, some of which have been implemented as research prototypes, the use and research of these prototypes suggests that it is difficult to obtain clear concepts of the proposed method and to compare them. More generally, there are some undoubted difficulties in understanding, thinking and teaching the behavior of active database systems. We believe it is necessary to formally describe the grammar of these systems in order to describe and understand them with less duality, comparing them, and make some progress in defining the standard concepts and functions of active database.
Many social applications, for example, in the fields of healthcare, land use, catastrophic management and environmental monitoring, are increasingly dependent on geographical information for their decisions. With the appearance of the world wide network, this information is usually located in a variety of, distributed, diverse, independent maintenance systems. Therefore, strategic decision making in these social applications depends on the ability to enrich with geographical information related grammar to support a variety of tasks, including data integration, interactivity, knowledge recycling, knowledge acquisition, knowledge management, space rationalization, etc.
In order to ensure high data quality, the database must verify and clean input data from an external source. In many cases, the purification must match the acceptable uranium in the reference table. For example, the product name and description fields in the sales record of the dealer must match the pre-recording name and description fields in the product reference relationship. In this case, an important challenge is to implement effective and accurate mixed matching operations that can effectively clean the input uranium if it cannot accurately match any uranium in the reference relationship.
Over the past few decades, data integration has had a great interest and introduced a lot of work in this field. Here, we discussed a method to solve the problem of integrated unusual data models, that is, relative and XML. Because relative models have been the most commonly used data models for years. Similarly, XML is rapidly becoming more and more popular as a standard format for information exchange. Therefore, building a connection to these two models is a clear need. To this point, we are committed to defining a system to extract data, regardless of their model nature, and make a query enough from different models, which is XML and relative, in our case.
This second long-term planning topic on PLS-SEMin Strategic Management Research and Practice aims to make further progress towards this goal.The magazine received 41 articles on the subject of PLS-SEM, 12 of which successfully completed the process of deep review based on the number of high-quality manuscripts, the decision to separate the subject.
This work shows us how we integrate data mining with a variety of techniques, such as decision-making tree guidelines, prejudice analysis and multi-conceptual levels, forming an intuitive and new method to investigate customer loyalty and forecast the possibility of their errors.
Automatic selection of the appropriate combination of the materialized view and indicators for the SQL database is a non-second task. A judgment choice must be cost-driven and impact the work load experienced by the system. Although in the background of the multi-dimensional database (OLAP) there is a materialized view choice work, no past work has seen the creation of an industry intensity tool for the automatic selection of the materialized view and indicators for the SQL work load problem. In this article, we introduce a terminal solution to the problem of the choice of the materialized view and indicators. We describe a wide range of experimental evaluation results, proofing the effectiveness of our technology.
Relative database provides storage of user defined features and forecasts, which can be mentioned in SQL queries. When evaluating a user defined forecasts is relatively expensive, the traditional methods of evaluating forecasts the fastest possible optimization of the algorithm is no longer a sound effect method. There are two previous methods to optimize such queries. However, it is not guaranteed the best plan over the required implementation space. We provide effective technology that can guarantee the choice of the best plan over the required implementation space.
In this article, the generic data of the multimedia file is classified according to its nature, different types of generic data are linked to different purposes, we describe how according to the ISO standard SGML organizations generic data, which promotes structured file processing, as well as DFR, which supports the storage of the file set, and finally, we explain the impact of our observations on the future development.
Multimedia Database is the control collection of multimedia data projects, such as text, images, graphic objects, video and audio. Multimedia Database Management System (DBMS) provides support for creation, storage, access, query and control of multimedia databases. Multimedia DBMS requirements are: Multimedia data models; Multimedia objects storage; Multimedia indexing, accessing and browsing; and Multimedia query support. This paper discusses the general framework of multimedia databases systems and describes the requirements and architecture of these systems.
When multiple queries are performed simultaneously, this model introduces capacity because of the unknown physical program when conventional queries - competitors access the basic I/O and calculation resources. Therefore, while modern systems can effectively optimize and evaluate a complex data analysis query, their performance is significantly painful when running simultaneously in multiple complex queries. We describe an added traditional query engine, through large-scale queries simultaneously performing the database. Unlike the unknown model when conventional queries, our method adopts a single physical program, which can be shared I/O, calculated and performed in all queries.
Editor's Notes: For many observations of this issue time, I invite Robert Gephart (Robert Gephart) to observe at the University of Alberta in order to well reflect his observations, as a long-term service, award-winning reviewers can increase their opportunities for quality research published in A!vII in the past two and a half years, I have developed an enormous review of the Boo's hot eyes to evaluate quality research submitted, and to anyone interested in the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and
This article examine comment concevoir un tel extranet. Il presente une solution automatisant la definition et la mise en oeuvre du contrôle d'acces ainsi que l'administration a grande echelle des utilisateurs en utilisant un model a base de capacites.
We view wireless data transmission as a way to transmit information to a large number of users, organizing and accessing information on wireless communications channels is different from the problem of organizing and accessing data on the disk, we describe two methods, (1m) indexing and distributing, organizing and accessing data transmission, we prove the recommended algorithm contributes to significantly improving the battery life whileining low access time.
The key problem in carrying out spatial connectivity is to find the cross-right opponent. For unindexed data sets, this is usually solved by dividing data, and then in a single division to make aircraft connection. The result of the connection can be seen as a two-step process, where the division is equal to the hash-based connection, while the aircraft connection data is equal only to the type of connection based. In this article, we are looking for an extension of the whole idea of the connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection connection
As a teacher, if we consider the content knowledge complex of the six successful teaching, we may not understand that understanding the students and finding their strength is equally important. The teacher is the teaching island, with many content shared, but almost no connection to the student composed of the classroom. If as a teacher, we describe the student saying, “It is a mathematical concept that can see these academic concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts and academic concepts of complex concepts
XML is rapidly becoming the actual standard for data exchange on the Internet. This is creating a new data management requirements that involve XML, such as the need for storage and query XML file. The researchers suggest using the relationship database system to meet these requirements, by guessing methods of “distributing” the XML file to the relationship, and converting the XML query to SQL query these relationships. However, a key question, this method is widely ignored in the research literature, is how (and whether) the order of the XML data model can effectively support the inaccurate relationship data model. This paper shows that the XML order data model can actually be effectively supported through a relationship database system. This is by encoding the order as a value of the data.
SHORE (Scalable Heterogeneous Object REpository) is a stable object system that is being developed by the University of Wisconsin. SHORE represents the combination of object-oriented databases and file system technologies. In this article, we provide the goal and motivation for SHORE and describe the features of the two technologies. We also describe some of the new aspects of the SHORE architecture, including interactive peer-to-peer server architecture, server customization by adding value to the server facilities, and the extensibility of supporting multiple processor systems.
In this paper, as a result of the discussion of the OOPSLA 2002 workshop on agent-oriented methodology, we describe the current situation of agent-oriented methodology, how to integrate it into a model-based framework, and what the community needs to do to make its products accessible to the industry.
The study of data flow algorithms has prospered since the late 1990s, and the speech will track the history of the short-frequency calculation paper, how it is conceptualized and how it affects data flow research.
Expanded trading models have recently attracted a lot of interest in the academic community and industry [2]. These models are trying to solve the restrictions of traditional ACID trading to support multi-system applications, operating in an abnormal environment. These applications are increasingly demonstrated as strategically important for some and government agencies. Different trading models, however, tend to be closed, they cannot easily with other such models, thus limit their applicability situation, which is exactly matching one of them. We have not yet proposed another trading model.
This report is a summary of the first international active and real-time database system seminar (ARTDB-95) held at the University of Scotland in June 1995.
Traditionally, recording is written when all the fields of the recording are known in advance, the need to know in advance all the fields will lead to a very obscure program, especially in the paperwork of searching for external data sources, not in the paperwork that is widespread in the database community, Raymi proposed the duration of recording operations in the background of the programming language, without such a requirement.
Therefore, visual analysis is an appropriate and effective tool, so special requirements arise from the abnormality of data (different data types s, different data sources), the quality of data (lost value, error value), as well as a large amount of data. the visualization of marine data is especially imported within their geographical background and their time process. First, this paper introduces a classification of visualized space and space-related data, which is not only applicable to marine data.
In this article, we presented two new divisional algorithms, known as adaptive divisions and divisions (APSJ) and adaptive divisions and divisions (ADCJ), allowing the computing combination to be effective.We show that APSJ exceeds the algorithms proposed before many data sets, often through the size order We introduce the detailed analysis of the algorithms, their performance research and test data.
Research and development of multi-database systems is caused by the need to integrate data from sources of information that are abnormal and physically distributed Multi-database architecture and racial abnormal issues have been studied.This paper introduces multi-database systems that are carried out at the University of Dresden and are currently ongoing.
In this article, we introduce to the definition of a correct database a rule update (UR) program and discuss the fixed implementation of these programs in a DBMS environment. Updated rules implemented by updating relationships in a database description, which may lead to the further implementation of the rules. The correct database must ensure that the implementation rules will end and it will produce a minimum updating database settings. Database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database defined as a database.
A typical load creates orins a data storage process GB, it takes several hours or even days to perform, and involves many complex and user-defined data conversions (e.g., find a copy, solve the data incoherence, and add a unique key). If the load fails, a possible method is to “recurse” the entire load. A better way is to repeat the incomplete load, from which it is interrupted. Unfortunately, the traditional algorithm repeat the load, either during normal operation, or depends on the conversion characteristics.

This paper introduces QuickStore, a permanent C++ storage system built on the top of the EXODUS storage manager.QuickStore allows applications to access objects through normal virtual memory indicators.This paper also introduces a detailed performance study using the OO7 reference label.The study compares the performance of QuickStore with the latest implemented E programming language.These systems simulate two basic methods (hardware and software) that have been used to implement sustainability in the object-oriented database system.
For this exchange is useful, the individual system must agree with the meaning of their exchange data. that is, the organization must ensure the interactivity of the exchange content. the paper provides a exchange unit of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the exchange content of the ex
Tree cover Pattern Queries (TPQ), Branching Path Queries (BPQ), and Core XPath (CXPath) are subclasses of the XML query language XPath, TPQ  BPQ  CX Path  X Path. Let TPQ = TPQ+  BPQ+  CX Path+  X Path+ denote the corresponding subclasses, consisting of queries that do not involve the boolean Path Queries (BPQ), and Core XPath (CXPath) is not in its predicates.
This paper introduces a new method, the database disk bubble, known as the LRU-K method. The basic idea of LRU-K is to track the past K reference to the popular database page time, using this information to statistically estimate the reference indirect time on a page. Although the LRU-K method makes the best statistical conclusions under the relative standard assumption, it is quite simple and causes a few bubble. As we through the simulation experiment showed, the LRU-K algorithm exceeds the difference between the common bubble algorithm in the frequent and frequent reference pages.
In this article, we explore the problem of automatic adjustment of the DBMS multi-programming levels and memory allocation to a set of msponse time targets for a complex multi-class recharge. We begin to describe these phenomena, making this a very challenging problem, the most important of which is interdependence between classes, the result is their competitive shared resources. We then describe M&M, an algorithm based on feedback to determine each class of MPL and memory setting independently, we evaluate the effectiveness of the algorithm using a detailed simulation model. We show that our algorithm can successfully the response time, which is a few percent of the target mixed work recharge, including conversion and longer time conversion.
We describe a structured text access system (text machine) integrated into an object-oriented database system (OpOur method is a significant, using the external functionality of the database system to include the text access system as an external source of information. However, we are able to provide a close integration in the query language and processing; users can access the text access system using a standard database query language.
As a universal similarity model, multi-dimensional, adaptable application requirements and user preferences, we use a quadruple distance feature, which has been successfully applied to color image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image
With regard to the specific requirements for the advanced OODB application, the index data structure in OODBMS must provide effective support for multiple queries and must allow the index to be optimized to a specific query file. We describe the multi-critical type index and an effective implementation of this index scheme. It meets two requirements: in addition to its multi-critical query capacity it is designed for two standard design alternatives, key group and type group. A multi-critical type index is a linear algorithm, the map type algorithm is a linear command character domain so that each subtitle is represented by the interval of this domain.
This is a modified library in the active main archive and adding a complete archive system, but we decided in this area of various research activities. We edited this library for our own purposes, but hopefully it may be useful to others. All the texts appear in the following lists, generally available. We do not think the library is complete and covers the entire range of library, such as students on the activity of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library of the library
The deep training converter network is effective in various ranking tasks, such as queries and ad-hoc document ranking. However, their calculation costs consider them cost-blind in practice. Our recommended method, known as PreTTR (pre-calculator time representative), significantly reduces the query time of the deep converter network (augmented 42 times on the online document ranking), making these networks more practically used for real-time ranking scenes.
In recent years, the impact of high-end fraud has been thoroughly studied several issues, such as classification, closest neighbor search, and indexing. In high-end space, data becomes dispersed, traditional indexing and algorithm technologies fail from the point of view of efficiency and/or efficiency. Recent research results show that in high-end space, closest concept, distance or closest neighbor may even have no quality meaning. In this article, we see the size fraud from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view of distance, from the point of view
This article is about the Information Network Integration Seminar (IIWeb-03) held in Acapulco, Mexico on 9-10 August, as part of the 2003 International Joint Conference of Artificial Intelligence, the full process of the seminar is available online [1]. a small sample of the paper submitted at the seminar also includes in the special topics of IEEE Intelligent Systems [2]. the effective integration of unusual databases and information sources is referred to as the most stressful challenges in various areas such as corporate data management, local security, anti-terrorism and human genome projects.
Although the relative model of the database provides advantages over other data models, it lacks a comprehensive way of dealing with incompleteness and uncertainty of data.The data value uncertainty, however, is invasive in all real world environments and has gained a lot of attention in literature.Some methods have been proposed to incorporate uncertainty data into the relative database.The current method has many disadvantages and does not build an acceptable relative model extension.In this article, we propose a continuous relative model extension.We introduce a re-examination of the relative structure and extended the relative algorithm.
Supporting independent IS and integrating it into the distributed database (materialized view) as the growth of WW becomes more important. However, the view of the independent IS defined easily changes the chart. In the EVE project, we are developing technology to support the maintenance of the database of the distributed dynamic IS defined.
For this purpose, an effective classification strategy, which distributes the signature to a set of parallel independent discs, must be combined with a classification, which is used to avoid searching the entire signature file while performing a query. This article proposes two parallel signature file organizations, Hamming Filter (HF+) and Hamming + Filter, its common classification strategy is based on error correction code, while in classification is through the organization signature to a fixed size of classification, each contains the signature sharing the same key value.
We introduce a controlled, dynamic user-controlled order operator, used for data-intense applications. allowing the user to re-order data delivery on the aircraft increases interactivity in multiple backgrounds, such as online integrated and large-scale distribution panels; it allows the user to control the data processing, by dynamically specified preferences of different data elements, based on prior feedback, so the data interest is priority early processing. In this article, we describe an effective, time-free re-order mechanism, which can be used over voluntary data flow from files, indicators and continuous data transmission. We also studied several policies of re-order, based on the target of various typical applications.
This paper explores the impact of the product development range: a new product is based on the degree of unique components in the domestic development. using a larger study of the product development data in the world automotive industry, this paper introduces the scope of evidence of the impact in the leading time and engineering productivity. the study of the automotive supplier industry shows that very different structures and relationships exist in Japan, the United States and Europe. however, there has been a few studies of the impact of these differences on the development performance.
One feature of the distinction between digital libraries and traditional databases is the new cost model of customer access to intellectual property. Customers will pay the cost of access to data projects in digital libraries, we believe that optimization of these costs will be as important as the performance of traditional databases. In this article, we discuss the cost model and protocol of digital library access, designed to determine the minimum cost protocol of each model. We expect future information equipment will be equipped with cost optimizers, just as today's computers adopt built-in operating systems.
An extended key is to send an query only to the relevant server and to avoid waste of the data source resources that will not provide any results. Therefore, a directory service, which will determine the relevant data source query, is an essential component of effectively processing the query in a distributed environment. This paper proposes a directory framework, distributed in the data source itself, does not require any central infrastructure. Because the new data source becomes available, they automatically become part of the directory service infrastructure, allowing extension to a large number of nodes.
Smartcard is today the safest and cheapest mobile computer device. it has successfully used a variety of applications around the world involving payment and identification, making it the world's highest semiconductor market in 2K. As the smart card becomes multi-applications (by hosting a special Java virtual machine) and increasingly powerful (32-bit processor, more than 1 MB of stable storage), the demand for database management arises.
The majority of previous studies adopted candidate mining methods and test methods similar to Apriori. However, candidate mining methods remain expensive, especially when there are reproductive modes and/or long modes. In this study, we proposed a new specified frequency modes (FP series database) structure, which is an extended predictive database structure, which is compressed in data mining studies, key information about frequency modes, and developed an effective FP-like candidate mining method, the FP growth method to reduce the full combination of new frequency modes. Efficiency of mining methods is the result of the FP database: the result of the FP database is the result of the FP database.
At the end of the 1970s, when the second generation of DBMS products and technologies entered the market, there were significant research activities aimed at the wider use of grammatical information in the database systems. The study focused primarily on grammatical data models and data models, including grammatical query processing and integrity checking. However, few of these efforts resulted in finding their own ways in the database technology or database management practice.
The semi-structured information space consists of multiple collections of text files containing fields or parts of labels. The space can be very unusual, because each collection has its own chart, and there is no compulsory key or format of data elements in the collection. Therefore, structured methods such as SQL cannot be easy to use, users often have to only do a complete text search. In this article, we describe a method to provide a structured query for a specific type of entities such as companies and people. The entity-based query is by normalizing the way of entity reference, the type-based way. The method can be used to query files, and can also be used to build entity files - common information summary, based on entity content, we describe a method to provide a specific type of entity structure query, such as the
Data quality is a serious concern, in any data-driven enterprise, often creates error discoveries, in data mining, and leads to process interference in operating databases. Data quality problems performance can be very expensive "loss" customers, "feil" billions of dollars of value of equipment, error allocation of resources, because of sliding forecast, etc. Data quality problems solving usually takes a very large investment time and energy - often 80% to 90% of data analysis projects spend in making the data reliable enough, the results can be trusted. In this tutorial, we introduce a multidisciplinary method of data quality issues. We begin to discuss the significance of data quality issues and the source of data quality issues. How we show these problems can be through multidisciplinary methods, case measurement, technical data management, data management, data management, data
Everyone knows the small world phenomenon: after meeting a stranger, we are surprised to find that we have a friend, or we are connected through a short chain of acquaintances. in his book, Duncan Watts uses this exciting phenomenon - known as "Six-level separation" - as a more common exploration prophecy: under what conditions, a small world can appear in any type of network? the story of the network is everywhere: the brain is a nerve network; the organization is a human network; the global economy is a network of the national economy, these networks are a market network, these networks are an interactive network of producers and consumers.
EROC (Extensible, Reusable Optimization Components) is a set of existing tools for creating the query optimizer. The EROC components are based on the abstract C++ class, we have defined as the query optimization center, not only in the relative DBMS, but in the expansion of the relative and object-oriented DBMS. I EROC uses the C++ EROC search results, only for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the C++ EROC search results, for the
Serious maps between data sources play a key role in multiple data sharing architectures. maps provide the relationship between data stored by different sources, thus allowing answers to the benefits of data formatting required from other nodes in the data sharing network. composite maps are one of the core issues of multiple optimization methods in the data sharing network, such as queries of frequently passed routes and return analysis. This paper studies the theoretical basis of map combination. We study a rich map language, GLAV, which combines the benefits of known map forms. We first show that even when composing two simple GLAV maps, the entire map composition may be an infinite format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format
Information recycling (IR) engine can be ranked according to the text closer to the keywords in each document. In this article, we apply this concept to search the entire database objects are “approximate” to other related objects. Near search allows simple “focus” queries based on the overall relationship between the objects, useful interactive queries meetings. We see the database as a chart, with the data on the vertical (object) and the relationship specified the edge. Near is based on the shortest path between the objects. We have implemented a prototype search engine, using this model to search the keywords on the database, we find it is very effective to find the relevant information between the objects stored on the chart can be very expensive.
In this report, we describe the Columbia Quick Question Project (Section 2), the JAM Project (Section 3), the CARDGIS Project (Section 4), the Columbia Internet Information Search Project (Section 5), the Columbia Content-based Visual Question Project (Section 6), and the projects related to the Columbia Programming System Laboratory (Section 7).
The basic services provided by various software providers The service manages the continuous storage of apphcatlon data characteristics and the use patterns of data stored by different applications are widely different: one storage service is designed and for one class of applications may not serve another apphcatlon good "fo one first close, two extreme tlus spectrum of storage service requirements are now provided by the relative database and file system.
The majority of the databases contain “name continuous” such as route numbers, personal names, and location names, equivalent to the entities in the real world. The previous work on integrating the unusual databases has been assumed that the local names continuously can be through a standardized map to a suitable global domain. Here, we assume that the name is provided in the natural language text. We then submit a logical database integration called WHIRL, which is a clear reason for similar local names, according to the measurement using the vector-space model generally adopted in statistical information collection based on WHIRL implemented data integration system has been used to successfully integrate information from several sites in two areas.
Data warehouse stores a material view of the basic data from an external source. Customer usually performs complex only reading queries on the view. View is updated regularly from the base board, spreading a large number of updates. In the current warehouse system, maintenance transactions are usually isolated from the customer’s reading activity, limiting the availability and/or size of the warehouse. We describe an algorithm called 2VNL, allowing warehouse maintenance transactions to work simultaneously with the reader. Through logical maintenance of the database’s two versions, no lock is necessary and guarantees feasibility. We introduce our algorithm, explaining its relationship with other multi-variable competitive control algorithms and describe how it is implemented using a regular DBMS response method.
Classification is an important data mining problem. Although classification is a carefully studied problem, most current classification algorithms require the entire set of data to remain permanently in memory. This limits the suitability of their mining on a large database. We introduce a new decision-making base of classification algorithm, called SPRINT, removes all memory limits and is fast and extensible. The algorithm is also designed to be easily parallel, allowing many processors to work together to create a consistent model.
In these applications, a large number of state samples are obtained through the sensors and flow to the databases. In addition, updates are very frequent and may display the location. Although R tree is a multi-dimensional data selection index, low size, so related to these applications, R tree updates are also relatively ineffective. We introduce a bottom-level updating strategy of R tree, generalizing existing updating technologies, aimed at improving updating performance. It has different restructuring levels - from global to local - continuous updating, avoiding expensive top updating. The main computer memory structure allows direct access to multi-dimensional data with low size, therefore, R tree updates are also relatively ineffective.
Information Security Strategy Program must be comprehensive, including business processes, people, and physical infrastructure, as well as information systems. Security risk assessment requires calculating the asset value forecast effects and consequences of security events. Security Investment Revenue (ROSI) is determining the value of all investments in the security aspects, by determining the cost of the asset, which may interfere with the security violations and the cost of its impact. Knowledge is a source of many competitive advantages, the enterprise and it should be protected from the right security control of the robbery, abuse and disasters. All the elements involved in the knowledge creation process are knowledge assets. A IPO model with a combination of Scandia and Balance assessment method requires the development of a measurement system of knowledge value assessment model. This acknowled
The Cornell Jaguar project is exploring a variety of issues related to mobility and query processing, with a wide-ranging topic breaking the traditional client and server boundaries, leading to immaterial query processing, and another topic is expanding databases and query processing technologies to small and mobile devices, which builds and expands the Cornell PREDATOR database engine.
In a networked world, data management has shown us some of the same challenges we’ve seen in the past, but emphasizes our ability to deal with the scale, with multiple commands of the database users, the database size grows faster than the law of Moore.
Time databases take a single time evolution line. In other words, they support time evolution data. However, there are applications that require time data support and branch time evolution. As time flow creates new branches, branches and time data tend to rapidly increase the size, making the e science index demand critical. We propose a new (branche) method of access to branches and time data: BT Tree.
Enterprise-level databases need to be responsible for performance adjustment of the database administrator. With the large-scale deployment, the minimization of the database management function becomes important. One important task of the database administrator is to choose the indicators suitable for the system's workload. In data enhanced applications, such as decision support and data storage, the right indicator group becomes key performance. In addition, the selected indicator should track the changes in the workload.
When we, humans, talk to each other, we have no trouble dividing what the other person means, although our statements are almost never carefully described to the final details. We "filled the flaws" using our common knowledge of the world. We introduced a powerful mechanism that allows the object-oriented database system users to determine certain types of advertising queries in a way closer to the way we ask each other the question. In particular cases, the system is accepted as the input queries and incomplete, therefore double, path expression. From them, it produces queries with a fully defined path expression, which is consistent with these as the input and catch user's most likely meaning. This is the path of the map queries to get closer to each other.
Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database
Publisher Review Traditional databases allow storing and obtaining a lot of data, but not making any promises of data uncertainty. In many fields, it is difficult, if not impossible, to mention all information's 100% certainty. Scientific research, for example, has suffered a lot of uncertainty and errors, cannot be modeled by traditional databases. errors of the experimental machine, pollution samples, and simple human errors are many possible sources of this uncertainty.
Over the past decade, the speed of the cargo CPU has made a great progress in memory delay. main memory access is therefore increasing in many computer applications performance string, including the database system. In this article, we use a simple scan test to show this string a serious impact. obtaining insight is translated into the database architecture guidelines; from the data structure and the algorithm perspective. We discuss how vertically divided data structure optimizes storage performance on serial data access. Then we focus on equi-join, which is usually randomly accessed operation and introduced the algorithm of the string division.
The complex queries are becoming a common place with the decision-making support system growing. These complex queries often have a lot of common sub-expressions, either in one queries, or in multiple such queries operate as a combination. Multi queries optimization is designed to use the common sub-expressions to reduce the cost of assessment. Multi queries optimization has always been considered false because the previous algorithms are complete and explore a double exposure search space. In this paper, we prove that Multi queries optimization using linguistics is practical and provides significant benefits. We proposed three cost-based linguistic algorithms: Volcano - SH and Volcano - RU, these algorithms are based on simple modifications of the cost of search, a big queries optimization because the previous algorithm is complete.

This article introduces a distributed file organization record structured, the disk residential file is based on key accuracy matching access. the file is organized to distributed, distributed on multiple servers, where one server can keep multiple distributed. the customer's request is through the map key to distributed, and in a address table to view the corresponding server. the dynamic growth of file size and access load is supported by distributed distribution and transferred to other existing or new purchased servers. the key and challenging question here is how to distributed so that the file size and the customer through distributed can linearly increase the number of servers and the dynamic distributed data.
In today’s information-driven world, a major problem is that sharing unusual, unusual data is incredible.The Square is an interactive data management system that allows us to share unusual data in a distributed and extensible way.The Square assumes that participants are interested in data sharing and are willing to define two-way maps between their charts.Then, users submit queries on their favorite charts and a query response system repeats to extend any maps about queries, obtaining data from other opponents.In this article, we provide a brief overview of the Square project, including our work in developing map language and query reform algorithms, helping users define maps, index and share data.
Microsoft Universal Data Access defines a platform that develops multi-level enterprise applications that require effective access to various relative or non-relative data sources inside or on the Internet.Universal Data Access consists of software components, using system-level interfaces defined by OLE DB, and provides an application-level data access model called ActiveX Data Objects (ADO).
One way to overcome these problems is to go to fewer scenes. In this context, we introduce a large-scale reality world data set designed to evaluate human action recognition technology, beyond the manual-made data set. For this purpose, we put the data collection process on the foot and start a test group of 250 cooking videos.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
LH* RS is a new Availability Extended Distributed Data Structure (SDDS).LH* RS’s data storage system and search performance are essentially LH*’s data storage system.LH* RS also manages equal information to tolerate the inaccessibility of k g 1 server sites.
Garlic is a intermediate software system that provides a comprehensive view of various hereditary data sources without changing the way or location of data. In this article, we describe our packaging architecture, the key components of Garlic, these data sources are packed and mediated between them and the intermediate software.Garlic packaging model hereditary data as an object, participating in query planning and providing a standard interface for the method of submission and query implementation.
In this article, we introduce XML-GL, XML document graphic query language, using visual formalism to represent XML document content (and their DTD) as well as query synthesis and grammar, making query intuitive expression even if they are quite complex.
In system surveillance, people are often interested in checking the properties of integrated data. Current policy surveillance methods are limited to the integrated type they deal with. To correct this, we extend an expression language, measuring the provisional logic of the first command, with the integrated operator. Our expansion is inspired by the integrated operator in the database query language, such as SQL. We provide a monitoring algorithm for this rich policy specification language. We show that our language is better suitable for policy expression compared to the relevant data processing methods, our monitoring algorithm has competitive performance.
This paper covers the TPC-D parameters we learn in the NCR as we implement and publish our first concentration of the Teradata database. Customers should carefully read the full disclosure area, as well as the weaknesses of the parameters related to the actual customer application. It discusses the key implementation and optimization elements of the Teradata database and the 5100WorldMark platform to help us publish the results.
Therefore, we can then use a highly sophisticated space access method (SAM) to answer multiple types of queries, including "queries through examples" type of queries to queries to a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query, it shows a regional query.
Many business organizations often collect a large number of databases for various marketing and business analytics functions. The task is by identifying the information of different databases to identify the different individuals that appear in different databases, usually in an incompatible and frequently wrong way. The problem we study here is to combine the data from multiple sources as effectively as possible, while maximizing the accuracy of the results. We call it a combination/cleaning problem. In this article, we describe in detail by some people used to solve the combination/cleaning classification neighboring methods, and existing experimental results, to prove that this method can work in practice, but at a huge cost. Based on classification alternative methods also present a comparative assessment to the classification neighboring method.
Magical rewrite is a complex decision supporting query known optimization paper. Even a query can choose this rewrite many variables, which has a great difference in performance performance. We proposed cost-based technology to choose an effective variable from many choices. Our first contribution is a practical program, model magical rewrite as a special combination method, which can be added to any cost-based query optimizer. We derived from the cost-based formula, allowing an optimizer to choose the best rewrite variables and decide whether it is useful.
Abstract.This paper contains a overview of the technology used in the query processing and optimization component of Oracle Rdb, a relative database management system originally developed by Digital Equipment Corporation and now under development by Oracle Corporation.
Data extraction from HTML pages is carried out by software modules, usually referred to as plugins. By the way, plugins identify and extract the related text parts within a web page and reorganize them into a more structural format. In literature, there are several systems (half-) automatic generating plugins for HTML pages. We have recently studied the original method aimed at further pushing the automated process of generating plugins. Our main intuition is that in a data-intensive web page, the page can be divided into a small number of categories, therefore, belonging to the same category of pages to share a fairly close structure.
In this article, we introduce C2P, a new classification algorithm, using space access methods to determine the closest pair. Multiple extensions are introduced as extensible classification in the big database, containing different shapes and appearances of classification. Due to its characteristics, the recommended algorithm achieves the benefits of classification and classification algorithm, providing the efficiency and quality of the classification results.
Since object technology is through the analysis and design of software systems, languages, GUI and frameworks, the database community also strives to support objects and develop these support standards. One key advantage of object technology is the ability to interact with different objects and objects tools, so it’s important that such DBMS objects standards interact with other objects world. From discussing new objects bringing query standards, we introduce a variety of relevant groups’ efforts, including ODMG, OMG, ANSI X3H2 (SQL3) and recent combination efforts to feed to SQL3.
This method, although it will be effective space (someone storing the combination we need) will waste a lot of time (create all possible combinations). this paper introduces some algorithms starting from a seed combination (a has already met Brian’s forecast we want to evaluate) and grow them to the largest combination.
For some reasons, even the best query optimizer can often produce low-optimized query execution plan, which leads to a significant decline in performance. This is especially in the database used for complex decision support query and/or object relationship databases, we describe an algorithm that detects low optimization of query execution plan during query execution and tries to correct the problem. The basic idea is to collect critical points of statistics during complex query execution. These statistics are then used to optimize query execution, either by improving query resource allocation, or by changing the query residual database execution plan. To ensure that this algorithm does not significantly slowly perform the normal query results, query can be carried out during query execution and query data can be carried out during query execution and query
In this article, it provides a new color space for content-based images, based on human perception of psychophysics, which provides both the ability to measure similarities and determine similarities, using confusing logic and psycho-based theoretical similarity measurements, which are proven equal or superior to traditional color space.
This paper describes our experimental access logs using from a commercial yellow page service called "iTOWNPAGE."Our initial statistical analysis shows that many users search different categories - even at the level provided - together, or complete their search without any results matching their queries.To solve these problems, we first gather user requests from access logs using enhanced K classification algorithms, and then apply their queries extension.Our method includes two steps of expansion 1) recommending similar categories of requests, 2) although the relevant categories are provided at the level - completing their search results or not matching any of their queries, we solve these problems.
We describe the basic characteristics of the Lixto tractor generator by describing a simple example of the current Lixto prototype, such as user interaction with the system, capable visual interface, labelling and selecting programs, and extracting tasks.
Classification is one of the most basic algorithms in computer science, and the common operation in the database is not only the result of the classification query, but as part of the portfolio (i.e. the classification portfolio portfolio) or index. In this work, we introduced a completely new type of classification, using a model learned experimenting data of CDF. Our algorithms use a model to effectively get a near-scale experimenting CDF of each record key and put its map to the corresponding location of the output sequence. We then apply a defined classification algorithm that works well (i.e. the classification portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio port
It describes a series of translation rules from existing extended entity relationship (EER) models to object model technology (OMT) models, from object models to general object-oriented (OO) models. This re-engineering practice can not only provide us with a significant insight into the "interactivity" between OO and traditional seed model technologies, but it can also guide us to develop objects-oriented database (OODB) practical design methods.
E-commerce is not a static field, but continuously developing to find new and more effective ways to support the business. Data management is a component of this effort, this special issue aims to report some recent developments and determine some research direction in this field. Initially, e-commerce involves the use of ED] and intranets. Today we see the domination of XML. Almost all recent e-commerce standards are based on X1VD... Therefore, the amount of stored XML data is huge, it is increasing. This naturally leads to how to store and question XML files. The paper is described by Tian, DeWitt, Chen and Zhang to design and evaluate the performance of alternative XM, storage strategy. The result of the study is how the value is based on XM almost all recent e-commerce standards are based on X...
We introduce a new algorithm with the use of materialist views calculating answers to SQL queries with combinations and collections, in the existence of multi-level definitions, in addition to its obvious potential in queries optimization, this problem is available in many applications, such as data storage, very large trading recording systems, global information systems and mobile computing, where access to local or hidden materialist views can be cheaper than access to local or hidden materialist views.
In this paper, we introduce a new and fast index system time sequence, when the distance function is any LP standard (p = 1; 2; : 1). One characteristic of the recommended method is that only one index structure is required for all LP standards, including the popular Euclidean distance (L2 standard). Our system achieved a significant speed beyond the art state: the best model of experimental and synthetic time. In this paper, we introduce a new and fast index system time sequence, when the distance function is any LP standard (p = 1; 2; : 1).
In recent years, genetic new developments have generated a great deal of interest, genetic and protein data, investing in international significance (and competition) in the fluid disciplines of bioinformatics researchers in pharmaceutical and biotech companies found that database products can bring a wide range of related technologies to take over their problems. Benefiting from some new technological improvements, Oracle has emerged a popular platform for drug knowledge management and bioinformatics. We see four powerful technologies that show commitment to solving so far unbelievable issues in bioinformatics: extensive architecture, to store genetic sequence data, and to carry out high-level structural searches in the database; storage technology and genetic mineral data; data integrated technology to integrate genetic knowledge management and bioinformatics We see four powerful technologies to solve the current problem of bioinformatics
This media is “proliferating” sensitive, erroneous and low-quality information. From April 10 to May 10, 1996, a web survey of 1,700 users[1] pointed out that 30% of users “find known information” was their problem, and 2,780% of users the organization gathered information as their problem.
We introduce an expandable disk transmission data structure, called LH*.LH* is generalized to linear transmission (LH) for the RAM and disk size file.LH* file can be created from the record, the main key record, or in the worst case, the OID record, provided by any number of distribution and automatic client file.It does not require a central file, and a single file can be through a code of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of the file of
Multimedia applications require specific support for the database management system because of the characteristics of the multimedia data and their interactive use. This includes integrated support for high capacity and time dependency (continuous) data types, such as audio and video. A key issue is to provide continuous data flow processing, including bubble management, such as the necessary multimedia presentation. The bubble management strategy of continuous data must take into account specific requirements, such as providing the continuity of the presentation, immediately continuing the presentation of frequent user interaction through the appropriate bubble resources consumption. The existing bubble management strategy is not sufficient to support the processing of continuous data flow in highly interactive multimedia presentation. In this article, we introduce the "most relevant / most relevant presentation" (LJMRP) bubble management strategy to optimize the presentation of information to optimize the presentation, optimize the
XML is rising rapidly as a representative of data in the world wide network.Sofist search engine, which allows users to effectively type data stored in XML documents will be critical to use the full power of XML documents.Al recently there have been many activities to introduce new semi-structure data models and query languages for this purpose, this paper explores the use of the traditional relationship database engine to process XML documents according to the document type descriptor (DTDs) more conservative methods.For this purpose, we have developed an algorithm and implemented a prototype system, converting XML documents into relative graphics, translating semi-structure query documents to SQL query and converting the results to XML.
In the data continuously added databases, users may want to send a permanent query and be notified when the data match query. If these continuous queries only check a single record, it can be achieved by checking each record. This is very effective because only the input record needs to be scanned. This simple method does not apply to the query involving access or time. The graph system allows the user to send such queries through the mail and paperboard mail.
Many of these scientific data sets are commonly evaluated and stored in file formats, such as HDF5 and NetCDF. Despite the scientific data management systems, such as SciDB, designed to manipulate the time flow of data analysis workflows, it is challenging to integrate these systems into the existing analytical workflows. The main obstacles include expensive tasks, preparing and charging the data system, in question before the data system, and converting the final results into a format, using the mass data evaluation system, using the existing post-treatment and visual analysis tools. Therefore, integrating the data management system into the existing scientific data analysis workflows, requires a large number of users to participate on this paper, we are designing a new data analysis system to process the data system parallel to process the data system.
In this article, we consider a variety of space relationships, these relationships have a common interest in the image database system. We introduce a series of rules that allow us to extract new relationships from a specific relationship group. using these rules the extract mechanism can be used in the query processing system to obtain images. the specific set of rules has been proven to be correct, that is, the extract is logically correct. the rules have also been proven to be a complete three-dimensional system, that is, by a specific consistent relationship group involving each relationship F can be extracted from F, using specific rules.
In this article, we review the question of locations depending on data in a large-scale distributed mobile environment. hopefully, we draw the vision of the future data space - the physical space enhances the built-in digital information. Finally, we describe some applications being enabled by data space because of the large-scale advertising shock sensors network, short-range wireless communication and the availability of detailed location information.
In this article, we introduce a system design called content-based Hypermedia (CBH), which allows users to use metal data through media objects to accumulate intelligent browsing.We describe the methods we use model data to make it browsing, explore our method browsing, we call it metal data browsing, instructs how to use metal data in similarity concepts, introduce our system architecture, and discuss similarity browsing technology using content-based metal data and classification methods, producing higher levels of metal data to help users browsing more effectively.
We proposed a false data indexing method based on reversal files to significantly accelerate access by early stop publishing lists, which is possible because the entry in the publishing list is organized in a way to ensure that there is no more than one specific point matching item in the list. Therefore, we can significantly reduce the number of false positive, leading to the gain of performance. We have implemented our method and experimentally evaluated it, including the testing of false and real world data, and compared it to the superior methods of other methods that have previously been proven.
The paper reports the experience of management, technical methods, and the lessons learned from re-engineering eight departments of large-scale information systems.The strategic goal of each project is to transfer these systems to a collective enterprise range of systems, which contains current and future requirements, significantly reducing operating and maintenance costs, and promoting common understanding between stakeholders (i.e., policy makers, senior management, IS developers / maintenance / terminal users).A logical data model, which includes requirements, rules, physical data representatives and logical data objects, clearly recording these systems’ reference data requirements and is essential for achieving this strategic goal.
As a teacher, if we believe that content knowledge is the most important for successful teaching, we may not understand that understanding the students and finding their strength as a student is equally important. The teacher is an island of teaching, there is a lot of content shared, but in fact there is no connection to the student composing the classroom. If as a teacher, we describe the student saying, "She's a mathematical concept wizard," "She's a science thought axis," or "She's a different teacher's movement, to different historical facts," we can well communicate about the students and discover their strength, as a student is the smallest language. These images help us make a comparison with a multi-level meaning, however, thinking is an aspect of everyday life.
Early research analyzes the performance benefits of reliable memory; we focus on how different designs affect reliability. we propose three designs to integrate reliable memory into the database: non-reliable database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database database
The paper developed an integrated method of vertical data migration between the third-party and the second-party storage, it balances speculative pre-loading to cover high delays in third-party storage, with alternative policy of the second-party storage. In addition, it takes into account the interaction of these policies with the third-party storage schedules and control time pre-loading aggressivity, by taking the controversial third-party storage driven consideration. The integrated migration policy is based on a durable duration of the Markov Chain (CTMC) model, predicting the expected number of access to the file in a specific time.
Most previous solutions of graph matching problems depend on a way to determine the "like" lists in the graph matching, or by identifying the common domain names in the data stored in the graph data. Although each of these methods are valuable in many cases, they are irreversible, and there are examples of graph matching problems, they are not even applicable. This problem examples often occur when the lists and data in the graph are "blind" or very difficult to explain. In this article, we presented a two-step technique, even in the presence of the blind graph names and data values work.
Financial mathematicians believe they can predict the future by looking from the past a series of transactions and quotes (known as points). this assumption's main proof is that the price is only a small amount of volatility on a day and less or less subject to random walking mathematics. this assumption allows traders to make price options and speculating stocks. This assumption introduces a query language and a parallel database (50 routes parallel) that supports traders who want to analyze each point rather than just the date point, using temporary statistical queries, such as time delayed interrelations and point trends. This is the first attempt we know to store and analyze hundreds of gigabyte of time series and data queries, using a time sequence of data queries.
In the past few years, the unusual growth of the computer network has created an incredible range of products and services on the network. this vast amount of information makes a single person unable to analyze all existing products on the network and decide which of them is more suitable for her needs.
Documents stored in a database system may have a language described by a complex internal structure, such as SGML. How to use this structure raises the challenge of the database system executives. We provide the type of query that needs to be supported by the SGML database system. Then we describe several data models that have been proposed to represent the file in the database system and these models provide the degree of support for the SGML.

In India, the planning and development process is a transition from concentration to more distributed methods to provide adequate awareness of the micro-level needs and decision-making potential. The research team on the information gap, was established by the Planning Committee, the Government of India, in 1989, suggested the creation of a database (i) Planning Information, (ii) Planning Monitoring, and (iii) Planning Assessment, in the region. The committee also suggested the development of a database that meets (i) Social Economy, (ii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture, (iii) Agriculture
In a multi-database system, a series conflict between two objects is usually only of interest, when an object has a series similarity, we use a series approach concept, which is essentially an abstract/map between the two objects related to the comparison background. a clear although part of the background representation is proposed, the characteristic relationship between the background is defined. the background is defined as a meeting semi-latine and related operation, as the largest bottom line is defined. the comparison background and type of abstract, used for the relationship of two objects, constitutes a series approach basis.
The network can expand locations, mechanical architecture and software applications, providing unlimited strength, smooth cooperation, and broader access to information.With the changes in the market demand and the development of technology itself, network computing begins at the boundaries of individual organizations, these partner networks create a collaborative environment, sharing computing resources, data and applications.OPGSA is a network-based implementation, based on the business network system, it uses the Open Standard of Globus Toolkits 4 and DRAMA.Through this architecture, it allows enterprises involved in the supply chain to integrate greater management capabilities and gain greater control of complex business processes.
Video is made up of audio visual information. providing content-based video data access is essential for the successful integration of video to the computer. organizing content-based video access requires the use of video generation data. this paper explores the nature of video generation data. video database data model is based on the study of video applications, the nature of video collection requests, and the characteristics of video.
The size and diversity of computer networks and network components, making network management one of the most challenging issues that network administrators face, unable to perform network management functions without the support of automatic tools and applications, in this chapter, including network management requirements, features, technologies, security, some well-known network management protocols and tools will be discussed.
Personalization, which will be the key success factor for the future based on XML web standards, such as eBXML or MPEG-7, requires a strong preference framework. The University of Augsburg’s research program “is a preference world” deals with preferences as a first-level citizen of personalized electronic services. Based on a rich preference theory, simulated as a part of the order, we are developing technology that allows for personalized client and intermediate software components.
SchemaSQL is a recent proposed extension to the interoperability of multiple databases. However, some recently confirmed ed applications for SchemaSQL depend mainly on its ability to process data and graph labels in a uniform way and calls for scientific implementation on a single RDBMS. We developed a logical algorithm for SchemaSQL by combining the classic relationship algorithm with four re-organized operators. We can trade using four graph-based data algorithms. We can trade using four graph-based data algorithms. We can trade using four graph-based data algorithms. We can use four graph-based data algorithms. We can trade with four graph-based data algorithms.
A variety of models and languages to describe and manipulate the basic structure of data have been submitted. algorithms, based on calculations, and logical programming-oriented languages have been considered. This article introduces a general model of complex values (i.e., values with the basic structure), and the language for it is based on three paradigms. algorithm language generalization that is submitted in literature; it has been proven with the current functionality, calculation style of programming has no restrictions to support Bacus (1978). the concept of domain independence (from the relative database) is defined, and the synthetic limitation (reference to security conditions) of the calculation queries is guaranteed domain independence the main results: domain independence, security computing, algorithm language, algorithm language, algor
We consider evaluating a large number of XPath filters, each with many predictions, in the top-class XML file problem. The solution we propose is to easily build a single determining motor, called the XPush machine from the XPath filter provided. We describe some optimization techniques to make the XPush machine more efficient, both in space and time perspective. These optimized combinations lead to high, continuous filters. For example, if the atomic predictions in the filter reach 200000, then the filter is at least 0.5 MB/sec: it increases to 4.5 MB/sec, when each filter contains a single predictions.
Workflow Management Systems (WfMSs) have been used to support various types of business processes for more than a decade now. In the workflow or Web processes of e-commerce and Web Service Applications, suppliers and customers defined compulsory agreements or contracts between the two sides, specify the quality of service (QoS) projects, such as the product or service provided, duration, product quality, and service costs. Management QoS measurements directly affect the success of the organization involved in e-commerce. Therefore, when the service or product is created or managed using workflow or Web processes, the basic workflow engine must accept specifications and be able to estimate, monitor and control the QoS provided to the customer.
Int#ra-operator (or dividing) parallelism is a mechanism established for achieving high performance of the parallel database systems. However, how to use the internal operator parallelism in a multi-demand environment is not very good, the product. This paper introduces the detailed performance assessment of the internal operator parallelism in the parallel database system. A dynamic program based on the concept of matching the double traffic between the operators, showing good performance on various work loads and configurations.
The new generation of SQL standard, known as SQL3, has been developed by ANSI X3H2 and ISO/LEC JTC 1/SC2 1 over the past few years.SQL3 is a module capacity that is compatible with the extension to SQL-92 contammg over its pioneer, these new features in separate, independent capacity, are expected to begin to appear as the new SQL standard this year.
In this article, we first focus on how much space the current association rule mining algorithm has to improve the performance. Our strategy is to compare the performance of an "Oracle algorithm" that is known in advance to the identity of all common project groups in the database and only needs to collect actual support to complete the mining process. Our experimental results show that the current mining algorithm works differently from all the database characteristics and supports the Oracle boundaries. In many cases, there is a significant gap between the performance of Oracle and the current mining algorithm.
Important challenges for web technology, such as proxy caching, web portals and application servers are to keep hidden data updated. Customers may have different preferences so that their data can be delayed and reviewed. Some preferences up-to-date data, others will receive hidden data available quickly. Existing methods to maintain hidden continuity without taking into account this diversity, may increase the request delayed, excessive bandwidth consumption, or both. In addition, this exceeding may be unnecessary, in many cases, the customer will tolerate the fast delivery of hidden data. The file introduces the delayed response data, a set of parameters, allowing the customer to express their different applications preferences A hidden data port or the number of references to determine whether a hidden object is provided.
Introduction and main contribution Providing the mechanism that allows users to obtain the necessary multimedia information through their voice content is now an important issue in a multimedia database. However, the current prototype (e.g., Oracle 8i interMedia and Informix Datablade modules) index is mainly just the low-level characteristics of multimedia objects. Therefore, special technology is necessary for voice indexing and accessing multimedia objects. In this context, we introduce the SMOOTH system, a distributed multimedia database system prototype. It implements an integrated query, recording and navigation framework depending on a general video index model. The framework allows the video to be structured into logical and physical units and through these types of unified objects.
We describe a new method to classify and apply it to analyze and mining categories of data "category data" we refer to the fields of tables that cannot be commanded naturally - for example, the name of the car manufacturer, or the product name provided by the manufacturer.
Due to the appearance of system biology, it is necessary to develop a variety of platforms and technologies to analyze and organize the significance of biological data so that it is mined and carefully processed. Due to the complexity of biological data, it must take into account different standards, and it is mandatory to study all available databases, then it must be placed in a format, easy to evaluate and produce information interest. There are various techniques and methods of mining biological data. Here, we will introduce all possible techniques and operations involving data mining and will compare them in order to find the advantages and disadvantages of various methods.
Database researchers have made significant advances in several research issues related to multi-dimensional data analysis, including the development of fast-cover algorithms, the creation and maintenance of effective solutions for pre-calculation groups, and the design of effective storage structures for multi-dimensional data. However, so far, there is little or no work to optimize multi-dimensional queries. Recently, Microsoft presented "OLE DB for OLAP" as a standard multi-dimensional interface for the database. OLE DB for OLAP defines the development of multi-dimensional expressions (MDX), with interesting and challenging features, allowing customers to ask multiple relevant size queries in a single MDX expression. In this article, we present three algorithms to optimize multi-dimensional queries related size queries for multi-dimensional queries,
Abstract: Worldwide technological advances and with electronic data exchange and standard intermediate software, such as CORBA, have triggered a new computing model, based on a smoothly combined service-oriented architecture called Web Services. Direct machine to machine interaction, so far considered impossible now is possible because of rapid technological advances in XML and SOAP technology. However, there is an unprecedented hip around this new paradigm.
The design of the software architecture is considered abstract to the software field, describing the architecture is considered a simulation process. The general view of the simulation process is presented and described in the background of application field simulation and software field simulation. The consequences of this view are studied to catch the objectives and specific forms described by the building.
The database design usually assumes, clearly or suggests that these examples must belong to a category, which can be called the data model assumption, representing the knowledge of the field of application. As an alternative, we propose a level of model method in which information examples are separated from any specific category, and interactivity is, to a large extent, the consequences of the internal category. In addition, we believe that the internal category model forms a model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model
XQuery is de facto standard XML query language, and it is important to have effective query assessment technology available to it. In XQuery assessment, a core operation is to find the relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative relative
Unfortunately, the site is mainly designed for the use of human browsing rather than for computer programs. Machine extract its content is generally a fairly admired work, if not impossible [4]. Software system using this network information source software system usually uses manual code to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system to extract a system
The phrase match is a common IR technology to search for text and identify the related files in the document collection. The phrase match poses new challenges in XML as the text can interact with voluntary labels, overthrow the search technology, requiring strict dialogue or close approach to keywords. We introduce a technique to match the phrase in XML, allowing dynamic specifications, whether the phrase match and the labels are ignored. We develop an effective algorithm, our technology, using the phrase word and the XML labels reverse index. We describe the experimental results, comparing our algorithm with an indexed circular algorithm, which shows the efficiency of our algorithm.
Although the Oracle8i extension box successfully indexed data from multiple domains (including text, images, space objects, chemicals, molecular structures and genome series), the development of an indexing program is considered a difficult task, launched only by experts, which is also, to build complex domains support. This sample is aimed at showing: 1) building and integrating an indexing program with the Oracle8i extension box is quite simple, 2) the availability of the frame is not limited to complex domains.
As XML appears to be the most popular candidate language for data exchange on the Internet, the integration of the distribution, abnormal and independent XML data sources is becoming a key issue. In this article, we introduce a new and original query rewrite algorithm to answer query to the different sources of XML, under the presence of the XML key. The algorithm combines the features of MiniCon (Mini-Con description) and Styx algorithm (premium and suffix query) to rewrite more algorithm.
In order to overcome the current situation of business to business (B2B) e-commerce, we need intelligent solutions to mechanize the processes of data structuring, standardization, adjustment and personalization, this article examines the overall content management process and discusses the requirements of extended support.
LOPiX is XPathLog [May01b], XML/XPath originally, a rule-based programming language, used to manipulate and integrate XML documents. The main synthetic structure is the XPath expression, extended to variables. Due to the close relationship with XPath, the sequence of rules is easy to understand. Unlike other methods, XPath synthesis and sequence are also used to describe how the database should be updated: When using the rule head, the XPath filter is interpreted as the specifics and properties of the elements, it should be added to the database.
TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator TPlo Indicator
The relative database system traditionally optimizes the I/O performance and continuously organizes the record on the disk page, using the Nary storage model (NSM) (e.g., lock page). However, recent studies show that the use and performance of the storage library is increasingly important on the modern platform. In this paper, we first show that the data configuration on the page is the key to high storage performance, while on the modern platform shows the use of low storage library. Next, we presented a new data organization model called PAX (Partition Attributes Across), which significantly improves the performance of the storage library by integrating all the values of each property together. Because PAX only affects the internal configuration of the page, it does not affect the storage loss and does not affect the I/O behavior, according to the NSM
Publisher Review This chapter introduces the first XPath query assessment algorithm that runs in a multi-time data and query size.XPath has been proposed by W3C to select the nodes from the XML document tree.XPath is important because of its potential application to the XML query language, it is the core of many other XML-related technologies, such as XSLT, XPointer and XQuery, and these technologies have gained great and valuable interest.
This article describes the XSB system, which is used as a memory-driven database engine. XSB begins with a Prolog base, the traditional Prolog system is known to have serious defects when used as a database system. therefore, XSB has a basic lower-layer extension, introduced through the table (or memory)[4], making it suitable for a basic query engine of the drive database system. As it eliminates return calculation, the table extension enables XSB to calculate all the modular series of the database program in the end and diversify data complexity. For non-serie programs, a meta interpretator with the same properties.
InfoSleuth in the MCC project aims to use and synthesize new technologies to a unified system, collecting and processing information in a constantly changing information source network. InfoSleuth has its roots in the Carnot project in the MCC, specialized in integrating an unusual database. However, recent emerging technologies such as the Internet work and the World Wide Web have significantly expanded the type, availability, and quantity of data available for information management systems. In addition, in these new environments, there is no formal control of the new information sources registration, and applications tend to develop, no complete knowledge of the resources will be available when they run.
When a XML document meets a specific type (such as a DTD or XML chart type), it is called a valid document.
We discussed the main methods and technical issues arising in the development of enterprise integrated databases in the past few years, and then discussed these two efforts in the main databases management of Telecom Italia databases applications, although driven by different needs and requirements, can be seen as the continuous development of the enterprise data integrated perspective.We reviewed the experience accumulated in the integrated more than 50 internal databases, emphasizing the advantages and disadvantages of this scene of the databases, and discussed the development of large specialized databases to support customer data analysis and telephone traffic.
Recently, the researchers proposed a new type of B+-Trees optimized for CPU storage performance in the main memory environment, where the tree line size is one or more storage lines. Unfortunately, mainly due to this big difference in the best nod size, the existing storage optimization B+-Trees suffer from poor storage performance, while storage optimization B+-Trees show bad storage performance in no storage optimization. In this article, we proposed divided B+-Trees optimized for CPU storage performance (fpB+-Trees), which includes "save optimized" trees in the "save optimized" trees to optimize and I/O design optimized according to the IBM forecast storage, optimization B+-Trees for two storage predictions factors.
We introduced a new technology to efficiently gather waste in a large permanent item store, which is divided into independent gathered divisions, using information about the divisions reference, that information is stored on the disk so that after an accident can be restored, we use new technologies to organize and update this information while avoiding the disk access, we also introduced a new global labelling system to gather the circular waste between the divisions. the global labelling is based on the divisions gathered; the result is an effective system that retains the essence of the divisions gathered, but still can gather all the waste.
In this article, we introduce a method of integrity maintenance, which involves automatically producing the integrity of the rules of production.The restrictions are expressed as the specific formula of the domain relationship calculation; they are automatically translated into a set of repair actions, encoded into an active database system of the rules of production.The rules of production can be converted (they perform the same restrictions in different ways) and conflicts (because repair of one restriction may lead to another restriction violation).
With approximately 8,000 researchers and 40,000 students, RWTH Ahnen is the largest technological university in Europe. Science and Engineering department and its industrial partners offer many challenges for database research. The headquarters of Informatik V (Information Systems) focuses on theoretical analysis, prototype development and practical evaluation of meta information systems. Meta information systems, also known as storage, documentation and coordinate distribution of processes of production, integration, operation and development of database intensive applications. Our research approaches these issues from a technological and applied perspective. On the one hand, we pursue theoretical and systemic aspects of the integrated database and object-oriented technology. On the one hand, the result of this work is the object-oriented management, called the object-oriented concept, which has been developed in the past few years, the
In this paper, we showed some small, independent primates how to produce a rich combination of effective implementation strategies - covering the standard recommendations of primates proposed in previous literature. These small primates are divided into two main, formal fields: the corresponding removal, and the effective handling of the external and current TP system use efficiency. A method of optimization based on these parts, provides a synthetic independent query processing, with the following query, that is, the same query writing with or without the following query producing the same effective plan. We describe the technology implemented in Microsoft SQL Server (7.0 and 8.0) query and query, even based on SQL query and query query query and query query query,
Data storage systems will integrate information from operating data sources to a central storage library to analyze and mining integrated information. In the integration process, source data usually experiences a series of transformations that can be from simple algorithm operations or integrated to complex "data clean" programs. In the storage environment, data linear problems are from storage data components to the original source components, from which they are derived. We formally define linear tracking problems in the existence of total data storage transformations, we introduce linear tracking algorithms in this environment. Our tracking programs use the known structural characteristics or transformations of the current state, but also work in the lack of such information. Our data linear problems can be used as a linear tracking tool from where they are derived from the formal linear tracking problems.
The eCommerce system on the Internet is business to business transactions still in their childhood. Implementing the Internet eCommerce market for business to business follows a n supplier: m customer scene is still unable to business to business solutions. The comprehensive Internet eCommerce system should provide easy access and processing systems that help overcome time differences, business location, language between suppliers and data between customers, while should support the entire transaction process of business to business transactions. In this article, we introduce a DBMS-based eCommerce architecture and its prototype implementation of business to business transactions according to a n supplier: m customer’s distribution scene. The single system of business transactions can be through the modular data and application data system, can be through the modular data and data system, and the data can be applied through the modules and
As video data penetration into many information systems requires database support video data is developing. In this article, we introduce a genetic data model that captures the structure of video document and provides a video stream indexing tool. We also discussed the model that can be used for query language function. We have identified the basic operators that should be implemented in query language to support content-based query. The paper also analyzes how these operators are used to provide video data query. The model has been used as a prototype for TV news files and some experimental results have been introduced.
The behavior of scientific and engineering research is becoming critical to the effective management of scientific and engineering data and technical information. The rapid progress of scientific tools, computers and communication technologies enables scientists to collect, produce, process and share unprecedented amounts of data. For example, the mission of Earth Observation Systems Data and Information Systems (EOSDIS) is to manage the data from NASA's Earth Scientific Research satellite and field measurement programmes, as well as other necessary data to explain these measurements support global change research. In addition to being able to process a traffic of 1 trabat data daily to 2000, EOSDIS also needs to provide transparent access to immaterial data stored in the files of several U.S. government agencies and national organizations.
Parallel database systems must support the effective parallelization of complex queries in multi-user modes, i.e. combined with inter-queries~mter transaction parallelization. For this purpose, the dynamic planning and load balance strategy’ is necessary to the existing system state to determine the internal queries parallelity and select the processor for sub-queries. We study these issues to parallel queries combined and show that two sub-problems should be solved in an integrated way. However, more importantly, using a multi-person load balance method, taking into account all potential bottle resources.
In many distributed systems, the copying is often used to provide higher performance, reliability and availability. The light copy update protocol, after the original transaction promises to spread through independent transaction to the copy update, has become popular by the database providers because of its excellent performance characteristics. However, if the light copy protocol is used, they can lead to non-series implementation. In this article, we presented two new light copy update protocols, which guarantees sequential, but imposes weak data placement requirements than the previous protocols. In addition, many naturally appearing distributed systems, such as the distributed database, meet this requirement. We also expand our light copy protocol to eliminate all data placement requirements.
The application of the model shows that it will relate and expand important work on packaging search, transfer files, index selection, dynamic hash-based files, general access route structure, differentiated files, network databases and multi-layer query processing.
Influence detection has become a key component of network management because a large number of attacks are constantly threatening our computers. The traditional influence detection system is limited and does not provide a complete solution to the problem. They are looking for potential malicious activity in network traffic; They sometimes succeed in finding real security attacks and abnormalities. However, in many cases, they are unable to detect malicious behavior (false negative) or they burn warnings when there is no mistake in the network (false positive). In addition, they need comprehensive manual treatment and human expert interference.
We introduce MOCHA, a new self-extended database intermediate software system designed to interconnect data sources distributed. MOCHA is designed to expand to a large environment, and based on this idea, some user defined features in the system should be deployed by the intermediate software system itself. It is by sending Java code to implement advanced data type or customized query operator to a remote data source and running it remote. Optimized query program promotes the evaluation of powerful data reducing operator to a data source site, while performing data penetration operator near the customer site.
Probability theory is a mathematically best understanding model and manipulation of uncertainty information paradigm. The probability of complex events can be calculated from the basic events they depend on, using any series of strategies from the perspective of probability theory. The appropriate strategy is very dependent on the interdependence between the known events. The work of the probability database has previously taken a fixed and restrictive combination strategy (e.g., assuming that all events are double independent). In this article, we will use the paper, the entire class of probability, from the perspective of probability theory, we presented a probability data model and a probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability probability
As Internet traffic continues to grow, the index decrease of the site becomes increasingly complex, performance and scalability are the main problems of the site. the site increasingly depends on the dynamic content generating applications to provide the site visitors with a dynamic, interactive and personalized experience. however, the dynamic content generating requires a cost --- each request requires calculation and communication across multiple components. to solve these problems, a variety of dynamic content encryption methods have been proposed. based on indicators and scalability, based on indicators and scalability, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators, based on indicators,
Database is an integrated and relative and non-relative database storage. because of the summary of history and reference data from many warehouses integrated portals and distribution sources. storage data can be analyzed through multiple SQL processing, storage cq load data from size such as time, product and geography to ari’opeiational system with SQL imerf...seZecf identify trends and gain competitive advantage.
Relative database support applications in a concentrated environment in the 1960s and the 1970s. They progressed to a client / server environment in the 1980s. The 1990s saw the application server has a multi-layer architecture, in most cases supporting RDBMS. Recently we saw the appearance of XML, stored in DBMS, navigating through the XML document, and XQuery query language for XML. In this article, Susan provided an introduction to the network and describes how the database will be used in this new environment.
In this article, we first introduce the data base of the Lotus Domino/Notes group products, and then describe in detail the many archive and recovery improvements introduced in R5, and we briefly discuss some of the changes required for the ARIES recovery method to adapt to the unique storage management features of the Notes.
Similarity search is a very important operation in multimedia databases and other databases applications involving complex objects, and involves finding objects in a data set S similar to a query object q, based on certain similarity measurements. In this article, we focus on similarity search methods, making the general assumption that similarity is by a distance measurement d. In this setting the existing method of processing similarity search usually falls into one of two categories. The first direct index is based on distance objects (based on distance index), and the second is based on map to a vector space (based on map method). The main part of the article is focused on the investigation based on distance index method, but we also briefly describe how it occurs on the basis of distance search method we also provide a direct search frame on the first basis.
This article describes a new way of combining data mining technology with internet data to find feasible marketing intelligence in the e-commerce scenario.
Compensation-based query processing has been proposed to avoid lock disputes between up-to-date transactions and ad-hoc query. This article introduces an undo /no-redo-based compensation algorithm. query will read the database's non-conformity version, but is carried out by the current exchange after non-conformity, so that the query results are consistent with the transactions. by processing the internal record of the database to obtain information about current updates, query does not force up-to-date transactions additional work. a simulation study shows that the query execution response time is significantly improved compared to the previous compensation-based algorithm.
Editor's Notes: For many observations of this issue time, I invite Robert Gephart (Robert Gephart) to observe at the University of Alberta in order to well reflect his observations, as a long-term service, award-winning reviewers can increase their opportunities for quality research published in A!vII in the past two and a half years, I have developed an enormous review of the Boo's hot eyes to evaluate quality research submitted, and to anyone interested in the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and comment of the comment and
Information available on the Internet is growing at a very high rate. in particular, news articles are added and updated around the moment. news receipt system, currently being used, is not much able to handle such a huge number of news articles, effective and accurate. due to the need for frequent and intense processing, a news receipt system needs to be extended, strong and error tolerance. through the use of cloud technology, this can be achieved. a news receipt system on the cloud can be used to catch, process, organize, and can also be used for faster and more accurate receipt. it can be used to operate less supervision or without anything. cloud news, the next generation news receipt system here, designed and implemented, over the number of news receipt, strong and error tolerance through the use of cloud technology, this can be achieved by a news receipt system on the
We introduce the BHUNT setup to automatically detect the algorithm limitations between the two columns in the relationship data.BHUNT’s limitations may be “mixed” in which they hold most, but not all, records, and columns may be performed in the same table or different tables, these limitations are interesting in the background of the data mining and query optimization process, BHUNT’s defining method can be defined by BHUNT’s defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data, defining data
Data is often stored and obtained using B-Trees. In the &cis,ion isugprt application, the key of B-Tree often involves the mix of multiple fields of the "table" of the relationship. In the process of obtaining, it is expected to be able to access a small field in the table based on some key information, in which the key of certain fields may not exist, including a series or a "value" list.
The paper describes the security issues of the federal database management system, which is used to manage distributed, abnormal and independent multi-layer databases, which is based on our previous work on multi-layer security distribution databases management systems, as well as the results of the work of others in the federal database systems.
We describe the use of parallel execution technology and measure in NonStop SQL/MP, a commercial parallel database system from Tandem Computers. Non-Stop SQL uses internal operations parallel to parallel fusion, combination and scanning. Parallel execution includes the launch of multiple processes and the transmission of data between them. Our measurement shows (a) the start costs are unforgettable when the process is reused instead of creating fresh (b) communication costs are significant - they may exceed the cost of the operator, such as scanning, combination or joining.
As the telecommunications industry strives to reinvent itself, the effective management and use of information, the data provided in the background is now the key weapon to win and maintain the customer.The challenge of data management is to introduce a large-scale growth in the data volume and complexity of the distribution processing.The framework and methods of information management are introduced and introduced the term "background data".
In mobile computing systems, in mobile client query data project is important to reduce data access delays in unreliable and low bandwidth mobile networks. However, it must be used effective methods to ensure the consistency between query project and data project on the database server. By exploring the properties of the data project, we propose a query disruption program called: through the absolute validity interval (IAVI). We define an absolute validity interval (AVI) for each data project based on its real-time properties, such as the update interval. Mobile client can be verified by comparing the final update time and the validity of its AVI data project. query project is damaged, if the current time is greater than the final update time through AVI self-update, we can do this by verifying an absolute validity interval (AVI) for each data project
A new type of data processing application is no longer satisfying the capacity of the relative data model provided. An example of this phenomenon is the increasing use of the Internet as a data source. Data on the Internet is essentially non-relative. Therefore, the demand for the database management system is essentially built on advanced data model. Similar binary data model (Rishe, 1992) meets the standards of the model required for the current application, by providing the ability to build a rich program, voluntarily flexible relationship between objects. In this article, we discussed a new design to a similar database management system, based on a similar binary data model. Our challenge is to design and implement a database engine, while the similar model is reasonable, effective application in the application industry and the diversity of applications, providing the ability to build a rich program.
The traditional method of the relative database design is based on the logical organization of the data to a series of relevant standardized tables. An assumption is that the nature and structure of the data in the design phase are well known. In the case of the design of a relative database, to store history of dental epidemic data from a personal clinical survey, the structure of the data is well known until the data is submitted to the database contained. This paper discusses the issues related to the theoretical design of a clinical dynamic database that can adapt to the internal table structure to adapt to the clinical survey data and introduce a prototype database application that can process, display and query dental data.
In the storage database (ODB) system, the database owners publish their data through multiple remote servers, which aims to allow customers to access and request data on the network margins more effectively to do analytical assessments, because the server may be unreliable or may be damaged, query verification becomes an essential component of the ODB system, the existing solution of this problem is mainly focused on the static scene and based on the ideal characteristics of some encrypted primary material. In this work, first, we defined the various basic and practical cost measurements related to the ODB system. Then, we analyzed some different methods in finding a solution that is best suitable for all the important measurement measurement measurement measurement measurement measurement measurement measurement measurement measurement measurement
Publishing Review Hipocratic Otto has been guiding the behavior of the doctor for centuries. Inspired by his desire for privacy protection, it is believed that the future database systems must include the responsibility for the privacy of personal information that they manage as a founder desire. The explosive progress of the network, storage and processing technology leads to unprecedented number of information digitalization. The estimates that the number of information in the world increases twice every 20 months, the size and number of the database increases faster and faster. With this dramatic and upgrading digital data increases, concerns about the privacy of personal information issues arise globally.
Database copying is traditionally considered a way to increase availability and distribution of the performance of the database. Although a large number of protocols provide data consistency and error tolerance has been proposed, but due to its complexity and performance impact, some of these ideas have been used in commercial products. on the contrary, the current product allows inconsistency and frequently uses a concentrated method to eliminate some of the benefits of copying. as an alternative, we proposed a series of copying protocols to solve the main problems related to the database copying. on the one hand, our protocols maintain data consistency and the same transactionability. on the other hand, they provide flexibility and reasonable performance. therefore, our protocols use rich collective copying and frequently use a concentrated method to eliminate the benefits of copying as a substitute, we proposed
We draw the object model to a business relationship multiprocessor database system, using copying and visualizing to provide rapid recovery. In order to accelerate the complex updating operations, we use cross-trade parallelism, destroying such operations to shorter relations operations, these operations are carried out as updating transactions parallel transactions. In order to ensure the accuracy and recovery of the operation, we use multi-level transactions. In addition, we will try to reduce the registration of the compensation reversal operations required by the multi-level concept, by logging in to non-derived data.
Specifically, storage is available, not large enough, extreme points, or time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points, time points.
The Semantic B2B integrated architecture must allow companies to communicate on a standard-based B2B event, such as purchase orders, with any potential trading partner.
Environmental Management Information Systems (EMIS) is a social technology system as a business application to collect, process, and provide environmental information, within the company and exchange with other participants in the industry. They help identify environmental impacts and support measures to avoid these impacts or reduce them. EMIS provides the necessary information support decision-making in the company. Therefore, EMIS can be seen as certain information systems (IS) are usually implemented in the company as part of its Environmental Management System (EMS), therefore, it is the right effort to enable in its Environmental Management System (EMS) practice. To provide a practical example of its application practicality, its application practicality, its application, its application, its application, its application, its application, its application, its application, its application, its application, its application, its application, its application, its application, its application, its application
SQL Server 7.0 offers three different copy styles, which we call Transactional Replication, Snapshot Replication, and Merge Replication. Merge Replication means data changes can be made in any copy and the changes that are made in multiple copies are subsequently combined. Because Merge Replication allows updating the connection of the copy, it is especially suitable for applications that require a large number of autonomous.
QURSED enables the development of Web-based query tables and reports (QFRs) query and report semi-structure XML data, i.e. data interacted by the end user with QFR, irregularities and structural variations. the query group is captured by query group specifications, the group formally encoded several parameter state parts and can describe a large number of query. query components by QURSED produce its XQuery match query, by synthesizing query group specifications query components, which components are activated in QFR and QFR interaction query components, called QURSED editor, query components, query components, query components, query components, query components, query components, query components, query components, query components
Global eCommerce and Electronic Management Program has caused a strong focus that requires databases to effectively store and process data in multiple languages. Although existing databases provide some means of storing and query multi-language data, they suffer proportional discounts to the number of language supported. In this article, we proposed a system that multi-language data management in a distributed environment, in information theoretical way storing data in the form of encoding, with the minimum discounts.
With the current information exploding in the World Wide Web (WWW) a wealth of information about many different topics has been available online. Many sources contain information that can be categorized as a semi-structure page. However, at the present time, the only way to access information is by browsing the individual page. We cannot search for the web document database in a similar way, based on their infrastructure. However, we can provide the database with similar queries to the semi-structure of the WWW source, by building around these sources of packaging. We introduce a method, semi-automatic to generate such packaging. The key idea is to use the formated information page from the source to assume a page's bottom structure. From this structure, we can create a packaging to provide a packaging source and its base, however, we can provide the database with similar types of data.
This paper introduces a form-based similarity measurement, called form-based similarity (AMSS), for time sequence data. Unlike most similarity or similarity measurements, AMSS is not based on a time sequence of individual data points, but based on the same way representing its vectors. AMSS processes a time sequence as a vector sequence to focus on the form of data and by using the similarity of the variable comparing the data form. AMSS is, according to the design, expected for time and width conversion and expansion, but sensitive to short-term conversion.
ACRP's Association Trust Committee Chairman tells him how he left the production and research and development a decade ago to serve as a clinical research manager, which is a move that allows him to enter a role he rarely knows because he did not take part in clinical research in advance, if it sounds like a familiar experience to others, the lessons shared in this column emphasizes the importance of the individual's willingness to continue learning, as well as the organization supports learning.
Data Mining System, DBMiner, has developed multi-level knowledge of interactive mining in a large relative database. The system implements a wide range of data mining functions, including generalization, specification, association, classification and forecast. By incorporating several interesting data mining technologies, including character-oriented guidance, statistical analysis, gradually deepening multi-level knowledge of mining, and meta-rule-oriented mining, the system provides a user-friendly, interactive data mining environment with good performance.
We study using materialized views to answer SQL queries questions. We focus on modern decision support queries, which include attachments, algorithm operations and other (may be user-defined) features, integrated (sometimes along multiple dimensions), and underlying queries. Considering the complexity of these queries, a large amount of data they run, and the requirements for interactive response time, using materialized views (MVs) similar complexities are often compulsory acceptable performance. We introduce a new algorithm that is able to rewrite a user queries so that it will access one or more available MVs rather than underlying tables.
We briefly describe the main characteristics of an effective server-based algorithm for the collection of object-oriented databases in a client server environment. The algorithm is continuous and works simultaneously with the client transaction. Unlike the previous algorithm, it does not retain any data locks and does not need to call the client. It is erroneous, but performs a few logs. The algorithm is designed to integrate into the existing OODB system, so it uses standard implementation technologies such as two stages of lock and writing logs. In addition, it supports client server performance optimization, such as client lock and client flexibility management.
Web caching is a technology to improve the network tra-c on the web. It is a temporary savings of Web objects backward. Three meaningful advertising benefits of Web caching include reduced bandwidth consumption, server burden, and delay. These benefits make the network cheaper, but it provides better performance. This study aims to introduce an advanced machine learning method of classification of problems in Web caching requires a decision to save or not save Web objects on a proxy cache server. This classification siflcation problem challenges include problems in identifying specific points ranking and improving classification accuracy. This study includes four methods, i.e. classification and classification of categories (CART),
Data mining services require accurate input data so that their results are meaningful, but privacy issues may affect the user providing shocking information. We investigate here about the rules of the mining association, whether it can encourage the user to provide the correct information, ensure that the mining process cannot, there is any reasonable degree of certainty, violating their privacy. We introduce a system, based on the probability of user data distortion, can simultaneously provide the user with a high level of privacy and maintain high accuracy in the mining results.
The model is a complex application art description, such as database maps, application interfaces, UML models, ontology, or message formats. This model’s integration problem lies in the core of many meta data applications, such as view integration, data integration intermediate graph creation and ontology integration. This paper explores the two model’s integration problems, which have corresponding compatibility. It introduces the implementation of the integration requirements and a specific algorithm to complement the previous work.
This book introduces space data models, algorithms, access methods and methods of query processing. The main focus is on expanding DBMS technology to accommodate space data. This book also includes space methods used in Geographic Information Systems (GIS). In history, GIS is developed separately from the database system. GIS is specialized in the various aspects of space data processing, including space editing, re-design coordinates and map display. However, their query capacity is generally limited or requires low-level programming. Unlike GIS software, the method includes space data in DBMS using ADT and scalability.
We consider the problem of searching in a big database. The typical application of this problem is genetic data, web data and event sequence. Because of the size of these databases unusually grows, so for these issues the use of memory algorithms becomes unpractical. In this article, we recommend that the data of the font map to a full space, using the wavelet equation. Later, we index these equations through MBR (the smallest edge straight). We define a distance function, which is the lower limit of the actual editing distance between the lines. We try with the closest neighbor queries and range queries results show that our technology will make a significant number of the databases (usually reduced to 95%), thus making the IO and CPU cost significant.
Using views to answer questions is to find an effective way to answer a query, using a set of previously defined materialist views in the database rather than access to the database relationship. The problem has recently received significant attention because of its relevance, a wide range of data management issues. In query optimization, finding a query rewrite, using a set of materialist views can produce a more effective query implementation plan. Supporting the separation of logic and physical views of data, a storage program can be described, using the views of logic chart. Therefore, find a query implementation plan, access to storage problem solving, using query problem. Finally, the data system's problems, in which the source of data can be described, can be resolved in advance by a query implementation plan.
SEMCOG (SEMantics and COGnition-based image retrieval) is designed to integrate cognitive-based methods that provide users with greater flexibility to submit queries.SEMCOG's image match is based on the object in the image rather than the flexibility of the entire image.In SEMCOG, a query "search all the images in which a man is on the right side of the car and a man looks like this image" can use a combination of visual and expression.
Through sequence merger is a key summary technology in a variety of online applications, including decision support and network management (e.g., IP classification, refusal service attack monitoring). Despite the number of recent studies, focused on the online collection of collections (e.g., quantum, thermal elements), surprisingly, a little attention has been spent on the summary of the sequence structure in the flow of data. The problem we study in this article is to find a sequence heater (HHH): according to a sequence and a split φ, we want to find all HHH nodes, in the data stream there is a total number of descendants, not less than the total number of nodes in the data stream, after the score nodes, HHH nodes, H nodes, H nodes, H nodes, H nodes, H nodes, H
High-speed computers and telephone networks carry a large amount of data and signal traffic. build and maintain these networks engineers use a combination of hardware and software tools to monitor network traffic. some of them work directly on the site network; other record data on the tape so that it can then be analyzed offline through the software. most analytical tasks require dozens to hundreds of gigabyte of data.
This paper introduces the temporary extension of the relationship algorithm that is different from the original relationship model, but at least the same expression of any previous method. This algorithm uses multi-dimensional time shortcuts to capture the data's complete temporary behavior. The basic relationship operations are redefined as the continued extension of existing operations, in a way that retains the basic algorithm equal screenshots (i.e., the traditional static) algorithm. A new operation, i.e., the temporary projections, is introduced. The complete algorithm is a specific format and definition function. The algorithm defines the algorithm defines the algorithm defines the algorithm defines the algorithm defines the algorithm defines the algorith
The current computer science speech in Ontologies is based on computer resources, representing the reach of protocols.
Our work tries to reduce the delay time by converting the delay process to the editing time. In addition, we offer an alternative delay program based on logic to mixed integrated programming. This can be used for the study and implementation of the delay program for the database and the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program for the delay program
Current data models such as NF2 models and object-oriented models support the setting of assessment properties. therefore, combination predictions can be made based on the setting comparison. This paper introduces and evaluates several major memory algorithms to effectively evaluate this type of combination.
Therefore, if XML is to meet its potential, it takes some mechanisms to publish relationship data as a XML file. To this goal, a major challenge is to find a way to effectively structure and label data from one or more tables as a series of XML files. Different alternatives are possible, depending on when this processing occurs, and the number it takes place in the relationship engine. In this article, we will describe and study the performance of these alternatives. In which, we explore using new extensions and integrated features to build complex XML files. To this goal, a major challenge is to find a way to effectively structure and label data from one or more tables as a series of XML files. Different alternatives are possible, depending on when this processing occurs, and the number of it takes place in the relationship.
In this paper, we explore a space merger method to dynamically build the index of the tree called the tree in the merger time. This method uses the knowledge of the data set involved in the merger process. The tree is R tree, such as the structure, and is divided into the seed level and growth level. The seed level nodes are used to guide the tree growth during the tree construction. The seed level can also be used to filter some input data, thus reducing the size of the tree. We develop a technology, using the middle list of the tree in the merger time. This method uses the knowledge of the data set involved in the merger process. The seed tree is R tree, such as the structure, and is divided into the seed level and growth level. The seed level nodes are used to guide the tree growth during the construction. The seed level can also be used to
With the rapid development of the Internet and the World Wide Web (WWW), a great deal of information is available and ready to download, most of which are free. at the same time, the hard disk with a large capacity of the hard disk is available at a price, millions of files, we often throw a lot of different types of files to our computers, without much thinking. on the other hand, the file system has not changed so much over the past decades. they organize the directory of the files, forming a tree structure, and a file is identified by its name and name in the directory tree. remember the name of the file was created for a period of time, and the thousands of files will never be a easy task to help these files from a search result.
The paper describes the POESIA system composition method for Web services. This practical method is strongly focused on the use of specific field multi-dimensional ontologies. Inspired by application requirements and based on ontologies, workflow and activity models, POESIA provides accurate operations (integrated, specialized and instance) supporting Web services composition.
Advanced technological applications, such as routing systems or network management systems introduced the need for complex operations of large-scale charts. Effectively support this requirement is now considered a key feature of the future database system. The paper proposed an abstract mechanism, called the database chart view, defining and manipulating various types of charts stored in relationships, object-oriented or file system. The database chart view provides a functional definition of charts, allowing it to manipulate independently of its physical organization. Derivative operators are recommended to define new chart view existing. These operators allow the composition, in a single chart view, chart with different nodes and marginal types and implementing different chart view mechanisms with the model, where and set the chart view chart view chart.
When implementing a persistent object on a relationship database, a major performance problem is forecasting the data to try to reduce the number of circles of the database. This is difficult to navigate applications because future access is unpredictable. We recommend using an object as a future access forecast load background, where the background can be a stored relationship collection, a query result, or a complex object. When an object of O state load, the background of O is forecast similar status of other objects. We introduce a design to maintain the background and use it to guide forecast.
The University of Georgia’s Mass Distributed Information Systems Laboratory (LSDIS) was established in the autumn of 1994 to conduct two complementary subjects of information systems: enabling the process of information and network organization; enabling the subject of information; achieving significant advances in the network infrastructure of the integration of computers and communications making millions of information sources accessible to a variety of information.
While physical sensors have been used for a long time in astronomy, biology and civil engineering, the recent appearance of wireless sensors networks and RFIDs has caused the rise of sensors interest in the academic community and industry. In this paper, we study the spectrum of the sensors platform, from billions of satellites to small RF labels, and discuss the technological differences between them. We show that the battery sensors network, with a low-power multiple radio stations and low-cost processors, occupies a dessert in this spectrum, which is filled with opportunities for new database research.
We introduced new technologies to monitor tracker generating and automatic web information extraction, a system called Lixto implementing these technologies. Our system can produce tracker, translating the relevant HTML pages into XML. Lixto, one of the work prototypes has been implemented, helps users semi-automatically create the tracker program, by providing a fully visual and interactive user interface.
Over the past few years, there have been many proposals on the expansion of data models to include the temporary size of the data, which have been significantly different globally.
We introduce a new algorithm with the use of materialist views calculating answers to SQL queries with combinations and collections, in the existence of multi-level definitions, in addition to its obvious potential in queries optimization, this problem is available in many applications, such as data storage, very large trading recording systems, global information systems and mobile computing, where access to local or hidden materialist views can be cheaper than access to local or hidden materialist views.
We present a rich description language for semi-structure trees data, and we explain how these descriptions are related to the data they describe.
In this paper we introduce a process management technique to coordinate the creative and large-scale distribution of the process. Our method is to use the analysis results in the fields of software development, construction/engineering/building etc., and the core conclusions of the electronic learning process are as follows: (1) cooperation process is the same description of the production process, but these descriptions are different interpretations, depending on the nature of the process, (2) the process description interpretation depends mainly on the flexibility of the control flow and the need for data flow, and the relationship between them, (3) the intermediate result management is the core characteristic of supporting the cooperation of these processes.
Introduction Modern database management systems have many mechanisms that allow administrators to use system resources, such as disk space and memory, but they usually do not include a mechanism that guides the use of the most important system resources for CPU time. Traditionally, this feature is the host operating system programmer. Because of this, the only use of Oracle controls the user's CPU using the platform imposes a hard limit on CPU consumption. If a user's meeting exceeds its CPU limit, it has ended. In Orac, we introduced the Oracle database resource manager, a new DBMSCPU management mechanism that allows administrators to allocate the work load and the logical distribution between CPU resources.
In this short article, we present some challenging issues that support the effective query of bioinformatics data sources and describe them through the selection of representative search scenes provided by biologists.We ended this article discussing how advanced research and technological development in the fields of Semantic Web, Ontology, Internet Data Management and Internet Computing Systems can help solve these problems.
The basic idea is to calculate many integrated values for small to medium size integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integr
A Problem Solving Agent (PSA) is a hardware or software system or person who has the ability to perform a terminal task in an application field composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks composing one or more tasks
We summarize four interesting and challenging issues in the development of very large-scale databases, i.e. failure recovery, continuous updating maintenance, planning and query optimized cost models, as well as meta data definitions and management. For each issue, we give us the reason it is important but not in the study of literature and business products, and our current research solves it.
In this article, we proposed a new definition of the distance-based exchange, based on a point of distance from its closest neighbors, we rank each point based on its distance to its closest neighbors, and announce in this ranking the top n point is exchange. In addition to developing a relatively simple solution to find such exchange based on the classic internal exchange connection and index connection algorithm, we develop an efficient division based on the algorithm of the mining exchange. This algorithm first division input data set to the division subgroup, and then in determining that they cannot include the entire division of exchange these results in the calculation of substantial savings.
Database settings effectively contain a lot of information, often collected from a variety of independent sources. Decision support features in a stock, such as online analytics processing (OLAP), involve hundreds of complex integrated query data large amounts of data. It is impossible to calculate these queries by scanning data sets every setting. Stock application thus builds a lot of summary tables, or materialized integrated views to help them improve system performance. As a change, mainly new transaction data, collected in the database, all integrated query tables, depending on these data need to be updated.
Traditional methods to solve historical problems take an unusual time evolution line; that is, a system (database, relationship) with time evolution, through a order of transactions. Each transaction is always applied to these problems unique, the current state of the system, leading to a new current state. However, there is a complex application, where the system's state evolution countless evolution line. In general, it creates a tree (hierarchy) of evolution line, where each tree's time point represents the time evolution of a specific subsystem. Multi-line creates new historical problems, such as the vertical vertical historical problems. The key feature is that these problems are part of the historical problems; the response to the historical problems should not be a replication of the historical problems, because this system is a replication of the historical problems.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
XQuery is a XML query language that is currently being developed in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C work projects and several reference implementations have been available on the web. If successful, XQuery is likely to introduce one of the most important new computer languages in a few years. This tutorial will provide an overview of the synthesis and grammar of XQuery, as well as an insight into the principles of language design.
New telecommunications services and mobile networks are introduced in the telecommunications network. Compared to the use of traditional databases, telecommunications databases must meet the very strict requirements for response time, channels and availability. ClustRa is a telecommunications databases prototype used to operate through ATM interconnected standard workstations. To meet the channels and real-time response requirements, ClustRa is a major memory database, neighboring mainly, memory storage.
Object-oriented and object-related DBMS supports setting assessment properties, which is a natural and short way to model complex information. However, there has been a limited study to the current assessment of query operators applicable to setting. In this paper, we study the two relationships of their setting assessment properties. Different combination types are considered, i.e. combination capacity, setting equality, and setting super layer combination. We show that converting files, a strong index choice query, can also promote effective assessment of the majority of the combination forecasts.
We develop a new chart unstructured data. Traditional chart is similar to a programming language type system. However, for unstructured data, the basic type may be less limited and therefore requires the expression of the data’s restrictions in alternative ways. Here, we recommend that data and charts are described as marginal labels. We develop the concept of compatibility between chart databases and chart charts, and show chart charts have a natural and efficient calculation order. We then check certain subclasses of charts and show charts closed under the query application.
Specifically, the preliminary vocabulary and the test package of the temporary database concepts were distributed before the seminar, both modified according to the analysis and criticism of the seminar, and the language design committee was established after the seminar, aimed at developing the consensus of the temporary database, extending to SQL-92; the design was also benefited by the seminar discussions, which recorded the seminar discussions and consensus.
1 Self-Organization Structured P2P Systems In the P2P community, a basic correct difference is made by the information provided by non-Structured and Structured P2P systems for the resource location. In the principle of non-Structured P2P systems, the partner does not know the cost of the resources that neighbor partner retains in the new partner network. Usually they solve search requests through flood technology. Gnutella [9] is the most outstanding example of this category. In contrast, in Structured P2P networks, the partner in Structured P2P networks, the partner in Structured P2P networks, the partner in Structured P2P networks, the partner in Structured P2P networks, the partner in Structured P2P networks, the
The sensors' broad distribution and availability of small-scale sensors, sensors and embedded processors is to transform the physical world into a computing platform. One such example is a sensor network, composed of a large number of sensor nodes, combined with the physical sensory capacity, such as temperature, light, or earthquake sensors with the sensor network and computing capacity. Application range from environmental control, warehouse storage and health care systems to the military environment. The existing sensor network is the sensor is pre-programmed and sends the data to a central front end, where the data is integrated and stored for offline query and analysis. This method has two main disadvantages. First, the user cannot change the system's behavior, such as temperature, light, earthquake sensors and network design factors, but the system's main sens

However, while the “active database system” has been required by many systems and in many research fields (even beyond the database technology) the concept of “active object” or “event” is still unclear what features the database management system must support in order to be legally regarded as an active system.
In many applications, such as production line or stock analysis, it is significant to create and process a large amount of data at high speed. This continuous data flow with unknown size and purpose is also known as data flow. Data flow processing and analysis are the common challenges of data management systems as they need to run and provide real-time results. Data flow management system (DSMS), as an advanced database management system, has been implemented to solve these problems. DSMS must adapt to different levels of data flow concepts, such as query language, processing or optimization. In this chapter, we provide an overview of the basic principles of data flow, the principles of construction of DSMS and the use of query language.
This text specifies the temporary extension to the SQL-92 language standard. language is specified as TSQLZ. the document is organized as follows. the next section shows the starting point of the design, SQL92 language. section 4 lists the characteristics required for the TSQL2 language design committee to reach a consensus. section 5 introduces the main concepts of TSQL2. compatibility with SQL-92 is the topic of section 6.
MDM is a tool that allows users to define different data models of charts and perform charts translation from one charts to another charts. These features can be based on a customizable and integrated CASE environment support analysis and information system design. MDM has two main components: charts administrator and charts administrator. charts administrator supports a special user, charts engineer, in defining different charts, based on a limited combination of charts structure, covering almost all known concept models. charts administrator allows designers to create and modify charts defined charts, and at every moment create a charts translated to any charts available charts between charts is automatically derived, combined with a pre-set of elements.
Today, more and more commercial applications depend on the spatial data type and require a solid and smooth integration of appropriate access methods to a reliable database server. This paper proposes an effective, dynamic and extensible method to manage a level of interval sequence within a level of object relationship database system. The technology submitted is perfectly in accordance with the concept of spatial filling curve, therefore, generalized to spatial expansion of objects in multiple levels of data space. Based on the relative interval tree, the method is easily embedded in the modern expansion index framework and significant lines and routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with routes with
The query optimizer relies on accurate cardinal estimates to produce a good execution plan. Despite decades of research, existing cardinal estimates are not accurate complex queries because of failing simulation assumptions rather than capturing interrelationships. In this work, we show that it is possible to learn interrelationships, in a database all of the tables, without any independent assumptions. We introduce NeuroCard, a joining cardinal estimates, to build a single nerve density estimator throughout the database.
Publisher Review Web Services are widely listed as the basis for the next generation of computing and solving integration, which is one of the biggest IT challenges. Web Services have learned a lot from the development of database management systems (DBMS), while the DBMS community has contributed a lot to the achievement of Web Services. Web Services have learned a lot from the failure of the previous service-oriented building (SOA). In the 1980s, multiple distributed computing recommendations appeared, including the Open Software Foundation’s distributed computing environment (DCE), the Common Object Request Broker Architecture (CORBA) of the Object Management Group (OMG), the Microsoft’s distributed component object model (DCOM), and the multiple distributed DBMS prototypes and products.
Current main memory (DRAM) access speed is far behind the CPU speed.Cache memory, made by static RAM, is used in today's architecture to replace this gap.It provides access delayed 2 to 4 processors cycles, contrary to the main memory, which takes 15 to 25 cycles.Therefore, the performance of the CPU depends on how to use cache.We show that there are significant advantages in re-designing our traditional query processing algorithms so that they can better use cache.
The database has been successfully used to assist in decision-making, providing a global view of corporate data, and providing an online analytical processing mechanism. Traditionally, the database is used within the scope of enterprises or organizations. The Internet and WWW’s growth, however, created new data sharing opportunities in advertising shoots, geographical tensions and possibly mobile users. Because it is useless for each business to build a global infrastructure, the current application is processed by a central warehouse. This is often due to the load of the central server and the low transmission rate of the network. In this article, we presented an architecture of the OLAP warehouse server (OCS). However, the Internet and WWW’s growth, created new opportunities for data sharing in advertising shoots, geographical tensions and possibly mobile users because it is useless for each business to build
To this goal of the temporary database, the concept of the temporary type is the correct extension of the traditional functional dependency, and takes the form of time breakdown and temporary functional dependency (TFDs). the temporary type is from time breakdown and time breakdown to time breakdown to time breakdown to time breakdown to time breakdown to time breakdown to time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown and time breakdown to time breakdown
Publishers review eXtensible Markup Language (XML) has become the main data exchange format for various applications (supply chain, scientific data processing, telecommunications infrastructure, etc.) not only is now processing more and more XML data, but also is increasingly used in business key applications. Effective and reliable storage is an important requirement for these applications.Related to the relationship engine for this purpose, XML developers can benefit from data management services (including competitive control, accident recovery and scalability) and highly optimized relationship query processors.
Considering the assessment of a large number of XPath expressions in the XML stream, our main contribution is to show that Deterministic Finite Automata (DFA) can be effectively used for this problem: in our experiments, we achieved a breakthrough of about 5MB/s, independent of the number of XPath expressions (in our tests, up to 1,000,000).
This paper introduces a new data structure, called the V tree, designed to store long-series points in the 2D space, but still allow effective access to their pieces. They also optimize access to a series point when queries involve smaller scale changes. The V tree's location works in the background of the long field, they can be considered a variable of the R tree. The V tree's design is a matter of storing and restoring quite long geographical objects, such as the river or political boundaries, and the geographical queries usually only access these pieces, often using smaller scales.
Web Application Server (WAS) is based on the component of the web application implementation and implementation of the intermediate software platform. In order to meet increasing QoS requirements, WAS must be able to adapt in the implementation process to change itself and to respond to the conditions of change in its external environment. In order to adapt to this change, WAS should provide running time configuration and running time reorganization. Unfortunately, most major web Application Server adopts a unique time architecture and the "Black Box" philosophy to adapt to their design and cannot correctly solve these requirements. In our view, adaptation and reorganization WAS should be available at any time throughout the life cycle. On this paper, a intermediate structure (Web Frame) reorganization supports multi-layer and time reorganization is unfortunate.
Recent high-performance processors use sophisticated technologies to exceed and simultaneously carry out multiple calculations and memory operations. Intuitively, these technologies should help database applications, the performance of these databases increasingly calculate and memory boundaries. Unfortunately, recent research reports that faster processors will not improve the performance of the databases systems to the same degree as the scientific work load. Recent work databases systems focus on reducing memory delays, such as storage awareness classification and data placement time algorithms, is a step to solve this problem. However, in order to design the best high-performance DBMS, we must carefully evaluate and understand the databases of the databases of the databases of the databases of the databases of the databases of the databases of the databas
Today's object-related DBMS (ORDBMS) is designed to support new application fields, providing an extended architecture, supplementing to the domain-specific database extension, provided by external suppliers with abstract data type template structure. An important aspect of ORDBMS is to support extended indexing, allowing core database server to expand through external access methods (AMs). This paper describes a new method of extended indexing implemented in Informix Dynamic Server with Universal Data Options (IDS/UDO).
Dear ACM TODS community, I wish you were good and safe in these very difficult moments.I am honoured to be the next editor and chief executive of ACM Database Systems Trading.No doubt, we are a basic community in computer science and other fields, developing methods that play an important role in many fields and contribute to insight and innovation.ACM TODS provides a place for some of the best work in the field and I thank our research community for its influential contribution and support.
The Community Earth System Model Data Management Program (CESM)[1] comes from the historic background of the National Atmospheric Research Center (NCAR) and its policies, definitions and characteristics are detailed.CESM Data Management drivers are being discussed, including the upcoming dual model comparison project 5 (CMIP5) and the ongoing Earth System Network (ESG) project, as well as the strategy for addressing these drivers.
In the semi-structure database, there are no pre-set charts. In order to provide the benefits of the charts in this environment, we introduce the data guidelines: a short and accurate structural overview of the semi-structure database. the data guidelines as a dynamic chart generated by the database; they are used to browse the database structure, submit queries, storage information, such as statistics and sample values, and allow queries to be optimized. This paper introduces the theoretical basis of the data guidelines as well as its creation of the algorithms and increased maintenance overview.
We introduced a new algorithm for calculating the amount of big data sets in a single channel. approaching guarantees are clear and apply to voluntary value distribution and reach data sets distribution. the main memory requirements are less than those mentioned in the previous scale order. We also discuss the method that will approach the algorithm with random samples further reduce memory requirements. through the samples, approaching guarantees are clear, but it is possible that they apply to a (user control) trust parameter. We introduce the algorithm, their theoretical analysis and simulation results in different data sets.
Due to the current cost of memory and large-scale storage and bandwidth requires large-scale multimedia databases, serial storage servers (including RAM, disk storage and robotic-based third-party library) are increasingly popular. However, the relevant research is that the lack and use of third-party storage is only for enhanced storage purposes. This work, using the increasing performance of providing (e.g., the manager) modern band library products, designed to use third-party storage to improve the performance of the system. We consider the problem of continuous data from its permanent location to third-party storage and display purposes. Our main goal is to save the traditional technology of the second-party storage bandwidth, but requires continuous display purposes, and not to provide additional space.
Based on the hash extended distributed data structure (SDDS), such as LH* and DDH, for connected computers (multicomputer) network has been proven to open a new perspective of file management. We propose a family order SDDS, known as RP*, providing orders and dynamic files on multiple computers, thus more effectively processing the range of queries and orders of file channels. The basic algorithm called RP*N, building files with the same key space division as a B tree, but by using multiple computers, avoiding indicators.
This paper is accompanied by an annual clinical update seminar held in Brussels on January 20, 2018, discussing the best paper published in the previous year (author opinion).
There are several database systems available for free to study the community, full access to source code. Some of them are produced by completed research projects, others have been developed outside the research community. How is the database community best using these publicly available systems? The most widely used open source database is MySQL. Their goal is to become "the best and most commonly used database in the world".
One of the reasons is that these graphs often involve model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model
A major disadvantage of this method is that whenever a new extraction target appears or a module is improved, the extraction must be repeatedly applied from cutting to the whole text body, although only a small part of the weight may be affected. In this article, we describe a new method of information extraction so that the extraction needs to be expressed in the form of the database query, these databases are evaluated and optimized.
George Mason University was founded in 1972 as an independent state university, its development was marked by rapid growth and innovation planning, resulting in the 1997 enrollment of more than 24,000 students. It is located near Fairfax, Virginia (about 15 Britain West Washington), with many government agencies and industrial companies specializing in information-intensive products and services. Information and Software Systems Engineering (ISSE) is one of the six departments of the GMU Information Technology and Engineering School (SITE).
In the information integrated system, the source may have multiple and limited query capabilities. In order to obtain the most information from these restrictive sources to answer a query, it is possible to access the source not specified in the query (i.e. from the query source). In this article, we propose a query planning framework to answer the query, in the presence of a limited access mode. In the framework, query and source description are translated into a repeated data programming. We then solve the optimization problem in this framework, including how to decide whether access to the query source is necessary, how to choose a query useful source, and how to test the query content we develop an effective program to answer a query.
Traditional database systems provide a user with the ability to query and manipulate a database state, that is, the current database state. However, in several emerging applications, being able to analyze the “if” scenario to understand the impact of an update (from the promise of this update) is crucial. Examples of applications include presumed database access, active database management system, and version management to mention some. The current model of core paper is to provide flexible support applications, such as these by improving the database state. representing the current database state update, is the first-level citizen. The description of the database is an application language, which will make the application application, the application application, the application, the application, the application, the application, the application, the application, the application, the application, the application, the application, the application, the application, the application
This method, although it will be effective space (someone storing the combination we need) will waste a lot of time (create all possible combinations). this paper introduces some algorithms starting from a seed combination (a has already met Brian’s forecast we want to evaluate) and grow them to the largest combination.
Object-based databases can take advantage of any application's design and implementation. Because of the growing popularity of the databases systems, many new databases systems based on different data models and implementation have entered the market. Databases systems have complex architecture, but they are key factors behind the business transformation. In any category, selecting the best one is based on performance analysis important task. This chapter deals with databases assessment methods, which include databases analysis tasks and performance analysis tasks.
This paper describes the design and implementation of PEST0, a user interface that supports browsing and query objects database. PEST0 allows the user to navigate the existing relationships and objects display tools. In addition, the user can submit query through an integrated query paradigm ("query site") as a natural extension of browsing. PEST0 is designed to move to any object database system, supporting high-level query language; In addition, PEST0 is extensible, providing special predictive formation and objects display tools for new data types (such as image or text).
Higher education institutions (higher education institutions) are increasingly focused on their environmental impact, which requires improved access to environmental information to improve decision-making and sustainability efforts. Educational institutions have different focus on other more industrial organizations, so the framework of these organizations is not necessarily suitable for the educational environment. Although some environmental management information systems (EMIS) have been proposed, due to the existing broad definition, should include a lack of understanding of the components in such systems.
This paper describes the overall design and architecture of the wood XML database system that is currently implemented at the University of Michigan. The system is based on a large number of wood processing algorithms and local XML storage. New access methods have been developed to evaluate queries in the context of XML and developed new cost estimates and queries optimization technologies. We introduce the performance digital to support some of our design decisions. We believe that this system's key intellectual contribution is a comprehensive timely queries processing ability in the local XML store, all the standard components of the relative queries processing, including the algorithm rewrite and cost-based optimizer.
A radar transmitter is equipped with a radar transmitter equipped with a regulator arranged in a oil-filled room, the regulator remains in the space relationship with the interior walls of the room to form a medium space of exchange flow of oil. The room is clearly visible or watt-shaped, covering a dry or watt-shaped cover. In the interior room or space between the cover and the regulator, the internal space is placed by oil, arranged, on the one hand, a magnet is attached to the cover and, on the other hand, a triangle, it is installed directly under the opening of the cover.

On 7 and 8 March 1996, the first international real-time database seminar (RTDB'96) took place in Newport Beach, California.Seminary participants from about 50 countries.
A real-time database system (RTDBS) can be seen as a combination of a traditional database management system (DBMS) and real-time system. Like DBMS, it must process transactions and guarantee ACID database properties. In addition, it must run in real-time, meet the time limit imposed on transactions. A RTDBS can be as a separate system or as an integrated component in a larger multiple database system. Published in 1988 a special issue of the ACM SIGMOD record in real-time database reports the birth of the RTDBS research area—a area that gathers researchers from the database and the community of real-time systems. Today, almost a few years later, I am pleased to review the current review of this special RTCM review and in the recent SIGM community and the other records.
Ad hoc queries are difficult in very large data sets because it is usually impossible to have a whole data set on the disk. Although compression can be used to reduce the size of the data sets, compression data is obviously difficult to index or access. In this paper, we consider a very large data set, including several different time series. each point in the series is digital. We show how to compress such data sets to support ad hoc queries format as long as a small error can be tolerated when data is not compressed.
To improve the query performance, the modern OLAP system uses a technology known as practical pre-combination, where the selection of the collection of queries is materialized and re-use of other collections; the complete pre-combination, where all the combinations are materialized, is unimaginable. However, this re-use of the collection is continuous size and the relationship between facts and sizes meet the strict limits, this book significantly expands the range of practical pre-combination to meet the range of reality, in the range of reality, in the range of reality, in the range of reality, in the range of reality, in the range of reality, in the range of reality.
SIGMOD Record encourages readers to join the dialogue by writing comments on the latest publications in the field. for more information, please contact Editor Paul King at paul.h.king@vanderbilt.edu
In this article, we proposed a new method to index a large number of points and space data in a high-size space. analysis shows that indexing structures such as R* trees are not suitable for indexing high-size data sets. the main problem of the indexing structure of R trees is the transition of the marginal box, increasing with the growth of the size. to avoid this problem, we introduce a new directory organization, using a split algorithm to reduce the transition and further use the concept of supercode.
As a result of the world wide network development, the integration of diversified data sources has become a major concern for the database community. the appropriate building and query language has been raised. however, the data conversion issue, which is essential for the development of the mediator / converter building is still largely not explored. in this article, we introduce the data conversion of the YAT system. the system provides tools to identify and implement the data conversion in the use between diversified data sources. it depends on a medium software model, a declaration language, a custom mechanism and a graphic interface. the model is based on the name of the tree and the labels. like the semi-structured data model, it is simple enough to represent any data instance of the originality of the system, it allows the originality of the project.
Effective storage, indexing and accessing space data development are some of the tasks that space database management systems (STDBMS) must support. Considering the indexing method and access structure designers, in this article we review the GSTD algorithms for creating space data sets based on several parameters defined by the user and introduced the WW-based environment to create and visualize these data sets.
The computing network provides access to the distribution of computing resources and the distribution of data resources, creating a unique opportunity to improve the access to information.When the database is from any platform, the application can be developed, supporting the use of non-traditional computing resources, the computing network will allow collective progress in the knowledge network, in which researchers collaborate on common issues, by publishing the results in the digital library, the digital government, in which policy decisions are based on knowledge from the expert team access to the distribution of the database.
Background exchange strategies introduce a new method to facilitate data access, in which an abnormal conflict between systems is not unidentified by one side, but by a background comparison, by a <italic> background exchanger</italic> detection and reconciliation.
In order to provide the database availability under the presence of nodes and site failures, the traditional L security algorithm allows the original and hot station copy to be evaluated on the same site, which means that the failure of the single primary nodes must be regarded as the failure of the entire original site. In addition, this excludes the similar site configuration, in which the original copy is located on the site of the closest access to the customer. In this article, we introduce three new L security algorithms, allowing the above limitations to be removed. The relative performance of these and traditional algorithms is evaluated through simulation research. Our main conclusion is that the limitations of the traditional algorithm can be removed without significantly increased processing, during the normal operation, the main copy is based on the customer's location in this article, we
In exploring the research issues of space databases (SIGMOD record No. 19, No. 4, December 1990) we point out the need for a solid analytical framework for a wide range of space access method analysis comparison. this comparison usefulness, even very closely related access methods, is shown in [FALO87]. meaningful analysis comparison is the necessary prerequisite for the existence of strong analytical results of personal access methods. In the next article, Salzberg and Lomet take the worst case analysis results of fans and average storage using the HB tree they get [LOME89, LOME90] and will expand the analysis to another solid method, command code ENOR [84].
Description Description Description Description Description Description Description Description Description Description Description
Mobile computing has the potential for global information management.Mobile computing data management issues have recently gained some attention and designed adaptive transmission protocols as an important forecast.These protocols are used by the database server to dynamically determine the content of bbroadcast to respond to the mobile and demand patterns.In this article, we design these protocols and propose effective recovery strategies that customers can use to download information from the broadcast.The goal is to design the cooperation strategy between the server and the customer to try to reduce the customer's energy spending.We evaluate the performance of our protocols by analyzing and simulating.
However, only a few possible Bitcoin Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic Graphic
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
While physical database adjustment has gained a lot of attention over the last decade, logical database adjustment seems to be studied.We have developed a project called a DBA company that is committed to understanding the limits of logical database adjustment that can be achieved.In this setting, it is necessary to solve two major data mining problems: the first is the design of the algorithms of functional dependency and integration of dependency, and the second is about the interestingness of the knowledge found.In this article, we point out some relationship between database analysis and data mining.In this setting, we draw the basic topic of our method.Some database applications can benefit from our project, including logical database adjustment.
Information integration is a core issue of distributed databases, cooperative information systems and data storage, which are key areas of the software development industry. The two key factors of the design and maintenance requires information integration applications are the concept model field, and rational support conceptual representation. We show that knowledge representation and rational technology can play an important role, the two factors, by submitting a description of the logic based on the information integration framework. We show that the development of successful information integration solutions requires not only the use of very expressive description logic, but also significantly expanding them. We introduce a new method, concept model information integration, allowing the appropriate model of the global concept application, information source and individual composition. We can provide for different methods, a description of the logic based on the information framework.
Internet sites are increasingly dependent on the database management system. There are several reasons for this trend: as the site grows, content management becomes impossible, without using DBMS to keep track of the essence, origin, author, and modify the history of each article. As the site becomes more interactive, tracking and logging user activity and user contribution creates valuable new data, again is the best management of using DBMS. The customer center's electronic business new paradigm puts an advantage of participating users, building a relationship with them on visit, and using their expertise and feedback. Support this paradigm means we need not only to track users access a site, we must also allow them to provide comments and contribute in various ways to the site content; Of course, this requires us to use DBMS, using DBMS, using DBMS, using DBMS, using DBMS, using DBMS, using DB
When SQL and relative data models were introduced 25 years ago as a general data management efficiency data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data collection data
definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition
In RBE and WYSIWIG the programming process begins with an example. Rendering By Example Rendering By Example (RBE) was presented in [KZ95] as a statement language to express a data transmission, where a transmission is defined as a data transmission with subsequent browsing and interaction. Data transmission by a set of screen widgets population data. This transmission application allows browsing and interaction with data in a specific way for the application. These applications specific transmission applications make it easier for the user to find and absorb data. Data transmission is widely used for most applications, with a GUI interface, therefore there is no new concept. But here the innovation is to build such a application transmission using this application transmission will require an artistic step to create the application.
We will introduce the AJAX system to two real-world issues: the consolidation of the telecommunications databases and the conversion of the library references into a set of clean, normalized and free relationship tables, keeping the same data.
This paper introduces a new method, the database disk bubble, known as the LRU-K method. The basic idea of LRU-K is to track the past K reference to the popular database page time, using this information to statistically estimate the reference indirect time on a page. Although the LRU-K method makes the best statistical conclusions under the relative standard assumption, it is quite simple and causes a few bubble. As we through the simulation experiment showed, the LRU-K algorithm exceeds the difference between the common bubble algorithm in the frequent and frequent reference pages.
Many applications consisting of data streams are internal distribution. Due to the amount of input flow and other system parameters, such as the available calculation resources can vary significantly, a flow query program must be able to adapt to these changes. the flow query program operators use the routing double in multiple data flow management systems as adaptive query optimization technology. the use of the routing policy can have a significant impact on the system performance. In this article, we use a routing network to simulate a distributed flow query program and determine the response time and system communication performance measurement. We also propose and evaluate several practical routing policies for the distributed flow management system.
However, many subtle challenges occur in building a real AQP engine that can be deployed and used in the real world applications. These subtle are often ignored (or at least not written) of the theoretical literature and academic prototypes the same. First to our best knowledge, in this article, we focus on these subtle challenges, one must be solved when designing a AQP system. Our intention for this article is as a manual to list the key design choices, the database practitioner must be aware of when using a AQP system rather than prescribing a specific solution for each challenge.
Today, data plays an important role in people's daily activities. Through some database applications, such as decision support systems and customer relationship management systems (CRM), useful information or knowledge can be obtained from a large number of data sources. However, research shows that many similar applications fail success. There are many reasons leading to failure, such as system infrastructure design or query performance. But there is nothing more accurate than data quality issues lack concerns. High quality data is a key to today's business success. High quality data is a key to clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean and clean
We provide a web service platform. Web service is implemented in a special XML programming language, called XL [1, 2]. a web service receives a XML message as a input and returns a XML message as a output. the platform supports some features that are specifically used for the implementation of Web services; for example, logging, timetable, dialogue, workflow management, automatic transactions, security. our platform will meet all W3C standards and emerging recommendations. the programming language is very abstract and can be automatically optimized (such as SQL).
Prospector Multimedia Object Manager is a content analysis multimedia server designed specifically for a large-scale parallel processor environment. Prospector defines and manipulates user-defined features while analyzing/manipulating multimedia objects content. Based on large-scale durable data sets, many computing intensity applications of the technology include: fingerprint matching, signature verification, face identification and voice identification/translation.
A database is an integrated database, the data is collected from multiple data sources and supports online analytical processing (OLAP). Generally, the database queries are often complex and involve a large amount of database data. To keep the database data consistent with the source data, the database changes should be regularly spread to the database. Because the spread of changes ( maintenance) is package processing, it takes a long time. Since queries transactions and maintenance transactions are long and involve a large amount of data, traditional competitive control mechanisms, such as two stages of lock, are not suitable for a database environment. We recommend a multi-transforming competitive control mechanism to adapt to the database using multi-dimensional database (MAPOL) server (MAPOL) server (MAPOL) server (MAPOL) server (MAPOL) server
In this article, we presented a new technology called Bypass processing to evaluate such a Bypass processing technology, based on new choices and combined operators to produce two outflow technologies, that is, those that contain at least one OR connection traditional predictions - have been widely ignored in literature, as well as in the business systems described by the Tuples of FALSE flow. In this article, we presented a new technology called Bypass processing to evaluate this Bypass processing technology, to evaluate this Bypass processing technology, to evaluate by Bypass processing technology, to evaluate by Bypass processing technology, to evaluate by Bypass processing technology, to evaluate by Bypass processing technology, to evaluate by Bypass processing technology, to evaluate by Bypass processing technology, to evaluate by Bypass processing technology, to evaluate
In the current warehouse system, warehouse administrators are granted full authority to grant user permits. She is confident to only grant permits that meet the basic data sources' security requirements, but there are no tools to determine what these needs may be, or detect changes. We think that trust is rarely a good idea and often impossible. In the previous paper, we have proposed a different strategy where source permits are used to automatically produce the corresponding warehouse permits. In this paper, we combine the technology, identifying the missing concepts from the standard SQL permits, and showing how to expand the synthesis of SQL permits, including them.
The paper shows how modern architecture can be used to accelerate ranking algorithms. in the paper "On optimization rates and covering in ranking additional search results", the authors study a new ranking issue. instead of ranking individual projects, they consider the ranking combination of projects, for example, one hotel and two restaurants. they study the sequence and query processing algorithms in this context.
The description of the new index technology is a common result of database research, but these descriptions are sometimes troubled by poor approaches and lack of comparison with other programs.In this article, we describe the introduction and comparison framework of the index program that we believe set the minimum standards for the development and dissemination of research results in the field.
In this article, a formal model is the domain of Internet search, which allows it to quantify the relationship of important parameters of the distribution of the search architecture. These are the physical network parameters, the query frequency, the required monetary search results, the shift of data for search, the logical network topology, and the total bandwidth consumption to answer a query. The model then is used to calculate the various parameters between many important relationships. The results can be used to quantify, order, and optimize the distribution of the Internet search architecture.
For the relationship database of the digital water mark appeared a candidate solution to provide copyright protection, detectors, tracking frauders,ining the integrity of the relationship data. Many water mark technologies are proposed in literature to address these purposes. In this article, we investigate the current state and according to their intentions, they express the way water mark, covering the type, the particle level, and their validity.
The narrow division analysis (similar to the "division" question submitted by Imelinski, et al. [9]) is the extract of similar cell characteristics couples, related to big changes in data division cells. cells are considered to be similar if they are rotated, divided, or 1D mutation operations related. the narrow division queries are expressive, able to catch the trends in the data and answer the "if" question. To facilitate our discussion, we call for a cell in one dividing dividing dividing cells and another dividing cells. an effective algorithm is developed to promote the limitation of the depth of the calculation process, finding all dividing cells in one dividing cells dividing cells in one dividing cells and dividing cells between cells.
W3C XQuery language recommendation, based on a series and order document model, supports a variety of structures and use cases. There is a diverse method and strategy to evaluate the XQuery document relationship, in many cases only processing the language limited subgroup of information. In this article, we describe a implementation method to deal with XQuery with voluntarily evaluated FLWR expression, element builders and built-in features (including structural evaluation). Our recommendation map a XQuery expressed to a single equivalent SQL query using a new dynamic interval encoded XML document relationship, increasing the information with query related information.
From a standard data exchange format to the Web, it has become a tool for the development of e-commerce applications and online information services, and has attracted many standardized efforts for all types of applications. Documents are not only used to represent multimedia information content, but are used for many other purposes, such as representing meta information and specification of component interfaces, protocols and processes.
In a variety of applications, we need to keep track of the development of a data set over time. Forining and querying these multiversion data efficiently, external storage structures are an absolute necessity. We propose a multiversion B-tree that supports insertions and deletions of data items at the current version and range queries and exact match queries for any version, current or past. Our multiversion B-tree is asymptotically optimal in the sense that the time and space boundaries are asymptotically the same as those of the (single-version) B-tree in the worst case.
Blood transplant forms an important and irreplaceable component of the medical management of many diseases. From volunteers, non-compensing blood donors from a low-risk population is an important measure to ensure the availability and safety of blood transplant. In a country such as Uttarakhand, during the parade season, the group of visitors are visited, in natural disasters and accidents, the availability of blood is crucial.
The materialized view of the database is continuous, due to efficiency, to introduce users to the latest updates. These views are used by many warehouses readers (users) to perform OLAP queries by running multiple readers meetings, these views areined regularly byining these views. Therefore, there is an internal problem that keeps these views and readers meetings continue to obtain consistent data from these views. In this article, we discuss a method that allows warehousesining transactions and readers meetings to run simultaneously.
The Distributed System Technology Center (DSTC) worksflow specifications, verification and management of the framework capture workflow similar to transaction behavior, a long-term process. FlowBack is an advanced prototype that enhances existing workflow management systems by providing post-response processes. It is based on a wide range of theoretical research, its construction and construction assumptions are product-independent. FlowBack clearly shows the degree of general post-response can be automated and system supported.
The paper focuses on the dynamic aspect of the IF02 concept model, which is an extension of the Semantic IF0 model defined by S. Abite boul and R. Hull, its primary aspect is the "full event" method, the architects use the expression event portfolio, as well as the designers work optimize the specification of the modularity and availability.
In a big data storage setting environment, it is often favorable to provide quick, close-range answers to complex decision support queries, using pre-calculation summary statistics, such as samples. decision support queries regular group data, then integrate each group of information (group according to queries). according to data, there can be a wide range of differences in each group of data items. as a result, close-range answers, based on unified random sample data, and without having to evaluate in this technique, we can get from a very few data projects, as these groups will be represented in the samples very few (usually zero) double. In this article, we propose a general technical category to get fast, very accurate answers to the colour list of data on these meetings.
With regard to the specific requirements for the advanced OODB application, the index data structure in OODBMS must provide effective support for multiple queries and must allow the index to be optimized to a specific query file. We describe the multi-critical type index and an effective implementation of this index scheme. It meets two requirements: in addition to its multi-critical query capacity it is designed for two standard design alternatives, key group and type group. A multi-critical type index is a linear algorithm, the map type algorithm is a linear command character domain so that each subtitle is represented by the interval of this domain.
In this work, we study alternative technologies to implement object-oriented database systems, and in query processing, that is, to use them as much as possible in the evaluation function portfolio.We will provide a comprehensive overview and performance assessment of all known technologies simple (single evaluation) as well as multi-evaluation function portfolio.In addition, we will describe the special command storage/function portfolio technology, especially attractive decision support query requires order results.While most of this paper is focused on object-oriented and object-oriented database systems, some results can also be applied to the database balance as they are a single-evaluation function portfolio.In addition, we will describe the special command storage/function portfolio technology with special support for query requires.
As the database is widespreadly deployed, it becomes more and more important, reducing the top of data management. An important aspect of data management, key impact performance is the ability to choose the data base indicators, estimate the changes in the cost of the workload, and study the right indicators of the database, which is the key database administrator (DBA) capable of quantitative analysis of the existing indicators. In addition, the DBA should be able to submit the assumptions of the indicators ("What- If") and quantitative analysis of the performance of their impact system. This impact analysis may include the analysis of the workload database, estimate the change in the cost of the workload, and the study of the indicators of the use, while taking into account the changes in the project in the data base size. In this document, we will analyze a new practical indic
Applications such as multimedia obtaining need effective support for similarity search in big data collection. However, the closest neighbor search is a difficult problem in large-scale space, making effective application difficult to realize: the index structure quickly falls, with large-scale increases, while continuous search is not an attractive solution for the storage library with millions of objects. This paper is close to this problem from different perspectives. A solution is to find in an unusual storage program, opening a new technology for processing k-NN queries, especially suitable for high-scale space. The recommended (physical) database design is suitable for a new variation branch and related search, reducing the high-scale space quickly to a small candidate.
The process of recurrence was studied in the apis mellifera colony (Apis mellifera L.) in emergency. mother women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women women
Global classification in the field of space database systems is rarely studied, although significant performance improvements can be achieved by using the appropriate technology. In this paper, we propose a simple approach to the global classification called the classification organization. We will prove that this classification organization leads to significant performance improvements, no algorithm exceeds. Based on real geographical data, we do a detailed experimental performance assessment and will classify the organization with other organization models not using the global classification. We will show that the global classification accelerates the processing of window queries and space connections, without reducing the input of new objects and selective queries performance, such as point queries.
Object-oriented database provides a wealth of structural capabilities for the organization of objects to apply to an application. Due to the possible complexity of the object structure, route expression has been accepted as a short synthetic means of object reference. Although known route expression methods provide quite elegant object access, it still seems to need to expand the availability of route expression.
This paper introduces MICP, a new multi-version integrated cache replacement and forecast algorithm designed for effective cache and transaction management in the mixed data transmission network.MICP takes into account the dynamic and order of change in cost/benefit ratio of cache and/or disclosure of the object version, by making cache replacement and forecast sensitive decisions to the possibility of access to the object, their location in the broadcast cycle, and their update frequency.
In this article, we explore the implementation of a pipe mixture of a multi-processor database system. In order to improve the implementation of the query, about the query implementation of the innovative methods proposed to use the division of the deep right tree, which are the deep right tree bubble. First, we get an analytical model to perform the pipe part, then, under the light of the model, developed the division-based deep right tree query implementation plan so that the query can be effectively implemented. As our simulation shows, the recommended method, without causing additional top implementation, has greater flexibility in the query plan and leads to the query plan performance better than the previous correct query plan.
The processing of user preferences is becoming an increasingly important problem in today’s information systems. In which, preferences are used for information filtering and extracting to reduce the amount of data submitted to the user. They are also used to track the user’s personal data and develop policies to improve and automatic decision-making. We presented here a simple logical framework to develop preferences as a preference formula. The framework will not impose any limitations on preference relationship and will allow voluntary operations and forecast operations in preference formula. It also makes its preference relationship composition simple. We presented a simple, natural composition of preference formula to the relative algorithm (and SQL) through a single operator parameter to preference formula. We presented a simple, logical framework to facilitate preference formula, preference formula, preference formula, prefer
All manufacturing companies need to control the flow of materials from suppliers, through value added processes and distribution channels, to customers. Supply chain is a series of activities related to planning, coordinating and controlling materials, parts and completing goods from suppliers to customers. Traditionally, the flow of materials is only considered on the operating level. However, it is no longer possible to ignore the potential of integrating the supply chain.
A series of mass-scale high-energy physics experiments take place on the horizon, some of which will produce a lot of a fifth of the scientific data each year.A variety of exploration projects are conducting the study of data management methods within the physical computing community.There is a controversial view of this big data issue: (1) there are too many data effectively managed within a real database; (2) there are too many data effectively managed without a real database; many people keep two perspectives.This paper aims to start calculating the dialogue between the physics and the very large database community, and simulate the direction of the study, this paper will try to highlight the scope and scope of these big data issues, several methods of investigation are effectively managed by physics in a real database; many people keep the two perspectives of this paper is
We live in an exciting moment in the field of clinical research and the changes that occur will change the way health care is provided and the work of clinical research professionals will guide these changes.
Although XML Stylesheet Language for Transformations (XSLT) is not designed as a query language, it is suitable for many query similar operations in XML files, including selection and reorganization of data. In addition, it actively performs a XML query language in modern applications and broadly supports application platform software. However, using database technology to optimize and implement XSLT only recently in the research community attention. In this article, we focus on XSL conversion will be carried out on the XML file, defined as a relative database view. For XSLT subgroup, we introduce an algorithm to form a conversion with the XML view, eliminating the implementation of XSLT needs and then we describe how to expand this algorithm to optimize and implement several additional features of XSLT.
Since multi-dimensional sequence is a natural data structure that supports multi-dimensional queries, while the object relationship (O/R) database system supports multi-dimensional sequence ADT (abstract data type), this is a natural problem if multi-dimensional sequence ADT can be used to improve the performance of multi-dimensional sequence DBMS. As a response to this issue, we have implemented a multi-dimensional sequence in Paradise O/R DBMS. In this article, we describe the implementation of this compressed sequence ADT and explore its performance queries including star combinations and options.
Given the combination of S objects, a space time query q receives the object, will be in the (future) interval of qT. the nearby neighbor query q receives the object of q. Given a boundary of d, a space time query q receives two data sets of objects, we will enter the distance of d to each other in qT. In this article, we introduce the probability cost model, estimate the space time query and connectivity, as well as the expected distance between a query and its nearest neighbor (s).
Assessing the performance of the database system often requires to produce a synthetic database - they have certain statistical characteristics, but are filled with stupid information. when evaluating the different database design, it is often necessary to produce multiple databases and to evaluate each design. the database size increases to a trabit, generating often takes longer than evaluating.
XQuery is not only used in the database to query XML, but also applies to applications that need to process XML files as a file or process. These applications are subject to the current major XQuery processors limitations, these processors are broken a fairly small file. In this paper, we propose the technology, based on a concept of projection as XML, can be used to significantly reduce memory requirements in the XQuery processor. The main contribution of this paper is static analytics technology, which can be determined in compilation, which parts of the input file needs to respond to a voluntary XQuery. We introduce a load algorithm that requires the result information to build a project file, which is smaller than the original file, and in which the query produces the same results.
Discover the rules of the association is an important database mining problem. The current algorithm search for the rules of the association requires multiple transition analysis of the database, obviously, the role of I/O is very important. We introduce new algorithms, reducing the database activity significantly. The idea is to choose a random sample, using this sample to find all the rules of the association, which may be kept throughout the database and then check the results with the rest of the database.
This article discusses the performance of the distributed database system. in particular, we introduce an algorithm to dynamically copy an object in the distributed system. the algorithm is adapted as it changes the copy of the object, i.e., the combination of the processor, the copy of the object) because the change occurs in the reading of the father (i.e., the number of reading and writing issued by each processor). the algorithm continuously moves the copy of the object to the best one.
A basic concept of these systems is the object identity (OID), as each object in the database has a unique detector to access and refer it to the relationship with other objects.The two methods can be used to implement the OID: physical or logical OID.To manage the complex data, the multimedia data manager core (NuGeM) is proposed, using a logical technology called indirect map.This paper proposes an improved technology used by NuGeM, its original contribution is to manage the OID with minor disk access and processing, thereby reducing the management of the page time and eliminating the problem of the OID.
Welcome to the first article in the new SIGMOD record: Discern database files, containing data base community columns, I am not a traveler, but I plan to leave my nose in the Middle East of Illinois, to offer you a new interview, in June I went to California to participate in a pension-related festival with Stanford University Professor Gio Wiederhold.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database
Clean the data structure and content errors are important to data storage and integration. The current data cleaning solutions involve many iterations of data “audit” to find errors and long conversions to correct them. The user needs to suffer long waiting and often write complex conversion scripts. We introduce the Porter wheel, an interactive data cleaning system, closely integrated conversion and discrepancy detection. The user gradually build the conversion to clean the data, by adding or leaving the conversion on a broadboard similar interface; the conversion effect once displayed on the screen visible records. These conversions are through simple graphic operations, or by displaying the required effect on the data background, the special wheel conversion automatic system can be used according to the data structure of the user.
The application of the traditional database model in the important fields of document receipt and office information systems has not yet given great evidence of success. here we introduce an alternative model based on serial theory. this model seems better suitable for other types of information systems, while it still applies to traditional database operations.
Decision support systems (DSS) and data storage work loads make up a growing part of the database market today. I/O capacity and DSS work loads related processing requirements are rapidly increasing, approximately twice every 9 to 12 months. In response to this growing storage and calculation demand, we introduced a computer architecture that supports the database server using a "smart" disk (IDISK). IDISK uses low-cost built-in general use processing, main memory and high-speed serial communication links on each disk. IDISK connects each other through these series links and high-speed exchangers, beyond the I/O bottle system.
LEO is a comprehensive way to correct the errors in statistics and cardinal estimates of the query implementation program. LEO introduces a feedback cycle to query optimization, improving the available information in which most query occurs, allowing the optimizer to actually learn from its past mistakes. We show the LEO how to learn old tables access statistics in a TPC-H database chart and show the LEO improves the tables cardinal estimates as well as the individual forecast filter factors. therefore, the LEO allows the query optimizer to choose a better query implementation program, leading to more effective query processing.
This article describes the critical deletion algorithms of the B+ Tree. There are published algorithms and passwords to search and insert the key, but due to its greater complexity and fewer perception of the importance of deletion is complete or left to the reader as a practice. To solve this situation, we provide a thorough documentation of the flow maps, algorithms and passwords deletion, their relationship to search and insert algorithms, as well as a reference to a free-to-use, complete B+ Tree Library written in programming language C.
In the early 1980s, the researchers realized that similarity information stored in the database as a integrity limit can be used for query optimization. Developed a series of technologies called similarity query optimization (SQO). Some ideas developed for SQO have been used commercially, but in our best knowledge, there is currently no widespread SQO implementation. In this paper, we describe the implementation of two SQO technologies, forecast introduction and adding removal, in the DB2 universal database. We presented the algorithms and performance results implemented using TPCD and APB-1 OLAP parameters. Our experiments show that SQO can lead to dramatic query performance improvement. A key aspect of our SQO implementation, in fact, it does not constitute a complex SQO technology, forecast introduction and adding
The paper takes into account the views of self-maintaining issues where the views are kept without the use of all the databases. However, without full use of the databases, keeping a point of view is undoubtedly not always possible. Therefore, it is necessary to solve two key issues is to determine, in a particular case, a point of view isined, and how to keep it. W provides e algorithm to answer these questions as a general point of view category, and for an important subcategory, the SQL question is to test whether a point of view is customizable and up-to-date point of view, if it can be solved by a clear point of view.
Currently, the majority of data accessed on large servers are structured data stored in traditional databases. the network is based on LAN and client range from simple terminals to powerful workstations. the user is a business and application developer is a MIS professional. by introducing broadband communications to the home and better 100 to 1 compression technology, a new form of network based on computing is emerging. the structured data is still important, but the accumulation of data becomes non-structured: client service: audio, video, news transmission, etc. the dominant user becomes a consumer. the client device becomes a TV setting. the application developer becomes an editor, director or video manufacturer. the media server supports all kinds of general network data storage network server can be converted and converted into complex data, but it remains
We showed the design and early prototype of IrisNet (Internet Scale Resource Intense Sensor Network Services), a common extensible network infrastructure used to deploy a wide range of regional sensor services.IrisNet is a potential global network of smart sensor nodes, with network cameras or other monitoring devices, and organize nodes, providing the means of finding the latest and historical sensor data.IrisNet uses the fact that high-capacity sensor nodes are usually connected to a device with significant calculation power and storage, and operate a standard operating system.It uses aggressive filters, intelligent query routes and grammatical queries to significantly reduce the network width of use and improve query response time, we show our presentation will provide two services, built on the application IrisNet, two very different sensor infrastructures from the
The arrival of the World Wide Web created a explosion in available online information. With the range of potential options expanded, time and effort needed to classify through them also expanded. We recommend a formal framework expression and combining user preferences to solve this problem. the preferences can be used to focus search queries and order search results. a preference is expressed by the user entity, described as a set of names of fields; each field can be taken value from a specific type. the symbols can be used to match any type of element. a preference can be combined with the use of a general portfolio operator, instantly with a value feature, thereby providing a great deal of flexibility. the same preference can be combined in a variety of ways and another preference combination of preferences, therefore each type of character can be taken from a specific type of value.
The paper describes the security issues of the federal database management system, which is used to manage distributed, abnormal and independent multi-layer databases, which is based on our previous work on multi-layer security distribution databases management systems, as well as the results of the work of others in the federal database systems.
Xmas is an extensible main storage system for high-performance built-in database applications. Xmas not only provides the core features of DBMS, such as data durability, accident recovery and competitiveness control, but also pursues an extensible architecture to meet the requirements of different applications fields. One key aspect of this extensibility is that an application developer can make up the application specific advanced operations with the basic operations provided by the system. In Xmas, the so-called complex operations, these operations are processed by a customized Xmas server, with the minimum interaction with the application process, thereby improving the overall performance.
For a large-scale data-intense environment, such as the World Wide Web or data storage, we often do local copies of remote data sources. Because of the network and computing resources are limited, however, it is often difficult to monitor the source continuously check the changes and download the changes of data projects to copies. In this case, our goal is to detect as many changes as possible, we can use fixed download resources we have. In this article, we propose three sample-based download policies that can effectively identify more changes of data projects. In our sample-based method, we first sample a small amount of data projects from each data source and download more data projects from the source more varied samples.
The personalization of electronic services brings new challenges to the database technology, requiring a strong and flexible simulation technology complex preferences. Preference queries must be shared by dealing with preferences as a soft limit, trying to match the best possible. We proposed a strict part of order of preferences, which is closely in accordance with people's intuition. A variety of natural and complex preferences are covered by this model. We show how to trigger the construction of complex preferences, through a variety of preferences manufacturers. This model is the key to a new discipline called preference engineering and preference algorithms. Considering the best competition (BMO) model, we study the complex preferences can be defined as preferences, for simple models and complex preferences, we can support this model through this model.
The limitation theory for the traditional structured database development is no longer applicable to XML data, so many efforts are focused on the key limitations of XML. In the paper, we present a multi-time-based key concept that is essential for an efficient change-detection algorithm due to the problems faced by the assessment of changes in XML documents.

The paper describes the ACM Multimedia '94 Conference Seminar on the Multimedia Database Management System held in San Francisco, California on October 21, 1994. the seminar consists of four conferences: the design of the Multimedia Database Management Systems, video and continuous media services, the Multimedia Storage and Access Management, and the errors of the Multimedia Data Management. the seminar ended the discussion conference on the direction of the Multimedia Database Management. the seminar was attended by twenty-eight participants from the United States, the UK, Germany, Norway and Egypt.
This article describes the work we are doing to develop a Stanford Stream Data Manager (STREAM), a system to perform multiple continuous data flows queries.STREAM system supports a statement queries language, it deals with high data rates and queries work burden, providing close-range answers when resources are limited.
The title of a book Economics cannot more importantly mention the existing five lines of economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics, it always has three lines of direct economics.
Unfortunately, both methods are considered to be wrong, through a complex and accurate gen portfolio, through a complex and accurate gen portfolio, which is a basic problem, in many practical scenarios, including gen portfolio of gen portfolio, we propose a new method, gen portfolio of gen portfolio, using gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio of gen portfolio
While the standardization efforts of the XML query language are advancing, researchers and users are increasingly focused on the database technology, the database must provide the data management database with a rich new challenge: verification, performance assessment and optimization of each query of the XML query processor is an upcoming problem. After long-term database research, we provide a framework to evaluate the ability of a XML database to deal with a variety of different types of query, usually in the real world scenarios. comparison can help executives and users in a standardized application to compare the XML database. For this, we provide a set of query, each query aims to challenge a specific query processor aspect.

In the modern database applications, the similarity or differentiality of complex objects is assessed by far-based queries (DBQs) on high-size data. R tree and its variants are commonly referred multi-dimensional access methods that can be used to answer these queries. Although the related algorithms work well on low-size data space, their performance decreases as large-size queries increase (big-size queries are large-size queries). To obtain the most acceptable queries time (DBQs) in high-size data space (DBQs) queries (DBQs) queries (DBQs) queries (DBQs) queries (DBs) queries (DBs) queries (DBs) queries (DBs) queries (DBs) queries (DBs) queries
Current exchange studies are focused only on the identification aspects; not providing any local knowledge of exchange, we refer to a description or explanation of why an identified exchange is special. for many applications, a description or explanation is at least the importance of the user as the identity of exchange aspects. in particular, the intensity of knowledge helps the user: (i) evaluate the effectiveness of exchange, and (ii) improve a person’s understanding of data. in this article, the two main issues discussed are: what kind of intensity of knowledge is provided, and how to optimize such knowledge.
In this article, we presented four methods to provide a highly competitive B+ tree index in the background of the data transmission, client server OODBMS architecture. The first method performs all index operations on the server, while other methods support client encryption and use index pages different degrees. We have implemented four methods, as well as the 2PL method, on the background of the Wisconsin SHORE OODB system, we presented the experimental results of the performance study based on running SHORE on IBM SP2 multi-computer. Our results emphasize the need for non-2PL methods and show the gap between 2PL, no encryption and three encryption alternatives.
Object-oriented programming concepts and database integration are one of the most important advances in the evolution of the database system. Many aspects of this combination have been studied, but there are few models that provide security for this rich structured information. We develop an object-oriented database licensed model. This model is based on a set of policies, a structure of licensed rules, and algorithms to evaluate access requests against licensed rules. User access policy is based on the concept of hereditary licensed, along the class structure application.
A XML document can be accompanied by a document type descriptor (DTD) it plays a regular expression of full expression, naive methods usually do not produce specific and intuitive DTD. DTD contains valuable information about the structure of the document, so in effective storage of XML data, as well as effective formatting and optimization of XML queries. In this article, we propose XTRACT, a new system to complete a DTD pattern of the database of XML documents. Because DTD synthesis contains a regular expression of full expression power, naive methods usually do not produce specific and intuitive DTD.
Publisher Review This chapter implementes ATLAS, a powerful database language and system that allows users to develop a complete data-intensive application in the structured query language (SQL) - by writing new integrated and table features in SQL rather than in the programming language, such as the current object relationship system. Therefore, ATLAS' SQL is complete, very suitable for advanced data-intensive applications, such as data mining and traffic query. ATLAS system is now available to download, as well as a range of applications, including various data mining features, have been encoded in ATLAS's SQL and performed with the same application written in C/C++.
Publishing/Subscribe is a user expressing long-term interest paradigm (“Subscribe”) and some agents “Publish” events (e.g., an offer). Publishing/Subscribe software’s task is to send the event holders to meet these events’ subscriptions. For example, user subscriptions may include an interest in a type of aircraft, not exceeding a specific price. Publishing events may include an offer of aircraft with certain properties including price. Each subscription is predicted by a combination (properties, comparison operators, value).
We define the efficiency standards related to the database computing. Efficiency assessment is suitable as a processing framework for the query language, and includes the coverage, operational and efficient assessment aspects. We introduce the unit computing and discuss its effectiveness on the object-oriented query language, by ODMG-93. the unit computing is easily captured as a variety of collections of types, collections, type builders voluntarily composed and based on query expressions. We also show how to expand the unit computing to deal with vectors and arrays in a more expressive way than the current query language and explain how it deals with identity and updates.
Continuous data flow appears naturally, for example, in the large telecommunications and internet service providers install, where the detailed use of information (telephone details record, SNMP/RMON package data, etc.) from the different parts of the basic network requires constantly collecting and analyzing interesting trends. This environment causes a critical need for effective flow processing algorithms, which can provide (usually, about) data analysis query answers, while using only small space (housing short flow combination) and small processing time for each class of projects.
In the past few years, the number of complex documents and texts has grown unusual, requiring a deeper understanding of machine learning methods that can accurately classify texts in many applications. Many machine learning methods have achieved outcome in natural language processing. The success of these learning algorithms depends on their ability to understand the complex models and non-linear relationships in data. However, finding the appropriate structure, structure and technology of text classification is a challenge for researchers.
1. file acquisition There is a wide range of technology how to acquire files in this way, they are a computer readable form, which can be stored on the file base. This spectrum from completely automatic low cost through semi-automatic use tools, such as scanner and optical character identification (OCR) to manual acquisition, according to the prepared rules and regulations. High quality file acquisition is intended to catch the file's structure and grammar content, rather than as possible, but as possible. The current state of the art of semi-automatic paper acquisition is scanned, then by OCR. This provides a similar image and text content, but no structure and no real language. In many cases, the OCR produces the quality of the text language and the law must be revised in order to ensure that the structure and the content of the file are not possible, but in the library
Cost-based XML query optimization requires accurate assessment of the selectivity of the route expression. Some other interactive and Internet applications can also benefit from these estimates. Although literature suggests some estimate of the route frequency distribution, there is almost no guarantee of the route accurate estimates within a specific space range. In addition, most people assume that XML data is more or less quiet, that is, there is a little update. In this article, we introduce a framework of the XML route selective estimates in a dynamic background.
The wood removal pattern forms a natural basis of the CD supplementary pattern of the existing technology to question the wood structured data, such as XML and LDAP. Because the efficiency of the wood structured pattern corresponds to the efficiency of the wood structured databases depends on the size of the pattern, it is essential to identify and eliminate the dividing nodes in the pattern, and as soon as possible. In this article, we study the wood structured pattern can reduce the results, both in the absence and the existence of the AC standard dividing nodes, we can use the AC standard dividing nodes; through the AC standard dividing nodes; through the AC standard dividing nodes; through the AC standard dividing nodes; through the AC standard dividing nodes; through the AC standard dividing nodes; through the AC standard
The Fourth International Conference on Flexible Questionnaire Response Systems (FQAS2000) was held on 25-27 October 2000 at the Warsaw Academy of Sciences in Poland.The series was launched in 1994 by Troels Andreasen, Henning Christiansen and Henrik Larsen from the University of Roskilde.
We present a structured, user-centered design and evaluation method of VE user interaction. We recommend implementing (1) user task analysis, then (2) based on expert guidance assessment, (3) user-centered training assessment, and finally (4) comparative assessment. In this article, we first provide motivation and background for our method and then we describe in detail each technique.
Previous research functional connections are limited in two ways: (1) We know all methods assumption reference is implemented as a physical object detector (OID) and (2) most methods, in addition, are limited to supporting only one-value reference properties. both are serious limitations as most object relationships and all object-oriented database systems support based on based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based based
A brief overview of the Nonstop SQL product will be described by a different ServerWare SQL. The current Nonstop SQL optimizer uses a traditional underlying dynamic programming optimizer.
Recently, the concept of self-similarity has been shown to apply to broad-regional and local network traffic. In this article, we reviewed the self-similarity mechanisms that cause network traffic. We proposed a assumption to explain possible self-similarity traffic, using a specific regional traffic subgroup: traffic due to the World Wide Network (WW). using a wide sphere of actual users performed by NCSA Mosaic, which reflects more than half a million requests for the WWW file, we reviewed the structure of WWW traffic. Although our measurements are not final, we show the evidence that WWW traffic shows behavior is consistent with the self-similarity model.
The March 2005 edition of TODS contains eight papers from the SIGMOD and PODS 2003 conferences, which are a significant extended version of the conference paper, allowing authors to improve and write without a 12-page limit.
The SIT-IN (Integrated Territorial Information System) system integrates a historic database that provides information about the temporary progress of the territorial administrative division; the Institute’s GIS, which provides Italian territory maps to the detailed level of the review route; the statistical database, which provides from a series of review space time data; and an address normalization/geological matching system, which provides information about the review route boundaries (for example, the part of the street or the side of the city square).
The purposes of graph analysis and classification activities can be different: from graph analysis and classification activities the purposes can be different: from graph analysis and classification activities the purposes can be different: from graph analysis and classification activities the purposes can be different: from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis, from graph analysis
Many decision-making support systems, using the association rules to find interesting patterns, need to find the association rules, over time, these rules describe the complex time patterns, such as the events occur at "the first working day of each month". In this article, we study the question of finding how the association rules change over time. In particular, we introduce the use of a calendar algorithm to describe the complex time phenomenon interests of the user. We then introduce the algorithm to find the association rules, which are the association rules, follow the calendar expressions patterns provided by the user. We propose a variety of optimizations to accelerate the discovery of the calendar rules.
DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool for a particular work load, automatically recommending physical design features, these features are indexed by any subgroup, materialized query tables (also known as materialized query tables), shared database divisions, and multi-dimensional group tables. Our work is the first industrial intensity tool, covering the design of four different features, a significant progress to the existing tools, supporting not only the index and materialized views. Building such tools is challenging because not only the interaction between the features introduced by the large search space, but also requires expansion to support the additional features of the tool in the future demand, we can use a "new designer" to re-adjust our tools
Electronic services are the business features provided by the service provider through the Internet and accessible to the customer, can be human users or software applications.The main benefits of the electronic service environment are that customers are able to dynamically find the most suitable available electronic services, check their properties and capabilities, and determine whether and how to access it.But, in order to provide electronic services to the customer, the service provider faces a variety of challenges.
The simplest type of query these data are those that are described by the path of the conventional path expressions. The more complex query combines several conventional path expressions, the complex data reorganization, and the lower query. This article solves the problem of effective query assessment, the distribution, the semi-structure of the database. In our settings, the databases nodes are effectively distributed in a fixed number of sites, and the margins are classified as local (there are two terminals in the same location) and cross-border (there are two terminals in two different locations). The effective query in this context means that the number of communication steps is fixed (independent of data or query and query), the total amount of data depends on the individual case query query query query query query query query
This article discusses the challenges of database management in the Internet of Things. We provide the scenarios to describe the new world will be produced by the Internet of Things, where the physical objects are fully integrated into the information highway. We discuss different types of data will become part of the Internet of Things. These include identification, location, history and description of data. We consider the challenges by the need to manage a lot of data in the unusual systems.
The data on the Internet is increasingly presented in the XML format, which makes the new application submitting queries about "all XML data on the Internet". queries XML data using route expression, through the structure of the data navigation, and optimizing these queries need to estimate the selectivity of these route expression. In this article, we presented two techniques to estimate the simple XML route expression selectivity, beyond the complex large-scale XML data, will be processed by the Internet scale application: the route tree and the Markov table. The two techniques work, summarizing the structure of the XML data in a small amount of memory and using this summary of the selective estimates.
A active eXtensible Markup Language (AXML) document content is dynamic as it can specify when a service call should be activated (e.g., if necessary, per hour, etc.), and how long its results should be considered effective. therefore, this simple mechanism allows to capture and combine different data integration styles, such as storage and mediation. To make full use of the service, AXML also allows to call continuous service (providing response flow) and service support intentional data (AXML document includes service calls) as a parameter and/or a result of a powerful, repeated integration program.
This term is also used to mean a perfect view of things coming. here, we focus on a speculative visual VLDB in 2020. this panel is tracking me organized (with S. Navathe) in Kyoto VLDB in 1986, titled: "Who for VLDB in 2000? on this panel, members discussed significant progress in the database field and imagine its future, after many researchers worry, the database field is running interesting research topics, so it may disappear in other research topics such as software engineering, operating systems and distribution systems.
Over the past few years, several works of literature have solved the issue of extracting data from the webpage.The importance of this issue arises from the fact that once extracted, the data extracted from the webpage.
Edit comments: Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments
PM3 is the vertical permanent extension of the Modula-3 system programming language, supporting vertical permanent availability from the so-called vertical root. We describe the PM3 prototype design and implementation, and show its performance with its non-vertical opponents, by direct comparison with the language of the SHORE/C++ on the SHORE object library. Experimental results, using the OO7 reference mark cross-parts, reveal that vertical permanent ultrasound is essentially not more expensive than non-vertical permanent, and proves our statement that vertical permanent is acceptable level with the current appearance of automatic memory management (i.e. even "collection"), in performance settings, will make vertical permanent ultrasound more flexible than non-vertical permanent system.
LeSelect is a mediator system that allows scientists to publish their resources (data and programs) so that they can access transparently. Scientists can usually send queries, these queries perform techniques, by fully exploiting the distributed mediator system structure and involving the execution of expensive functions (compatible with the program). In addition, queries may involve large objects, such as images (such as the archive of meteorological satellite data). In this context, the transfer of large objects costs, and cited expensive functions are the control of the execution of the time. In this article, we first propose three queries performing techniques to reduce these costs by fully exploiting the distributed mediator system structure, such as LeSelect and then we distribute strategies that include expensive functions.
We recommend expansion of the database system through Skyline operations.This operation filters a series of interesting points from the potential big data points. one point is interesting if it is not dominated by any other point. for example, if no other hotel is cheaper and closer to the beach, the hotel may be interesting for someone.We show how SSL can expand to submit Skyline queries, introduce and evaluate the alternative algorithms of Skyline operations, and shows how to combine this operation with other database operations, for example, join.
Considering a set of a database speed and reliability can make the difference between prosperity and destruction. information systems are no objects. data must be accessible from many points around the world with secondary reactions. the financial industry is exactly the environment. this tutorial introduces the configuration, regulation and allocation of case studies extracted from financial applications.
This article introduces a declaration language called Updated Computing, Relationship Database Updates. Updated Computing formulas involve the current database conditions, as well as statements about a new database. Logical connectors and quantum transforms into complex updates, providing flexible database transformation specifications. Updated Computing can express all undefined database transformations, which are multi-time.
Publisher’s Overview The disintegrated storage model (DSM) has not been accepted by the database provider. Given the technical trends and the needs of storage architecture, they are better aware of the disk hands and storage effects during the query processing, the DSM is likely to play an important role in the future. Based on some basic assumptions of the current database system has occurred significant changes in the past decade. CPU speed is improving rapidly, the available main memory is also increasing. Although the disk capacity also shows similar improvements, the disk time and effective transmission speed (transmission speed/capacity) have been improved at slower speed (about 10 slow factors).
In the past decade, research and development in the field of database technology has been pursued by the pursuit of better support, beyond the traditional world of applications, which are mainly high amounts of simple structural data must be effectively dealt with. Therefore, the future DBMS will include more features and clearly cover more reality worldology (in different forms), otherwise should be included in the application itself. Advanced database technology, however, in a sense is double. Although it provides new and highly needed solutions in many important fields, these solutions often need to be carefully considered to avoid introducing new problems.
This paper studies the Candy model, which is the iconic point process launched by Stoica et al. (2000). we prove Ruelle and local stability, study its Markov characteristics, and discuss how the sample model.
The appearance of Sustainable Memory (PM) in the database system components is re-designed to get full use of durability and hardware speed. The researchers study a key component is the sustainable indicator. However, these studies have so far been unsatisfied, in the quantity of expensive PM writing requires accident continuity. In this article, we propose a new sustainable indicator called DPTree (Deferential Persistent Tree) to solve this. The core idea of DPTree is to make multiple writings in DRAM full use and then integrate them into a PM component to reduce durability. DPTree contains several techniques and algorithms to accident continuity, significantly reducing PM writing andining excellence in this paper, we propose a new sustainable indicator called DPTree (Deferential Persistent T
Management of transactions in a real-time distributed computing system is not easy because it has an abnormal network computer to solve a problem. If transactions are running on certain different sites, it may be done on certain sites and may fail on another site, resulting in unconformity of transactions. The complexity increases in real-time applications, through the response time in the database system and the time of transaction processing. This system needs to process transactions before the end of these periods. A series of simulation studies have been carried out to analyze performance in different transaction management conditions, such as different work loads, distribution methods, implementation pattern distribution and parallel, etc. The data access arrangement is carried out to meet their duration and reduce the number of transactions. The system needs to process these durations before the end of a series of
Similar models are accepted by their simplicity and eIegance. On the other hand, similar models cause problems, the majority of similar types of structures are not represented as a simple relationship. Similar and abnormal structures belong to these structures not properly supported by simple similar models. In this article, we provide a overview of the model of flexible relationships, allowing the model and processing the voluntary abnormal structures, while retaining the similar philosophy of operations with a single constructor. Similar relationships support the model and the operations of the similar structures are incredible, our model really helps to further the gap between the similar and operating data models.
The requirements for the broad regional distribution of the database system are significantly different from the requirements for the local regional network system. In the broad regional network (WAN) configuration, a single site is usually to different system administrators, there are different access and charging algorithms, the site is installed specific data type expansion, and there are different restrictions on service remote requests. The typical last point is the production of trading environment, fully engaged in normal working hours and cannot bear additional burden. Finally, there may be many sites involved in a WAN distributed DBMS. In this world, a single program performs global query optimization, using cost-based optimizer will not work.
The second international seminar on health data management aims to bring together interdisciplinary audiences to discuss the challenges of health data management and provide new solutions for the next generation of data-oriented health care systems.
More and more individuals, companies and organizations rely on the cloud storage and management of their data, which will be converted into increased pressure on the cloud infrastructure. cloud data can be very diverse, including a variety of personal data collection, a very large multimedia content library and a very large data set. users and application developers can be very high, with a few DBMS expertise. data-intense applications can also be very diverse, requiring from basic database capabilities to complex analysis of big data.
Central to any XML query language is a path language, such as XPath, running in the wooden structure of XML documents. We prove in this article that the wooden structure can be effectively compressed and manipulated, using the technology produced from the symbol model check. Specifically, we first show that the short representation of the underground tree structure of the document is very effective. Second, we show that the compressed structure can be directly and effectively query, through a process of manipulation of the choice of the nodes and partial decompression. We study the the theory and experimental characteristics of this technology and provide the algorithm query our compressed examples using the nodes to choose the path query language, such as XPath. We believe that the ability to store and process the majority of the structure of the document is very important in the XML memory and the
In this article, we introduce a comparative non-parameter efficiency assessment calculation technology to close to the selective queries, especially the range queries. Unlike previous studies, our comparative focus is on calculating the selective properties that occur with the large spaces, such as in space and time databases. We also assume that only small samples of the required relationship can be used to estimate the selective data. In addition to popular histogram estimators, our comparison includes most of the so-called core estimation methods. Although these methods have been proven to be the most accurate parameters estimators in statistics, they have not been considered the most accurate parameters estimators, so until now.
Every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time, every time,
QBIC (Query By Image Content) is a prototype image collection software developed at IBM Almaden Research Center. It allows users to query images to collect using image content features - color, structure, shape, location and image objects layout. For example, users can query images with a green background, containing a round red object on the top of the left.
Unusual detection is an important challenge, such as error diagnosis and infiltration detection in the energy-restricted wireless sensor network, one of the key issues is how to reduce the communication top in the network at the same time, when the detection of unusual, our method is based on a formula, using a quarter of the field supporting vector machine in the data to identify unusual measurements.We use the sensor data from the Grand Duke project to prove that our distribution method is energy efficiency in the top of communication, while achieving similar accuracy to a centralized plan.
The author claims that the model will include the IFO data model [Codd 70], the entity relationship model [Chen 76], the functional data model [Kerschberg 76] and almost all structured data models information models support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Ham
Recently, several query languages have been proposed to query information sources of customers, their data is not limited to a chart, or their chart is unknown. Examples include: LOREL (query data from multiple abnormal sources combined), W3QS (query world wide network); and UnQL (query non-structured data). the data base of the natural data model for these languages is an effective, label chart. Their main innovation is to be able to express query, from a chart, or its chart is unknown. These queries, but may be difficult to evaluate in cases when the data is distributed in multiple abnormal sources, there are many margins between the sites. The result of a case is a site query, a site query, a site query, a site query, a site query, a
This paper introduces Crescando: an extensive, distributed relationship table implementation, designed to perform a large number of queries and updates with guarantees of access delays and data freshness. For this purpose, Crescando adopts a series of modern queries processing technologies and hardware trends. In particular, Crescando is based on parallel, collaborative scanning of the main memory and the so-called "queries data" adds to the known data flow processing. Although the proposed method is not always the best for a specific work load, it provides all work load with a delay and freshness guarantee.
In this article, we consider the phases of filtering the problem of spatial merger, for no input is indexed, we introduced a new algorithm that can be expanded based on spatial merger (SSSJ), achieving the effectiveness of real-time data and the intensity of high and worst data sets. The algorithm combines a method with the theoretically best I / O transfer boundaries, based on the recently proposed distribution - merger technology with the implementation of high-optimized internal memory flight merger. We introduce the effective implementation of the experimental results based on SSSJ algorithm and put it with the national software - part-based - space - space - space - space - space - space - space - space - space - space - space - space - space - space - space - space - space - space -
We consider finding the problem of associated rules, making the best double classification of the huge classification warehouse. The optimity of classification is defined by an objective function matching the target of the user. A objective function is usually defined in the meaning of allocating a specific target specificity. Our goal is to find the associated rules, divided into two subgroups, best suited to an objective function value. The problem is unreasonable to the general objective function, because making N a record of the number, a specific warehouse, there is 2N possible double classification, we may have to thoroughly check all. However, when the objective function is combined, there is a realisable algorithm to find the best double classification, we prove that the standard, such as the "objective function" value is unresolved.
The existing space selective assessment method does not accurately estimate a query of selectivity so that objects can be moved because they do not take into account the future moving objects' selective location, with time passing constantly changing. In this article, we proposed an effective method to make the space time selective assessment to solve this problem. We provide an analytical formula to accurately calculate a space time selective query as a space time information function. The extreme experimental results show that we proposed a precise method to make the real space selective assessment easier for the time passing constantly in this article, we proposed an effective method to make the space time selective assessment to solve this problem.
These are inspired by the traditional wavelet-based approximation, consisting of specific linear projections of basic data.We introduce the general methods based on slides to catch data’s various linear projections and use them to provide accurate and scale estimates of data flow.These methods use small space and every point of time, while through the data flow, and provide the accurate representation of our actual data flow experimental display.
The dynamic query interface (DQI) is a database access mechanism that provides users with continuous real-time feedback during query writing. Previous work shows that the DQIs is an elegant and powerful interface for small databases. Unfortunately, when applied to big databases, the previous DQI algorithms slow to a rotation. We introduce additional methods to the DQI algorithm and show updates working with big databases, both in theory and in practice.
For the purpose of applying this technology to a big database, it is necessary to develop a multi-dimensional index structure to support the nearest neighbor's queries with greater efficiency. SS tree has been proposed for this purpose and is known to exceed other index structures such as R* tree and K-D-B tree's non-image queries, and one of its most important features is that it uses this index in R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R/R
The development of the Sequoia 2000 program is based on the emerging geospatial standards to accelerate the development and promote data exchange. This paper focuses on the digital satellite image generic data program. We study how satellite generic data is defined, used andined. We discuss the geospatial standards we are using and describe a SQL prototype based on the space file and exchange format (SAIF) standard and implemented in the Illustra object relationship database.
Like the experiments carried out in the laboratory, the results of the electronic science in the silicon experiment are limited, if other scientists are unable to determine the origin or origin of these results. for the electronic science, we need to make a more systematic origin record on a range of electronic science activities and disciplines, as well as a more informed understanding of the information in these source data.
Many sites contain a large number of pages using a common template or layout. For example, Amazon in the same way lists the authors, titles, comments, etc. The values used to create a page (for example, the authors, titles, etc.) are usually from a database. In this article, we study the problem of automatically extracting the database values from the pages created by these templates, without any learning examples or other similar human entries. We formally define a template and submit a template, describing how to code the value to the page using the template.
Current research focuses on the use of foreign languages, or the mix of languages in advertising information on the Swedish consumer market.
Deciding the most suitable building is the most critical activity in the data warehouse life cycle. The building is the key factor in the data warehouse capacity and limitation settings. These articles were conducted (1) to better understand the selectivity factors affecting the data warehouse building and (2) the success of the different buildings. Academic and data warehouse literature and industry experts were used to identify the selectivity factors and successful measures of the building, and then to create problems for web-based surveys, used to collect data from many companies, their companies, their data warehouse, the building they use, and their building success.
In contrast to other interactive query interfaces, it combines (1) a pure icon specification, that is, no form of graph, only icon operations, with (2) strong browsing or metaquery tools, helping to develop a complete query without involving route specifications or accessing real data in the database.
Current browsers only obtain content from publicly indexed web pages, that is, the web collection is only by tracking hypertext links, ignoring the need for permission or pre-registration of search tables and pages. In particular, they ignore the huge number of "hidden" high-quality content behind the search tables, in large searchable electronic databases. In this article, we solved the problem of designing a browser that can extract content from this hidden web page. We introduced a hidden web browser's general operating model and described how this model is achieved by HiWE (hidden web screen), a prototype browser built in Stanford We introduced a new base to information extract technology (LITE) and using it to automatically extract the search tables for testing.
From publishers: Upgrade software project Napster, Gnutella, and Freenet have occupied the newspaper title, challenged the traditional methods of content distribution, their revolutionary use of the opposite file sharing technology. Journalists try to classify clearly unreliable right network division. Lawyers, business leaders and social commentators discussed the virtues and evils of these brave new distribution systems. But really behind what is this destructive technology - breakthrough innovation, they rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise rise
While many query tree optimization strategies have been proposed in literature, there is still a lack of a formal and complete representation of all possible query operations transition (i.e. implementation plan) in a unified way. The graphic theory method submitted in the paper provides a sound of the mathematical basis representing a query and search implementation plan. In this graphic model, a nod represents an operation and the direction of the edge between the two nodes represents the oldest implementation of these two operations in a implementation plan. Each nodes are related to a weight, therefore a edge. The weight is an expression, containing the optimization of the required parameters, such as the relationship size, double, adding selective factors. All possible implementation plan represents the overall implementation model in each width of this graphic one nodes
This tutorial introduces the consensus language for temporary queries TSQL2 through the main structure of the media planning scenario. media planning is a series of decisions involved in providing advertising information through the mass media. we will track the planning of specific advertising activities. we introduce the scenario by identifying marketing objectives. media planning involves business and records in the temporary database. media planning must then be evaluated; we show how TSQL2 can be used to obtain information from the data stored.
We consider analyzing the problem of the market material and presenting a few important contributions. First, we introduce a new algorithm, finding big items, using fewer than the classical algorithm through the data, but using fewer candidate items sets, rather than the sample-based method. We investigate items to rearrange the idea that can improve the low level of efficiency of the algorithm. Second, we introduce a new way to generate "participation rules", which are based on the precedent and the latter, and are real effects (not just a simultaneous measurement), we show how they produce more intuitive results rather than other methods.
The HiPAC (High Performance Active Database System) project solves two key issues in time limitation of data management: processing time limitations in the database and avoiding waste surveys by using the situation rules, which are part of the database and are monitored by the DBMS state monitor. A rich knowledge model provides defining time limitations, the situation rules and the primitivity required to predict events.
Internet service providers (ISPs) use real-time data transmission integrated traffic in their network to support effective technology and business decisions. A basic difficulty with building decision support tool based on integrated traffic data transmission is one of the data quality. Data quality issues arise from monitoring network specific problems (unregular queries by DP pack and delays, geological errors labels, etc.) and make it difficult to distinguish data system data integration is very large, data integration data principle is data display data and real phenomenon, providing data analysis based on these data transmission is ineffective.
Relative Online Analysis Processing (ROLAP) is the main method of data storage, and in order to improve query performance, the ROLAP method relies on the appropriate subgroup of the selection and substantialization of the view in the summary table and then participates in accelerating the OLAP query.
This study aims to explore the impact of multiple locks on the performance of multiple working class transaction processing systems, which is common in multiple user database systems. The study has two key findings. First, the same working class adopted locks granularity should not be different due to factors above 20; otherwise, severe data blocks may result. Second, short-term working class transactions are usually beneficial when their granularity levels are similar to the long working class, as this will reduce additional locks and content data locked in multiple working class.
E-commerce systems (Retail, Auction, etc.) are the right examples of data-based systems, operating under the accuracy and durability requirements of the nature of the transaction, but beyond the traditional databases framework, since they are formed by an abnormal, independent combination of components, we introduce a framework to identify, analyze and reason for the behavior of these systems, focusing on how they are designed to consistent progress in case of failure.
In this context, the query assessment engine needs to ensure that the user query only uses and returns the user allows access to XML data. These add-on access control checks can significantly increase the query assessment time. In this article, we consider optimizing the query safety assessment issues. We focus on simple but useful multi-level access control models, one of which the security level can be specified XML elements, or inherited its parents. For this model, the security query assessment is possible, by re-writing query using a repeated feature, calculating the security level of an element. Based on security information in DTD, we algorithm effectively determine the query safety assessment time. We focus on simple but useful multi-level access control models, one of which the security level can be specified by XML elements, or its parents.
The Illustra Web DataBlade module is a comprehensive toolkit for creating a web-functional database application to dynamically access and update the Illustra database content.You can build a simple query front and powerful Web application in a single hour in the Web DataBlade module.The Illustra Web DataBlade allows you to make full use of the Illustra sera, with many important features, including extended data types, a basic rule system and time travel capabilities, all of which make the Illustra a choice database for managing all types of of content on the Worm Wide Web.
We introduce an effective indexing method in a set to find one-dimensional next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next
Distributors of business database management systems face many challenges to incorporate their products into the academic development of innovative technologies. Practical considerations, ions and operational requirements can limit the feasibility of applying future research. The technical leadership of business products is often taking non-traditional methods rather than following the "traditional wisdom" as shown by the examples of several technologies such as Oracle and its pioneers.
Based on this index problem, most systems use the use of B+ wood implemented class index (CH) technology. Other technologies, such as[14,18,31], can lead to improving the average case performance of CD, but involve the implementation of the new data structure's basic size. As a special form of external dynamic two-dimensional range search, OODB index problem can be solved within the reasonable worst range of situations. Based on this view, we developed a technology called class index (CH) technology. Other technologies, such as[14,18,31], can be used as a practical instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance
The formation and evolution of the galaxies are the core drivers of the galaxies and cosmology studies.The recent studies provide a global image of the history of the constellation formation.Therefore, what drives the activity of the constellation formation in the galaxies has evolved for a long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long time, long
In temporary databases, searching frequently employs cryptocurrency operators, whether for encrypting results or data that is not encrypted during storage. Therefore, the performance and optimization programs used by the operator are essential for the performance of the temporary DBMS. So far, the performance of the various algorithms of the operator has been conducted, however, the joint optimization of the cryptocurrency operators appearing in the searching execution plan has only gained the least attention. In this article, we presented a plan to combine the cryptocurrency operators with the cryptocurrency operators generated from the cryptocurrency operation.
Automatic selection of the appropriate combination of the materialized view and indicators for the SQL database is a non-second task. A judgment choice must be cost-driven and impact the work load experienced by the system. Although in the background of the multi-dimensional database (OLAP) there is a materialized view choice work, no past work has seen the creation of an industry intensity tool for the automatic selection of the materialized view and indicators for the SQL work load problem. In this article, we introduce a terminal solution to the problem of the choice of the materialized view and indicators. We describe a wide range of experimental evaluation results, proofing the effectiveness of our technology.
Traditional methods for editing files are based on editing, and represent a continuous version of using editing text. This paper presents a reference-based editing plan, which retains the rich logical structure of the file, through the object reference. This method produces better support for queries, and harmonizes the level of storage and transportation level of representation of multiple versions of XML files. Specifically, we introduce the ecient algorithm to support projection and select queries, and queries the text evolution history. Then, we show our queries are also ecient at the transport level, where the XML file is the distant part of exchange in fact, with the reference plan, a XML file history can also be seen and processed as another file.
We consider the problem of searching in a big database. The typical application of this problem is genetic data, web data and event sequence. Because of the size of these databases unusually grows, so for these issues the use of memory algorithms becomes unpractical. In this article, we recommend that the data of the font map to a full space, using the wavelet equation. Later, we index these equations through MBR (the smallest edge straight). We define a distance function, which is the lower limit of the actual editing distance between the lines. We try with the closest neighbor queries and range queries results show that our technology will make a significant number of the databases (usually reduced to 95%), thus making the IO and CPU cost significant.
Data mining, or data base knowledge found, has been generally recognized as an important research issue with a wide range of applications.We provide a comprehensive survey, in the data base perspective, about data mining technology recently developed several major types of data mining methods, including generalization, characterization, classification, association, evolution, model matching, data visualization, and meta-oriented mining rules, will be reviewed.
A major problem with the existing method of the chart is that they cannot take into account many modern relations database design alternatives (e.g., using binary data storage multi-value properties). This paper introduces a chart program that can be applied to the existing relations database without changing its chart.
Real-time production systems and other dynamic environments often produce huge (potentially unlimited) traffic data; the data is too large to be stored on the disk or scan several times. We can do online, multidimensional analysis and data mining to warn people of the dramatic changes of the situation and start a timely, high-quality response? This is a challenging task. In this paper, we study online, multidimensional time traffic data traffic analysis methods, the most effective time traffic data, with the following contributions: (1) our analysis shows that only a small amount of compressed traffic measurements, rather than complete traffic data needs to register multidimensional traffic analysis, (2) in traffic analysis easily identify data, for data model traffic measurements, data for traffic measurements, data for traffic measurements, data for traffic measurements, data for traffic
Welcome to SIGMOD 2000! we think you’ll find both the meetings and the environment exciting.The eternal beauty of Britain’s Columbia will provide a suitable comparison to the dynamics of our large-scale, high-performance and increasingly intelligent database systems design and deploy.This dynamics reflects in our (extreme) key tips presentations, tutorials, research workshops, presentations, industrial workshops and product presentations.
Visualization includes design choices about data access, data transformation, visual performance and interaction. In order to explain static performance, a person must identify the compatibility between visual performance and basic data. These compatibilities become mobile targets when visual performance is dynamic. Dynamics can introduce visualization in any point of the analysis and visualization process. For example, the data itself can flow, the transformation subgroup can choose, visual performance can animate, and interaction can change the demonstration. In this article, we will focus on the dynamic data impact. We introduce a graphic and conceptual framework to understand how data changes visual performance interpretative effects.
The network is a emerging infrastructure to provide coordinated and consistent access to the distribution, unusual computing and information storage resources among autonomous organizations. the data network is building around the world as the next generation of data processing systems to share access to data and storage systems within multiple administrative areas. the data network provides logical name space for digital entities and storage resources created by the global detector is independent location. the data network system provides services for logical name space for operations, management and organization of digital entities. the data base is increasingly in network applications and data management data management, the multiple groups are now developing services for access and integrated structure data on the network. the service-based data access, through the adoption of the open network entities and storage resources of the global data space system provides logical name operations for organizations and the digital space operations.
With the appearance of XML as a data format in the Internet age, the number of data in XML format increased significantly, and to better describe these XML data structures and limitations, several XML format languages were presented, in this article we presented the six valuable comparative analysis of XML format languages.
We propose a data model and query language, integrating the graph of a clear model and query smoothly into a standard database environment. For standard applications, some object-oriented models provide key characteristics, such as object class organizations to a sequence, object identity, and property reference objects. query can be carried out in a familiar style, with derivative statement, can be used as a choice of... from where. On the other hand, the model allows a clear description of the graph by dividing the object class into a simple database environment, link class, and route class, its objects can be considered as the node of the graph, the edge, and clearly stored the path (this is the whole database example query, the database expands the graph of the graph of the graph of the graph of the graph of the graph of the graph of the graph of the graph of the graph of
Simple economic and performance arguments provide the appropriate lifetime for the main memory pages and recommend the best page size.
Cache FusionTM is a basic component of Oracle’s Real Application Cluster configuration, a shared collective database architecture that transparently expands the database applications from a single system to multiple nodes shared discs. In the classic shared discs implementation, the discs are the data sharing media, the data block is sent through the discs writing and reading under the arbitration of the distributed lock administrator. Cache Fusion expands this ability to share the discs architecture by allowing the nodes to connect through the collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective collective
The theoretical interpretation of this trend differs from the focus on the inner maturity and socialization effects that can help solve this problem. to what extent the increase in the stability of the character is due to the individual differences of continuity and crystalline genetic influence, to what extent the increase in the stability of the character character character, to what extent the growth stability is from adult stability, to what extent the growth stability, to what extent the growth stability, to what extent the growth stability, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to what extent the growth, to
The view as a means of describing the data collection part, plays an important role in many database applications.In the dynamic environment of data updates, not only the information provided by data collection.
Abstract. Our goal is to develop new databases technology to approach the non-structured series data use index. We explore the potential, in this context, suffix tree data structure. We introduce a new method to build suffix tree, allowing us to build a tree, over the RAM size, so far it is impossible. We show that this method works in practice, as well as the O(n) method of Ukkonen [70]. using this method, we build an index of 200 MB of protein and 300 Mbp of DNA, its disc image over the available RAM. We experiment show that suffix tree can effectively be used in the near sequence matching with data. For a range of query and error lines, the size of the tree has reduced an optimized O(n) dynamic program.
We presented a system to publish XML data from mixed (relative + XML) properties, while supporting storage discounts for adjustment. The match between public and character graphs is provided by a combination of LAV and GAV style views expressed in XQuery. XML and relative integrity limits are also considered. From client XQueries to public graphs, the system achieved a rewrite with view, composition with view and query combined effect, under integrity limits, to obtain the best reforms of character graphs.
Data storage and recording system usually has a large continuous input data flow, must be stored storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage storage stor
Our system architecture of the sensor data is described. our data mining application requires the history of the sensor data. Therefore, with most existing systems, focusing on the flow of data and hiding a small window of history data, we store the entire history data. In these scenarios there are several interesting issues. We study two: (a) As the sensor can send data at any given moment matching its current configuration, how do we define the data should be stored in the database? (b) the sensor tries to reduce the amount of data transmitted.
The latest advances in processor, memory and wireless technology have allowed three wireless sensor networks, which are deployed to collect useful information from an area of interest. Sensor data must be collected and transmitted to a base station so that the duration of the system can further process the terminal user queries. Because of the network by low-cost nodes with limited battery power comparison, the energy efficiency method must be used to collect and integrate data in order to a long network life LEAP. In a environment, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in one area, in
Although there are some early signs that our final FY2002 fund balance may be much lower than our initial end for FY02 $142k (see Tamer Ozsu in SIGMOD Record No. 31 No. 2), our financial situation remains good.
Space fusion is the most important method to evaluate the relationship of time objects. In this article, space fusion processing is a detailed study of extended space objects in two dimensions of data space. We introduce a method of space fusion processing is based on three steps. First, a space fusion is performing the minimum limit of the straight angle of the object returning to a complex data group of candidates. Through a complete space fusion shows a personal data group. Different methods to accelerate this step of space fusion processing several steps, we conducted a detailed study on the last year’s meeting. In this article, we focus on how to calculate the answer of the two steps of the candidates. First, the accurate approach to the application is correctly identified as the answer, and through a complex space fusion can be tested through a complex space.
While content-based image acquisition (CBIR) is a continuously expanding field and often proposed new methods for more effective acquisition, relatively less attention has been paid to the assessment of the effectiveness of the CBIR method. The majority of the reports evaluate using the standard IR evaluation methods, less taking into account their statistical significance or adequacy for the CBIR, which makes it difficult to assess the accurate impact of the individual methods.
Traditional software systems, such as the "Desktop Image" system, are not applicable to managing electronic information and events of ordinary computer users.We introduce a new image, Lifestreams, to dynamically organize the user's personal work space.Lifestreams uses a simple organization image, time arrangement of the document flow, as a basic storage system.
Question Q is told to be effective restrictions if all data sets D, there is D of evaluation of subgroup D, such as Q(D) = Q(DQ), while the size of DQ and the time of evaluation of DQ are independent of the size of D. The need to study these queries is obvious because it allows us to calculate Q(D) by accessing a limited data set DQ, no matter how big the plan D is. This paper studies the effective restrictions of relevant queries (SPC) under a access program A, determining the indicators and cardinity restrictions in general use. We provide the characteristics (sufficient and necessary conditions) to determine whether a SPC query is effective restrictions in the A study. We determine several questions, whether the query is limited to the data collection, whether the query is limited to the data collection, whether the qu
The number of web pages in content and information is very dynamic, in order to fully exploit its enormous potential as a global database, we need to understand its size, geology and way of evolution of content, which allows for the development of new information location and accessing technologies that can better adapt and expand to its change and growth.
A prototype data mining system, DBLearn, has developed, effectively and effectively extracted different types of knowledge rules from the relationship database, it has the following characteristics: a high-level learning interface, closely integrated with the business relationship database system, automatically improving the concept level, effective detection algorithms and good performance.
World Wide Web (WWW) is a rapidly growing global information resource. it contains a lot of information and provides access to a variety of services. because there is no central control and a few information organizations or services provided by standards, search information and services is a widely recognized problem. to some extent, this problem is solved by the "search services" and is also known as "database indicators", such as Lycos, AltaVista, Yahoo, and others. these sites use the search engine of data systems, it is a significant tool called "robots" or "intellectuals", these services regularly scan the network and form the text-based indicators. these services are limited to some important aspects.
As the focus on global healthcare increases, the ability to effectively manage a large amount of patient information in various media becomes crucial. In this work we will show how advanced database technologies are used in the integrated system of screening and management of diabetic patients. RETINA captures the personal data and retinal images of diabetic patients and automatically processes the retinal foundation images to extract interesting features. Given the gain of information we use new technologies to identify the patient’s risk information to better manage the patient’s care and to target more detailed studies, these studies can be used to introduce effective preventive measures.
Testing SQL Database Systems by running a large number of definitions or static SQL Statements is a common practice in the development of business databases. However, code defects are often not found because the choice of an optimizer for the execution plan depends not only on the query, but is affected by many parameters described by the database and hardware environment. Modifying these parameters to guide the optimizer to choose other plans is difficult because this means predicting frequently complex search strategies implemented in the optimizer. In this article, we design algorithms of calculation, complete production, and unified sample of the plan from a complete search space. Our technology allows broad verification rather than an optimized database and hardware environment options - these parameters are modified to guide the choice of other programs because this means the implementation of the complex search strategy in the optimizer.
In literature, a series of implementing strategies for the parallel assessment of multiple queries; their performance is evaluated through simulation. In this article, we implement a comparative performance assessment of four implementing strategies on the same parallel database system (PRISMA/DB). The experiment has been completed to 80 processors. The basic strategy is to first determine a minimum total cost of the implementation schedule and then make the timetable for the parallel assessment of one of the four implementation strategies. These strategies, obtained from literature, are named: sequence parallel, synchronized implementation, divide right, and complete parallel based on a clear experimental guide when using the strategy.
Decision support systems (DSS) and data storage work loads make up a growing part of the database market today. I/O capacity and DSS work loads related processing requirements are rapidly increasing, approximately twice every 9 to 12 months. In response to this growing storage and calculation demand, we introduced a computer architecture that supports the database server using a "smart" disk (IDISK). IDISK uses low-cost built-in general use processing, main memory and high-speed serial communication links on each disk. IDISK connects each other through these series links and high-speed exchangers, beyond the I/O bottle system.
A platform called AnMol supports the structural data analysis application is described. the term "biomolecular structure" has different conclusions and different representations. AnMol reduces these representations to the graphic structure. each graphic then is stored as one or more vectors in a database. vectors encapsulate these graphic structural characteristics. structural queries such as similarities and infrastructure convert to space structures such as distance and container within the area. queries results are based on inaccurate game.
Multimedia technologies, global information infrastructure and other developments enable users to access more and more types of information sources. However, the individual "technology" availability (through the network, WWW, postal systems, databases, etc.) is not sufficient to make meaningful and advanced use of all information available online. Therefore, the issue of effective and effective access and query of abnormal and distributed data sources is an important research direction. This paper aims to classify existing methods that can be used to query abnormal data sources.
The current state of the most advanced technology to index high-dimensional data is first to reduce the size of the data, using the main component analysis, and then to index the size of the space using the multi-dimensional index structure. The above technology, known as the global size reduction technology (GDR), works well when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data set costs reduction technology (GDR), when the data center costs reduction technology (GDR), when the data center costs reduction technology (GDR), when the data center costs reduction
Personalization generally refers to making a site more responding to the unique and personalized needs of each user. We believe that to personalize the work of effective, detailed and interactive user profiles should be used globally for authorized sites, these profiles should dynamically reflect the changes in user interests. Creating user profiles from user click flow data seems to be an effective way to produce detailed and dynamic user profiles. However, this generates user profiles only on the computer, the user accesses his browser and is unaccessible when the same user works on different computers.
IBM Lotus Domino/Notes is an excellent office electronic collaboration platform and collaboration platform, industry-leading IM solutions integrated with industry-leading companies, creating a collaboration solution. This paper uses software engineering ideas and methods, based on Domino/Notes. First, Domino/Notes platform is built and composed; then, designed by the B/S and C/S combination of office automation system structure; then, designed a Domino database component, including access control lists, logic and data; Finally, design directory services, easy storage, access, management and use of resources.
Propel Distributed Services Platform (PDSP) is Propel’s core software product, a new Internet infrastructure software company.The PDSP product is designed to allow Java developers to build, implement, deploy and maintain Internet applications and services faster than ever before, while providing all of these applications with the RAS (reliability, availability and scalability).
In this article, we introduce a new technology to estimate the user defined method in advanced database systems. This technology is based on a multi-dimensional history chart. We explain how the system collects statistical data, a database user defines and adds to the system. From these statistics, a multi-dimensional history chart is built. Then, this history chart can be used to estimate the cost of the target method, each time this method is mentioned in a query. This cost estimate is required for the database system optimizer, because this cost estimate needs to know the cost of the method so that it is placed in the best position in the query implementation plan (QEP). We explain here how our technology works, we provide an example to better verify its functionality.
In the database framework of Kanellakis et al. [1990] it is believed that the limitation of the query language should take the limitation of the database as input and provide other limitation of the database using the same type of atomic limitation as output. This closed form requires it is difficult to the limitation of the query language containing the refusal symbols.
We have developed Gigascope, a network application flow database, including traffic analysis, infiltration detection, router configuration analysis, network research, network monitoring and performance monitoring and dismantling.Gigascope is installed on many websites within the AT&T network, including on the OC48 router for detailed monitoring.In this article, we describe our motivations and limitations in the development of Gigascope, the architecture and query language and performance issues.
Through a complete browser indicator to keep the currency quickly becomes impossible, because the size of the web page and the dynamic content increases. The target of the browser is to search for the subgroup of the web page only related to a specific category and to provide a potential solution to the monetary problem. The main problem of the browser is to perform the appropriate credit allocated to different files along a browser route, therefore, the short-term profits do not pursue the cost less apparent and visible browser route, ultimately producing a larger collection of valuable pages. To solve this problem, we introduce a focused browser algorithm, building a model of background that occurs on the web page. This background model can catch a typical link because the content on a browser route, therefore, the cost is not less expensive.
The program is defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as defined as
Remote data access from different sources in a wide range of networks, such as the Internet is a problem because of the unpredictable nature of the communication media and the lack of knowledge of the load and potential delays on the remote site, although providing good performance in many cases, it is also proven to be vulnerable to the problem because they are unable to adapt to unpredictable delays. Remote query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query
Despite the rapid decline in the price of the disk, third-party storage (i.e. the removable media in a robotic storage library) is increasingly popular. The fact that so many data can be stored encourages the application to use increasingly bigger data sets. The application drivers include multimedia databases, databases, scientific databases, digital libraries and files. Databases research community responded to research on system integration, performance model and performance optimization. The third-party storage system faces special challenges due to its unusual performance characteristics. Accessibility delays can be divided into a few minutes, even on the unloaded system, but the transfer rates can be very high. The third-party storage application adopts a wide range of technology, each database has its own performance.
XKeyword provides effective keywords close to query in a big XML chart database. query is just a list of keywords and does not require any chart or query language knowledge for its formatting. XKeyword is built on a relative database, therefore, can contain a very large chart. query evaluation is by using the chart optimized. in particular, XKeyword is composed of two stages. in the pre-processing stage, a set of keywords indicators is built together with the index route relationship, describing the route in the chart a specific pattern. In the query processing stage the plan is developed, using a close to the best route relationship to effectively find the result of the query is through the chart, using the new chart interactive results are based on the user chart.
The paper studies the technology of extracting data from HTML sites by using automatically generated plugins. To automatize the process of generating plugins and data extract, the paper developed a new technology to compare HTML pages and produce plugins based on their similarities and animals.
The entity relationship (ER) model plays an important role in the relationship database design. the data requirements are conceptualized with the ER model and then converted into relationships. if the requirements are understood by the designer, then if the ER model is modeled and converted into relationships, the result relationships will be normalized. however, the choice of the model relationship entity with the appropriate degree and cardinal proportion has a very serious impact on the database design. In this article, we focus on the model binary relationships, binary relationships, dividing binary relationships, converting the same relationship issues.
Although various loss compression programs have been developed for certain forms of digital data (e.g., image, audio, video), the loss of compression technology in the area for voluntary data tables has been relatively unexplored. However, these technologies are clearly motivated by increasing data collection rates of modern enterprises and the need to be effective, ensuring the quality of close-range answers about the large-scale relative data set. In this paper, we recommend a system, using the characteristics of the details and data mining effect model for the loss of large-scale data tables.
Mobile computers will soon have online access to a large number of databases through wireless networks. due to the limited bandwidth, wireless communications are more expensive than wireless communications. in this article, we introduce and analyze a variety of static and dynamic data allocation methods. the goal is to optimize the cost of communication between mobile computers and storage online databases. the analysis is carried out in two cost models. one is the connection (or time) based, such as in mobile phones, the user is connected per minute.
The evolution and system biology are increasingly dependent on building a large philosophical tree, representing the relationship between the species of interest. As the number and size of these trees increase, therefore it requires effective data storage and query capacity. Although a great deal of attention has been concentrated on XML as a tree data model, the philosophical tree in its size and depths are different from the document-oriented application, their needs are based on structural query rather than on the route query.
Workflow Management Systems (WFMS) for organizing business processes in multiple organizations are complex distributed systems: they are made by multiple workflow engines, application servers and communication intermediate software servers, such as ORBs, where each type of server can be repeated in multiple computers’ size and availability. finding a suitable system configuration and ensuring the application specific service quality channels, response time, and tolerable termination time is a major challenge for human system administrators to use. This paper introduces a tool, a large automated task configuration of a distributed WFMS. Based on a mathematical model, the tool produces the necessary repeated level of server different types to meet the specific goals and availability, such as "system availability" and "system availability" such as "system availability"
This paper introduces the InterPARES project, its objectives, objectives and research fields, its basic concepts and assumptions, methods and overall results, focusing on one of its products, the authenticity of the electronic records, and ends with the second phase of view.
In this meeting, my goal is to provide a background for such meetings, first about the topics related to the computer, second about the background of this special meeting. I think that I have the qualification of the keyboard as good as anyone: in this special meeting, I am now not directly involved in the type of computer work that we will discuss; the organization I represent, Bell Telephone Laboratories, is not in the business of making these computers.
The popularity of the distributed computing environment and the growth of the “information superfast” significantly increased the number of available databases. Unfortunately, there are major challenges to overcome. A specific problem is the background exchange in which each source of information and potential information recipients can operate with different backgrounds, leading to large-scale racial abnormalities. A background is the summary assumption of background definitions (i.e. meaning) and background characteristics (i.e. quality). This paper describes various forms of background challenges and potential background mediation services, such as data racial acquisition, data quality characteristics, development of ethnicity and quality, can relieve the problem.
In Berkeley, we are developing TelegraphCQ [1, 2], a data flow system to deal with the data flow continuous queries.TelegraphCQ is based on a new, highly adapted building that supports dynamic queries work load in a volatile data flow environment.In this presentation, we show our current version of TelegraphCQ, we implement the code base by using the open source PostgreSQL database system.Though TelegraphCQ is significantly different from the traditional database system, we found that a significant part of PostgreSQL code is easy to repeat.We also found the extensive feature of PostgreSQL is very useful, its data-rich type and the ability to load the user's development features challenge: discussing and sharing, our technology is the main challenge of continuous queries:
The basic idea is to calculate many integrated values for small to medium size integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integr
Like the relationship data, XML data changes over time with the method of creating, modifying and deleting the XML file. Expressing time variable (relationship or XML) data query is harder than writing occasionally data query. In this article, we introduce a temporary XML query language, τXQuery, in which we add effective time to support XQuery by minimizing the extension of XQuery synthesis and sequential data clarity. We take a level of method to map a tXQuery query to a regular XQuery. The paper focus is on how to perform this map, especially the map of sequential query, which is the most challenging. The key question is to support sequential query any time query (tQuery time query, while query on XQuery query.
The popularity of the World Wide Web (WWW) makes it the primary tool for the dissemination of information. the relevance of the database concepts to the management and questioning of these information has led to recent research to solve these problems by a signi cant institution. although the basic challenge is traditionally dealt with by the database community {How to manage big data {WWW's new background forces us to signi cantly expand the previous technology. the main objective of the survey is to classify the relevant tasks to the database concepts application and emphasize the need to do so by technological innovations.
Through a complete browser indicator to keep the currency quickly becomes impossible, because the size of the web page and the dynamic content increases. The target of the browser is to search for the subgroup of the web page only related to a specific category and to provide a potential solution to the monetary problem. The main problem of the browser is to perform the appropriate credit allocated to different files along a browser route, therefore, the short-term profits do not pursue the cost less apparent and visible browser route, ultimately producing a larger collection of valuable pages. To solve this problem, we introduce a focused browser algorithm, building a model of background that occurs on the web page. This background model can catch a typical link because the content on a browser route, therefore, the cost is not less expensive.
Recent studies discussed the importance of optimizing the use of L2 cache in the design of the main memory index, and presented the so-called cache awareness index, such as CSB+ Tree. However, these indicators do not take into account competitiveness control, which is crucial for running real world main memory database applications, including index updates, and using the external shell multiprocessor system to expand the performance of these applications.
We report the design and implementation of the Privacy Integrated Question (PINQ) data analysis platform.PINQ provides an analytics programming interface so that data can be smooth through a language similar to SQL. At the same time, the design and thorough implementation of the PINQ analytics language provides a formal guarantee of differential privacy for any use of the platform.PINQ’s unconditional structure guarantees no need to be placed in the expertise of the analyst or confidence, significantly expands the scope of the design and deployment of data protection analysis, especially non-expert.
We have introduced a multi-dimensional classification (MDC) of the physical layout, in DB2 version 8.0 of the relative table. Multi-dimensional classification is based on the definition of one or more square classification characteristics (or expression) of the table. Table is made by the physical body's related records with similar values of size characteristics in one classification. Each classification key is assigned to one or more categories of physical storage, designed to store multiple records belonging to classification in a almost consistent way. Block-oriented indicators are created to access these blocks. In this article, we describe the new technology of query processing operations, which provides significant improvement in performance for the MDC table. The current database system uses similar values, including the scanning and scanning databases.
The web page introduces the database area with rich opportunities and sophisticated challenges. the database and the web page are organic connections in many lev els. the web page is increasingly pow ered b y database. the link page collection distributed on the internet itself is the target of the database. the appearance of XML, as the web page of lingua fran a brings some m uchneed order and will greatly promote the use of the database technology to manage the web page information. this paper will discuss some of the development related to the web page from the database theory. as we see, the web scene needs to review some of the basic assumptions of the area.
For many KDD applications, such as detection of criminal activity in e-commerce, finding rare examples or foreign exchange can be more interesting than finding common patterns.The existing work in foreign exchange detection involves as a foreign exchange as a binary asset.In this paper, we believe that it is more meaningful for many scenarios to allocate each object to the degree of foreign exchange.This degree is known as the object of local foreign exchange factor (LOF).It is local, in this degree depends on how isolated the object is with the surrounding neighbors.We provide a detailed formal analysis that shows that LOF enjoys many of the necessary properties.Using real world data, we show that LOF can be used to find foreign exchange meaning but no other methods to identify foreign exchange.
The accurate synchronization between the source data objects and hidden copies is impossible in the environment, due to the bandwidth or other resource restrictions, static (from the date) copies are permitted. It is ideal to try to reduce the overall difference between the source objects and hidden copies, through selective updates to improve the objects. We call on the choice of the online process, which objects to update, to try to reduce the difference, the best efforts to synchronize. In most methods the best efforts to synchronize, hidden the coordination process, and select the objects to update. In this article, we propose a best effort to synchronize the planning policy, using the data source and hidden copies cooperation. We also propose a decision-making policy, even in the real communication, the data source and the data source are very different in
The expert databases are supporting the processing rules of the environment, against the disk residential databases. They occupy a middle position between the active and driven databases, as well as the static and dynamic load balance protocols of the abstract level of the basic rules language. The rules language operational grammar affects the problem-solving strategy, while the structure of the processing environment determines efficiency and scalability. In this article, we introduce the rules architecture of the elements and its core rules language, PARULEL. The rules environment provides support for the parallel and distributed rules process assessment, as well as the static and dynamic load balance protocols for the predictive balance calculation in the running time.
As a result of the progress in semiconductor production, the gap between main memory and secondary storage is increasing. This becomes a significant performance bottle of the database management system, depending on secondary storage serious storage big data sets. The latest advances in nanotechnology led to the invention of alternative storage. In particular, microengine systems (MEMS) based on storage technology appeared for the next generation of storage systems the leading candidate. In order to integrate MEMS based on storage into the traditional computing platform, the new technology is necessary for I/O programming and data configuration. In the context of relative data, it has been observed that the need for access to the relationship so that the color and color are stored.
We propose a new, highly expansive and effective technique to evaluate the point check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check check
One of the most challenging issues in the digital library is finding relevant information. information findings are the focus of the research of the Stanford component of the CS-TR project sponsored by ARPA and work has been one of the main drivers of the Stanford Integrated Digital Library project. In this article, we discuss some information finding issues such as text database findings, effective information dissemination, copy detection and deletion.
The latest advances in data input technology make it easier for financial and retail organizations to collect a lot of data and store them on a low cost. These organizations are interested in extracting unknown information from these giant databases, stimulating new marketing strategies. In this demonstration, we introduced SOAJAR, a system used for mining to optimize the association rules, using digital data from the databases as well as Boolean data.
Electronic data exchange (EDI) business standard is such a early adoption of progress in computer networks. the availability and availability of the network enables the corporate crowd to automatize their B2B interaction. However, several problems, with scale, content exchange, autonomy, and other issues still need to be solved. In this article, we investigate the main technologies, systems, products, and standards of B2B interaction. We proposed an assessment standard of different B2B interaction technologies, standards and products.
DISCOVER operates on a relative database and by allowing its users to send keywords queries without any knowledge of the database chart or SQL and easily obtain information on these databases. DISCOVER returns the qualified double network joining, i.e. the connection of the double network as they connect to their main and external keywords and collectively contain all keywords queries. DISCOVER progresses in two steps. First, the candidate network generator produces the relationship of all the candidate networks, i.e. the connection of the expressions, producing the connection of the double networks.
Data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data
It is common to find graphics increases, there are millions of nodes of spread and billions of average efficiency, for example, social networks. These graphics queries are often prohibited to be expensive. These motivations we suggest queries save graphics compressed to compress graphics relative to a class of L queries user choice. We calculate a small G from a real G graphics, so (a) for any queries Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q
Today, many applications operate in a browser-based client, medium (application) server and a background database server in a multi-layer environment. medium database query tries to discharge a part of the database work load from the middle database server to the middle database server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server to the middle server.
We have faced this challenge at the RHESSI Experimental Data Center (HEDC), a data center for the RHESSI NASA Space. In this paper, we describe our experience in developing the HEDC and discussed the design options. To successfully adapt to the typical adaptations in the scientific data management system, the HEDC(i) clearly separates the genes from the field specific code in all third parties, (ii) using a file system of actual data, combining a DBMS management of the corresponding data, and (iii) turning to a middle-level design if it requires a larger scale of processing or design power.
With the world wide network as a unified and reliable interface of computer applications and information spread, new opportunities introduce significant changes in all organizations and their processes.This presentation introduces the IDEA Web Laboratory (Web Lab), a web-based software design environment available on the Internet, which shows a new approach to the software production process on the Internet.
Data sources about the environment, energy and natural resources are very many worldwide. unfortunately, users often face several problems when searching for and using environmental information. in this article we analyze these issues. from a technical point of view, we describe the four main tasks of producing environmental data and describe the organization of the data generated from these tasks.
VERSANT is the industry’s leading object database management system (ODBMS) development application in multi-user, distributed environment. VERSANT ODBMS has an object-based client server architecture, especially suitable for complex applications such as telecommunications, conversion and practical network management systems. Because these applications are usually task-critical, not allowing my dtia. Based on time, one of our customers’ main requirements is 24x7 (24 hours a day, 7 days a week) object database high availability, even if there is software, hardware or network failure. Although many VERSANT features, such as online backup and dynamic charts, have supported these parts, they don’t need to deal with errors because these applications usually do not allow my dtia based time, a major requirement of our customer is 24x.
This paper introduces the technology of reducing the cost of data dissemination of query subscriptions. The reduction is achieved by combining query, but not necessarily equal answers. The paper formalizes query mixed questions and introduces a general cost model. We prove the problem is NP hard, and proposed a comprehensive algorithm and three algorithms: a pair of mixed algorithms, directional search algorithms and classification algorithms. We developed a simulator to evaluate different algorithms and show our algorithm’s performance close to the best.
The paper describes PREDATOR, a freely available object relationship database system, which has been developed at Cornell University, and a major motivation for developing PREDATOR is to create a modern code base that can be used as a research tool for the database community.
RAID5 disks offer high performance and high reliability for a reasonable cost. However, RAID5 suffered performance fines during the block update. To overcome this problem, the use of dynamic cutting is recommended. This method bubbled some updates, producing new cutting composed of the new update blocks, and then wrote a new complete cutting back on the disk. In this article, we reviewed the impact of the site on the dynamic cutting method. To further improve the performance in this environment, we introduced the dynamic cutting policy of the hot blocks.
Object Oriented and Object Relationship Database (OODB) must be able to load a large amount of data that OODB users bring to them. loaded OODB data is significantly more complex due to the existence of relationships, or reference data later load relationship data further experiments; the existence of these relationships means the naive load algorithms are so slow and unusual. In our previous work, we introduced data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data reoriented data re
The database has been adopting a program-based approach to store and restore structured data for decades. for the pair (P2P) network, similar approach has just begun to appear. although in this new context several database technologies can be reused, while a P2P data management infrastructure proposes additional challenges that need to be solved until the program-based P2P network becomes as common as the program-based database. We will describe some of these challenges and discuss how to solve them.
For some multimedia applications, it has been found that the domain objects cannot be represented as the functional influencer in its multidimensional space. on the contrary, the double distance between the data objects is the only input. supports content-based receipt, a method map of each object to a k-dimensional (k-d) point, and tries to retain the distance between the points. then, the existing space access index methods, such as R tree and KD tree's efficiency research methods can support the quick search results of k-d point. however, information loss is inevitable because the distance between the data objects can only be retained to a certain degree. here, we study using a data-based receipt method.
MLPQ/GIS [4,6] is a limited database system [5] such as CCUBE [1] and DEDALE [3] but specifically emphasized space time data. features include data input tools (the fourth icon in Figure 1), based on icon queries, such as @@@ Union, @@ Area, @@ Buffer, @@ Max and @@ Min, optimizing linear targeting features, and @@@ for Datalog queries.
For example, in OLAP, an analytical query involves the integrated sales performance of different locations and seasons of products, which can help identify interesting cells, such as the databases with a boundary of integrated sales. although the normal answer to this query is only returning all interesting cells, it may be more informed if the system returns the cells summary or the description of the area.
The availability of broad-space database copy technology and content delivery network allows Web applications to be hosted and served by a powerful data center. This form of application support requires a complete Web application package with database copies distributed with negative effects. One of the main advantages of this method is that dynamic content is from a location close to the user, resulting in a network delay and rapid response time reduced. However, this point is achieved by excessive cost, because (a) ineffective hidden dynamic content in the marginal account and (b) the database copy syncability. These applications require a complete Web application package to be distributed together with database copies. In this article, we provide a new dynamic content to ensure the user's state of adjustment to the user's state of adjustment.
In this article, we consider taking dynamic data - such as stock price and real-time weather information - from the source to a set of storage technology, we focus on the question of consistency of dynamic data elements in the network of collaborative storage, we show cooperation between storage - each storage drive data elements up to other storage - helps to reduce the entire system of communication and calculation storage to maintain consistency. however, unlike intuition, we also show that increased cooperation degree at some point, in fact, can damage the goal of consistency in the network of low communications and calculation storage.
The success of the business query optimizer and database management system (five based on objects or relative) depends on the accurate cost estimate (BGI) of the various query. estimate the selectivity, or the sequence of the scores in a database, meets a selection forecast, is the key to determining the best combination order. previous work focuses on estimate the selectivity for the digital field (ASW, HaSa, IoP, LNS, LNS, SAC, WVT). according to this method, we can estimate the prediction of this data, the popularity of the text data we store in the database, it has become important to estimate the accuracy of the specific issues in the field of mathematics, such as SQL forecast.
The success of the business query optimizer and database management system (five based on objects or relative) depends on the accurate cost estimate (BGI) of the various query. estimate the selectivity, or the sequence of the scores in a database, meets a selection forecast, is the key to determining the best combination order. previous work focuses on estimate the selectivity for the digital field (ASW, HaSa, IoP, LNS, LNS, SAC, WVT). according to this method, we can estimate the prediction of this data, the popularity of the text data we store in the database, it has become important to estimate the accuracy of the specific issues in the field of mathematics, such as SQL forecast.
We see the world as a fully connected information space, each object communicates with all other objects without any time and geographical limits.We can simulate this fully connected space, using delicate particle processing, which can be implemented through the sensor technology.We see the sensor as the atomic computing particle, which can be deployed to the geographical location capture and processing the data around.The report introduces a series of excellent research articles showing unique problems and their success in finding effective solutions.
The relative database provides the ability to store the user’s defined features and predictions, which can be mentioned in SQL queries. When evaluating a user’s defined predictions is relatively expensive, the traditional methods of evaluating the predictions of the naive optimization algorithms as soon as possible are no longer a sound dynamic. There are two previous methods to optimize such queries. However, no one can guarantee the best plan in the required implementation space. We introduce effective technologies that can ensure the best plan in the required implementation space. The optimization of the algorithm evaluation is relatively expensive, the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization of the optimization
In the past decade, the ability of individual experimental scientists to create and store data has increased significantly, which has not been matched by the same development of appropriate data management tools. In this paper, we introduce our efforts to develop a desktop experimental management environment results that many experimental scientists want on their desktop. The environment is known as Zoo, and collaborates with the field scientists from soil science and biochemistry to develop. We first describe the overall building of the Zoo and then focus on the key characteristics of its various components. We especially emphasize the aspect of the object-oriented database server, which is the core of the system, the experimental manager begins to perform the experiment as a result of the scientist, the system's mechanism to enable the use of their fields of scientists and biochemistry We first describe the
Through the "model", we mean a complex structure representing a design project, such as a relative program, object-oriented interface, UML model, XMLD, site program, grammatical network, complex file, or software configuration. Many models use involves the management model changes and data from one model to another conversion. These uses require a clear representation of the "map" between the model. We suggest that the database system makes it easier to use these applications, by creating the "model" and "model map" of the first class of objects with special operations to simplify their use. We call this ability management model. In addition to the model management case, our main contribution is a design model data model. These uses a clear representation of the "model" between the model we recommend that the database makes it easier to use these applications.
We have studied the effective maintenance of materialized views issues that may include translation. This issue is especially important when queries for these views involve integrated function, requires translation to produce the right results. With the majority of work visual maintenance issues, based on an algorithm method, our method is algorithm and based on balance of reason. This method has several advantages: it is solid, easy to expand to new language structure, it produces the production, it can be queries optimizer use, it simplifies their accuracy proofs. We use a natural extension of the relative algorithm operation to the bag (multi-group) as our basic language. We provide an algorithm that spreads the relationship from the materialist point of view. This method is based on the basic logical logical logical logical logical logical logical logical logical log
In the 1980s, practical and applied computer science conducted broad discussions on different backgrounds of versions and variable behavior issues, with the theme "concurrent control" and "technical product development and management" being central interests (DL88), (Ka90).
Through the "model", we mean a complex structure representing a design project, such as a relative program, object-oriented interface, UML model, XMLD, site program, grammatical network, complex file, or software configuration. Many models use involves the management model changes and data from one model to another conversion. These uses require a clear representation of the "map" between the model. We suggest that the database system makes it easier to use these applications, by creating the "model" and "model map" of the first class of objects with special operations to simplify their use. We call this ability management model. In addition to the model management case, our main contribution is a design model data model. These uses a clear representation of the "model" between the model we recommend that the database makes it easier to use these applications.
Abstract.Most research on attribute identification in database integration has focused on integrating attribute using schema and summary information derived from the attribute values.No research has tried to fully explore the use of attribute values to perform attribute identification.We propose an attribute identification method that uses schema and summary instance information as well as properties of attribute derived from their instances.
The possible trends in the development of future CAD, CASE and office information systems will be using object-oriented database systems to manage their internal databases. These applications will receive entities such as electronic parts and their connections or customer service records, which are usually large complex objects consisting of many interconnected abnormal objects, rather than thousands of modules. These applications may show a wide range of conversion use patterns because of their interactive mode of operation. This type of application will require a conversion method, which is applicable to convert large complex objects, and can adapt online to the conversion mode of use.
I first arrived at the AMS paper when I began to be interested in the data flow, in the spring of 2001. reading this paper was a real start for me. Although simple randomized ideas and basic probability tools (such as Chebyshev inequality and Chernoff boundaries) can be gathered together, providing elegant space efficiency randomly approaching the estimated problem, at first glance, it seems impossible to solve.
In the effective time relationship, each twin contains an interval character T, representing the effective time of the twin. The super layer merger between the two effective time relations determines the super layer gap of all the twin. Although the super layer merger is common, the existing division and indexing scheme is ineffective if the data includes the long-lived twin or if the gap crosses the division boundaries.
This is an important insight because it allows a complex statistical description of space data set; then, as we show, it also forms the basis of the theoretical analysis of space access methods, without using the typical, but unrealistic, unity assumptions. In this paper, we focus on estimating the number of four blocks, a real, space data set will need.
QuickStore is a permanent C++ storage system, built at the top of the EXODUS storage administrator. QuickStore allows applications to access objects quickly through normal virtual storage indicators. This article presents the results of detailed performance studies using the OO7 reference label. The study compares the performance of QuickStore with the latest implementation of the E programming language. QuickStore and E systems show two basic methods (hardware and software) have been used to implement durability in the object-oriented database systems.
We describe a compressed XML data tool, with data exchange and archive applications, which usually reaches the compression ratio of gzip approximately twice, the compressor called XMill integrates and combines the existing compressor to apply it to unusual XML data: it uses zlib, the library function is gzip, the data type of the compressor collects as a simple data type, the user may define the compressor to apply a specific data type.
In this article, we show how compression can be integrated into a relative database system. Specifically, we describe how storage manager, query execution engine, as well as a database system query optimizer can expand processing compression data. Our main result is that compression can significantly improve the query response time if using very light-level compression technology. We will introduce this light-level compression technology and provide running TPC-D reference results in such a compressed database and a non-compressed database using AODB database system, a experimental database system, developed at the University of Manhattan and Pasau. Our reference results show that compression actually provides high performance profits (up to 50% of IO and quality profits, etc. The CPU reference technology can be operated in such a compressed database and a non-compressed database using AODB
Statistics plays an important role in influencing a query optimizer produced program. Traditionally, the optimizer uses a base-based statistics built and recognizes the independence between properties while spreading statistics through query program. This approach can introduce large estimated errors that may lead to the optimizer choosing an ineffective implementation plan. In this article, we show how to expand a base-based based based based based based based based based based based based based based based based based based based based based based based based based based based based based
In the effective time relationship, each twin contains an interval character T, representing the effective time of the twin. The super layer merger between the two effective time relations determines the super layer gap of all the twin. Although the super layer merger is common, the existing division and indexing scheme is ineffective if the data includes the long-lived twin or if the gap crosses the division boundaries.
Oracle Symmetric Replication expands the unparalleled snapshot copying capacity of 0racle 7 to eliminate the main site or MW ownership restrictions imposed by the copying system. any sequence of copying tables can be updated at any time by any database storage sequence, the data copying configuration can be based on the read or updated subgroup of the subgroup of the subgroup of the subgroup of the subgroup of the subgroup of the subgroup of the subgroup.
QuickStore is a permanent C++ storage system, built at the top of the EXODUS storage administrator. QuickStore allows applications to access objects quickly through normal virtual storage indicators. This article presents the results of detailed performance studies using the OO7 reference label. The study compares the performance of QuickStore with the latest implementation of the E programming language. QuickStore and E systems show two basic methods (hardware and software) have been used to implement durability in the object-oriented database systems.
We describe a compressed XML data tool, with data exchange and archive applications, which usually reaches the compression ratio of gzip approximately twice, the compressor called XMill integrates and combines the existing compressor to apply it to unusual XML data: it uses zlib, the library function is gzip, the data type of the compressor collects as a simple data type, the user may define the compressor to apply a specific data type.
This paper is about the work of the multi-dimensional search tree and the issue of the investigation.We provide such methods of classification, we describe the relevant algorithms, we introduce performance analysis efforts, and ultimately list the future research direction.Multi-dimensional search tree and space access methods, generally designed to deal with space objects, such as points, line parts, polymers, polymers, etc. The goal is to support space queries, such as the closest neighbor queries (find all the cities in 10 miles from Washington BC), or range queries (find all the lakes on Earth, in the width of 30 and 40 degrees), etc.
Statistics plays an important role in influencing a query optimizer produced program. Traditionally, the optimizer uses a base-based statistics built and recognizes the independence between properties while spreading statistics through query program. This approach can introduce large estimated errors that may lead to the optimizer choosing an ineffective implementation plan. In this article, we show how to expand a base-based based based based based based based based based based based based based based based based based based based based based based based based based based based based based
In this article, we discussed the analysis issue of an indicator set based on an object to improve the overall system performance of the object-oriented database. Notes that the effect of two indicators can be directed to the contents of one of the indicators that may affect the benefits achieved by another indicator. This phenomenon is known as the indicator interaction. Clearly, the effect of the indicator interaction needs to be considered when an indicator set is built. The problem of the indicator selection is first proposed, the four indicator selection algorithms are evaluated by simulation. The effect of the two indicators in the search guide in the indicator selection algorithm is also studied by simulation results showing, in the indicator selection algorithm, the indicator object object object object object object object object object object object object object object object object object object object object
Processing a large set of digital multimedia data, commonly called a multimedia digital library, is a major challenge for information technology. Mirror DBMS is a research database system designed to better understand the types of data management required in the background of a multimedia digital library (see URL http://www.cs.utwente.nl/~arjen/mmdb.html).
The network is changing every aspect of our life, but no one field is experiencing rapid and significant changes, such as the way business operates, product promotion, or real-time automobile navigation and traffic information services. Today, large and small use the network to communicate with partners, connect to their backdrop systems, and carry out e-commerce transactions monitoring. The next chapter of the internet story is the evolution of today's e-commerce and e-commerce systems, in the way of "e-commerce and e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e-commerce" in the way of "e
In CAD, software engineering, real-time process control, enterprise storage libraries and digital libraries, advanced applications require building, effective access and management of a large, shared knowledge base. This knowledge base cannot be used by existing tools, such as expert system architecture, because these are not expanded, nor can be built in the sense of existing databases technology, because this technology does not support rich representative structures and reference mechanisms, which require knowledge-based systems. This paper proposed a general building, a knowledge-based management system designed for these applications. The building adopts an object-oriented knowledge representative language, used to express the limitations and the description of the rules. It also provides the general purpose of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description
Influence detection is an essential component of the computer security mechanism. It requires accurate and effective analysis of a large number of system and network audit data. There are several characteristics of audit data: rich raw data, rich system and cyber science, and always "flowing". Therefore, when developing data mining methods, we need to focus on: characteristics of mining and building, according to (general) algorithms, and optimizing the performance of the output model. In this article, we describe a data mining framework for the mining audit data of the input detection model we discuss its advantages and limits, as well as the open research issues.
Text data can be naturally divided into many databases on the Internet. Effective access to the required data can be achieved, if we can accurately predict the usefulness of each database, because there is such information, we only need to get the potential useful files from the useful database. In this article, we propose two new methods to estimate the usefulness of the text database. For a question, the usefulness of the text database in this article is defined as the number of files in the database, quite similar to the query. This usefulness measurement allows genius users to make a useful decision on which database to search for. We also consider collecting issues. Because the local database can use similar functions, different from the use of the global database, the use of the local database is limited to determine whether there is potential use in the local database.
Due to its expression capacity, conventional expressions (REs) are rapidly becoming a component of several important application scenarios of language specifications. Many of these applications have to manage a huge database of RE specifications and need to provide an effective matching mechanism, as a input chain, fast identifying search database of REs, suitable for it. In this article, we presented a new indicator structure for large database of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs
About the needs of management and the design of a complete "Information Refinery" separate standard so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery" so that the computer can reduce the data of "Information Refinery"
However, it remains a potentially expensive atomic operation. This paper introduces an algorithm to perform increased visual maintenance as a series of small, unsynchronous steps. The size of each step can be controlled to limit the process of adjustment and close to the physical visual or basic relationship while operating.
Active database systems can be used to create and implement data management policies. This trend is sometimes called "knowledge independence" a good consequence of achieving complete knowledge independence is that data management policies can be developed effectively by modifying the rules rather than the application. Active rules, however, can be quite complex understanding and management: rules react to voluntary events series, they stimulate each other, sometimes the rules processing results can depend on the order of events occurring or the rules are planned. Although in a large collection of rules is very difficult, the task becomes more managed rules are very few therefore, we believe in the development of applications, the modules can be developed through the application.
When a chart of an object-oriented database system is modified, the database must be modified in a way that the chart and the database remain consistent.This article describes the new upcoming algorithm that the database automatically achieves a consistent state after the chart is updated.
In this article, we presented a new framework for the dynamic distribution of query processing, based on the so-called HyperQueries, which are mainly query assessment subprograms "seat behind" hyperlinks. We show this distribution of query processing architecture in the background of the B2B electronic marketplace. Building an electronic marketplace as a database, by integrating all the data from all the involved enterprises in a concentrated library, will generate serious problems.
Due to the nature of the information on the network and the flexibility of XML-based queries, we expect many common issues related to XML-based queries to be semi-structured: data may be irregular or incomplete, its structure may change quickly or unexpectedly.This article describes Lore's queries processor, XML-based data's DBMS supports an expressive queries language.We mainly focus on Lore's cost-based queries optimizer.
The billions of SQL Database Management software markets are supported by national and international standardization efforts, and under the sponsorship of ANSI and ISO, the expert committee is actively striving to add objects to SQL.
When agents customize options they choose, for example, their resource consumption and production, and these options have an impact within the collective system scope, the best decision becomes a collective optimization issue called NP hard. In this challenging computing issue, the central management (deep) learning system often requires personal data with the influence of privacy and citizens' autonomy. This article introduces an alternative to uncontrolled and distributed collective learning methods they choose, for example, their resource consumption and production, and these options have an impact within the collective system scope, the optimistic decision becomes an efficient collective optimization issue called NP hard. In this challenging computing issue, the central management (deep) learning system often requires personal data with the influence of privacy and citizens' autonomy.
The application involves the timely distribution of data to a large number of consumers, and includes stock and sports icons, these servers of information systems, electronic-personalized information systems, electronic-personalized data needs, and entertainment delivering it. The application advances in wireless and satellite networks, as well as available wireless, high-band connections to the home, these applications include: a wide range, significant distribution in user data needs, these applications involve the timely distribution of data to a large number of consumers, these applications include the use of stock and sports icons, these servers of information systems, electronic-personalized data needs, electronic-personalized data needs, electronic-personalized data needs, electronic-personalized data needs, electronic-personalized data needs, electronic-personalized data needs, electronic-personalized data needs, electronic
Question size estimates are crucial for many databases system components. In particular, question optimizer requires effective and accurate question size estimates when deciding to alternate question plan. In this paper, we presented a new sample technology based on sample gold rule, introduced by von Newman in 1947 to estimate range question. The proposed technique used random sample frequency fields using accumulated frequency distribution, and produced good estimates, no one of the primary knowledge of the actual base distribution of space objects. Our experimental proves that the proposed sample technology provides compared to the Min-Skew-based based on based on based on based on based on based on based on based on based on based on based on based on based on based on based on based on based on based on
Panel Abstract This panel deals with a very important area that is often stored by databases systems, databases applications developers and databases designers, i.e. stored. We recommend notification, discussion and discussion of the use of "active storage sequels" in databases systems and applications. Through active storage sequels, we refer to a databases system that uses all storage media (i.e. optics, bands and discs) to store and receive data, and not just discs. We will review, discuss and discuss how to use active storage comparisons and/or complement the databases research community known as "active discs" [RGF 98] and other emerging discs storage models.
The database query optimizer needs to search for the selective estimates to find the most effective access plan. The query references to the same characteristics, we need a multi-dimensional selective estimate technology when the characteristics depend on each other, because the selective is determined by the characteristics of the common data distribution. In addition, for the multimedia database, there are internal requirements, multi-dimensional selective estimates, because the function vectors are stored in the multi-dimensional index tree. In 1dimensional cases, a histogram is actually the most popular. In multi-dimensional cases, however, a histogram is not suitable because of high storage and high error rate.
The database search engine is able to access and promote the database at WW. Jungle is a database search engine prototype developed at the University of Alberg. Connected to a remote database, Jungle extract and index the database and meta-data, building the database information. This information is used to evaluate and optimize the QUA query language query. AQUA is a natural and intuitive database query language that helps users search information without knowing how the information is structured.
Providing concept-level access to video data needs, video management systems adapt to data fields. Effective indexing and obtaining advanced access requires the use of domain knowledge. This paper proposes a method based on the use of knowledge models to build a domain-specific video information system.
Today’s world is through the popularity of information sources, such as WW, databases, semi-structure files (such as XML files), however, this information is usually dispersed, abnormal and weak structures, so it is difficult to automatically process it.DENODO has developed a intermediate system to build semi-structure and structured data integrated applications.
It is believed to become a universal format of data exchange database on the web page, and in the near future we will find a large amount of files in XML format on the web page. The result, it has become a key solution to the problem, the search database of a large collection of XML files can be classified and obtained by the documents of the IR seminar technology, so far, most work is about storing, indexing, query and searching the database seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar seminar
The paper tries to conduct a comprehensive study of the terminal detection of the distributed database systems. First, two main terminal models and four different distributed terminal detection methods in these systems are discussed. Next, a new terminal detection algorithm is introduced. The algorithm is based on the dynamic creation of the terminal detector (DDAs), each is responsible for the detection of the terminal detector in a connection component in the global waiting map (WFG) of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of the terminal detector of
The application server (ASs) has become a very popular meeting / evaluation method in the past few years, providing a platform for conducting transactions, server side applied in the online world. ASs are the traditional transaction processing displays (TPMs) of the modern descendants, such as CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a core role in order to conduct electronic transactions in the web background requirements. They are based on data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data-based data
Many of the information resources on the internet are mainly applied to limited geographical communities. For example, the site contains information about restaurants, theaters and apartment rental is mainly applied to the web users in geographically close to these locations. On the contrary, other information resources are applied to the wider geographical communities. For example, the web newspaper may apply to users across the United States. Unfortunately, the current web search engine mainly ignores the geographical scope of the web resources. In this article, we introduce the geographical scope of the web resources, the web resources based on text content, and the geographical distribution of these geographical links.
In this article, we describe some of the highly popular web design considerations, which are mainly the characteristics of reading or reading.
Traditional intermediaries will focus on the content of the source and their relationship with the user provided with an integrated viewpoint. They don’t consider the source’s ability to answer questions. This may lead to them producing a source issue plan that cannot be answered by the source. In the TSIMMIS system, we developed a source capacity sensitive program to generate modules, building a feasible user issue program, there is a limited source capacity
Rules based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule based on rule
A long time ago, we discussed the creation of a new part of the International Balloon Conference, the April standard, this XML related specification (SQL/XML added), in this column [1]. At that time, we mentioned the work, which has been completed as a "infrastructure" data type, SQL value is XML value. We are pleased to say that there has been significant progress, and SQL/XML value (XML value) is now undergoing the first formal phase of processing, the final committee draft (FCD) voting, in ISO/IEC data referring to the September data, we have discussed this project will be mentioned data. In our previous column, we described from the SQL comment "identification" to XML data type to XML data type, SQL data type to XML data type to XML data type.
In the data explosion online, consistent data access to various data sources has become a challenging task. In this tutorial will introduce themes of interest to the database researchers and developers: interactive intermediate software, portals, distributed unusual query processors, federal databases, data source packaging, intermediate and DBMS extension.
Recently, multiple query databases have been proposed to query information sources of two technologies, their data is not limited by a chart, or their chart is unknown. Examples include: LOREL (query data from multiple abnormal sources combination), W3QS (query world wide network); and UnQL (query model data is not structured data). Natural data model for these languages is, a simple, label chart shows a simple chart, we now chart, their main new things are, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one chart, from one ch
The ability to store, access and disseminate large amounts of data has changed the social, educational and government landscapes around the world.The development of technology, policy and computing economics has been seen by some governments as a means to make government institutions more respond to each other’s needs and citizens’ needs, and to make government actions transparent and responsible to citizens.
The current object database management system supports the user’s defined conversion feature to update the database once the chart has been modified.There are two main strategies that are possible when implementing the database conversion feature: instant or light database updates.In this article, we focus on the definition of the implementation strategy of the conversion feature, as a light database update.
This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. The fact that a software system must process and respond to continuous input from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area.
Information integration provides a competitive advantage for the business and is crucial in the computing of demand. This is the strategic field of software companies investing today, its objective is to provide a unified view of data, regardless of the differences in data format, data location and access interface, dynamic data management positioning to meet the availability, monetary and performance requirements, and to provide independent functions to reduce the burden of IT personnel managing complex data architecture. This paper describes the motivation for information integration to demand computing, explains its requirements, and describes its value through the use of scenes. As shown in the paper, there is still a large number of research, engineering and development work that needs to make information integration vision a reality, and the software company will continue to invest in information integration vision.
Through the "model", we mean a complex structure representing a design project, such as a relative program, object-oriented interface, UML model, XMLD, site program, grammatical network, complex file, or software configuration. Many models use involves the management model changes and data from one model to another conversion. These uses require a clear representation of the "map" between the model. We suggest that the database system makes it easier to use these applications, by creating the "model" and "model map" of the first class of objects with special operations to simplify their use. We call this ability management model. In addition to the model management case, our main contribution is a design model data model. These uses a clear representation of the "model" between the model we recommend that the database makes it easier to use these applications.
In this paper, we presented a solution that reduces these limits in DBS3, a shared memory parallel database system. This solution combines static data division and dynamic processor allocation to adapt to the implementation background. It makes DBS3 almost sensitive to the data division and allows the level of parallel division from the data division. To solve the problem of load balance in the presence of the data division, we analyze three important factors affecting our parallel implementation patterns of behavior: the division factor, the degree of division and the degree of division report.
Traditional query processors produce complete, accurate query results, whether in a combination or pipe model. We think this strict model is too strict to explore different types of query parts of data sources, such as in the Internet sources. On the contrary, we propose a easier query model, one of which the user submits a wide range of initial query output lines, the system continuously produces a part of the result map, which can contain the value of a part of the output field. Time user can view these parts of the results accumulated in the user interface, therefore, by describing our query model and user interface, we provide a easier query architecture, which is applied to the Telegraph system’s data flow system continuously produces a part of the result map, which can contain a part of the value, only a part of the
In the background of the object database, we study the update methods applied to the collection of recipients rather than a single. Clear strategy applied to the recipient one by one, in a kind of voluntary order, bringing the problem of order independence. On a very common level, we study how the update behavior can be analyzed in the sense of some charts, called color. We are able to describe those colors, always describe the order independent updates. We also consider a more specific model of update methods implemented in the relative algorithm.
We proposed a new data model and its language to survey an object-oriented database where objects can retain space, time or limit data, conceptually representing linear equality and inequality limits. The proposed LyriC language is designed to provide a unified and flexible framework for various applications, such as (1) based on limitation design in the second, third, or higher level of space, (2) large-scale optimization and analysis, mainly based on linear programming technologies, and (3) space and geographic databases.
Access control models, such as models supported by commercial DBMS, are not yet able to fully meet many applications requirements. an important requirement arises from the size of the license in many real world circumstances. the license is often limited for a time or may only hold a specific time period. In this article, we introduce a access control model in which regular time intervals are associated with the license. the license is automatically granted at a specific time interval and cancelled at the end of these time intervals.
Similarity combination is an important database primary, successfully applied accelerated application, such as similarity search, data analysis and data mining. Similarity combination combines two points groups of multi-dimensional vector space so that the result contains all points pairs, where the distance does not exceed a parameter ε. In this article, we propose Epsilon Grid order, a new algorithm to determine the similarity combination of very large data portfolio. Our solution is based on a particular type of order of data points, which is by placing a equal distance of the network with the cell length ε over the data space and comparing the grammatics of the network cells a typical problem based on network methods such as MSJ or εk-d-d-d-d-d-d-d-d-d-d-d
This article describes the Microsoft Repository’s version and work space features, a layer, in the top of Microsoft SQL Server implementing detailed mining objects and relationships, it supports the version of the division and merger, the Delta storage, the check check and the single-version view of the unconscious version of the application.
We believe that the maximum growth potential of soft real-time databases is not a isolated single database, but a component of an open system consisting of many unusual databases. In this environment, the flexibility of dealing with unpredictable situations and the ability to work with other databases (often non-real-time databases) is as important as the guarantee of a strict time limit. In this article, we describe a database designed specifically for an unusual environment, Stanford Real-time Information Processor (STRIP).
Currently, most database management systems are designed specifically for general use, and, of course, some commitments have been achieved to meet the most common users and the largest market.
Cubetree Storage Organization (CSO) 1 logically and physically integrated material viewing data, multi-dimensional index, and calculated integrated values, all in a small and narrow storage structure, using a component of traditional desktop space. This is a innovative technology, storing and accessing multi-dimensional data, from reduced storage, query performance and increased collective updating speed. CSO has been expanded to an active MultiSync controller to synchronize multiple continuous access and continuous irregular online updating a continuous data storage.
In many data-intense applications, the core is to quickly and accurately convert data into a new form of problem. Database researchers have long supported the process of using a statement query. However, the tool for creating, managing and understanding the data transformation requires complex query remains too primitive to allow broad adoption of this method. We introduce a new framework that uses data examples as the basis for understanding and improvement of the statement diagram map. We identify a group of intuitive operators to manipulate the examples. These operators allow users to track and improve an example through data sources. We show our operators are strong enough to determine a large category of diagram maps and effectively differentiate the diagram maps. These operators allow users to quickly rebuild a diagram map and transform it into a diagram map.
The strict performance targets faced by real-time databases (RTDBS) require the use of priority resource programming, which introduces a priority memory management (PMM) algorithm designed for programming queries in RTDBS, PMM tries to reduce the number of missed periods by adjusting its multi-programming levels and memory allocation strategies to meet the features of the work burden offered, a series of simulation experiments confirm PMM's receipt control and memory allocation mechanisms are very effective for real-time queries programming.
Web services are increasingly accepted as a framework for promoting application and application interaction, within the enterprise and cross-border. It is generally believed that a service description should include not only the interface, but the service supports the termination period of the business protocol. Current work focuses on an important category of formalization of protocols, which includes time-related limitations (known as time protocols), as well as the impact of time on compatibility and alternative analysis. We have formalized the following time limitations: C-Invoke limit defines the time window, one of which service operations can be called, and M-Invoke limit defines the termination period.
In the database literature, the classification problem is well known because it is applied in many issues such as customer classification, classification and trend analysis. Unfortunately, all known algorithms tend to be divided into high-size data mining applications, which are better classified due to the inner scalability of points. In such high-size space, not all sizes can apply to a specific classification. A method of processing is to choose the closely related sizes and find the classification in the corresponding subspace. Traditional characteristic selection algorithms try to this. The weakness of this method is that in the typical high-size data mining applications, different scalability scalability scalability scalability scalability scalability scalability scalability scalability scalability scalability scalability scalability sc
Space Index is one of the most active areas in recent database studies.The database literature presents several variables of the Quadtree and R-tree index.In this article we first briefly describe the implementation of our Quadtree and R-tree index structures and related optimizations in the Oracle space.Then we studied the relative values of two structures, such as implementation in the Oracle space, and compared the different types of queries and other operations.
In a variety of parking solutions that can help reduce parking problems or regulate parking activities, e Parking is developing and applying an innovative e-commerce application to optimize parking space. This paper aims to introduce the innovative e-commerce platform, from a technical point of view, by the University of Zurich. These ideas come from a cross-European Alliance, in the 5th Framework of the IST Information Social Technology Framework. E-Parking provides a database-based Web application solution, based on the concept model proposed by CIA (Channels, Integrated, Applications) for web applications. WAP, WEB and Bluetooth Communications Channels allow drivers to get early available information, parking, reservations, reservations, reservations, reservations, reservations, reservations.
Since the 1970s, business-oriented workflows have been conducted in a variety of names (office automation, workflow management, business flows management) and different communities, including the database community, with many basic and applied studies, such as workflow language and model theoretical studies (based on Petrinet or process calculation), its properties, trading behavior, and so on.
Adding strict security specifications to an application requirement list raises many new issues in the design and implementation of the DBMS, as well as the design, use and maintenance of the database. Strict security requirements, such as those that lead to silent hiding from the user's real information or introducing false information for query answers, also raises the basic questions about the meaning of the database and the related query language. In this article, we raise a trust-based grammatics of the use of the security database, providing a grammatics of the database that can "read" about the state of the world, or about their knowledge of the state of the world, to keep safe. This type of grammatics can be as a useful return recommendation for a "safe database model" for the "safe database" for the "safe language" for the "safe language"
In a previous article, we proposed a new method to produce alternative queries, using tracking (and returning queries) with logical limitations.The method combines the use of indicators, using materialist views, seed optimization and adding elimination (minimalization).Each of these techniques is known as useful for queries optimization.Our methods of innovation are to allow these technologies to interact systematically, for example, non-triple use indicators and materialist views can only be through seed limitations.
Explore search requires the system to help the user understand the information space and express the progress of the search intention to perform the unusual exploration and obtaining information. We introduce the interactive intention simulation, a technology simulating the user’s progress search intention and visualizing them as the keyword interaction. The user can provide feedback from the system learning and visualizing an improved intention of estimating and obtaining information. We report the experimental comparison of the system of the variables implementing the interactive intention simulation to a control system. Data includes search logs, interactive logs, paper answers, and questionnaires showing significant improvement in the performance of the task, re-evaluating the performance of the information, the performance of the information, the user experience.
This meeting has three workshops that use today’s wide range of storage equipment and the Internet to provide data with the power of transparency. First describe a product to manage a very large, multimedia database (petabytes even exabytes...
Effective management of multi-versions of data with branch evolution is critical for many applications, requiring databases designers to understand the gap between the index structure and policy, this paper defines a framework and analytical method to understand the behavior of different index policies, taking into account the data and query characteristics, analysis can determine the most appropriate index structure, analysis is verified through experimental research.
Statistical databases often use random data interference (RDP) methods to protect against the disclosure of secret digital characteristics.A key requirement of the RDP method is that they provide an adequate level of security for scanners trying to obtain secret characteristics through statistical conclusions.
InfoSleuth Project is developing and deploying technologies in MCC [7, 9, 8, 1] to find information in corp~ speed networks and external networks, such as networks on the constantly changing national information infrastructure, InfoSleuth is a Carnot technology previously developed by MCC [2, 6, 10], successfully used to integrate heterosexual information resources. The project has developed simulation technology that allows information resources to be described and advanced using agents to provide interaction between autonomous systems. InfoSleuth Project has studied Carnot technology used in a dynamically changing environment, such as the Internet, where no formal control, new sources of information and resource identity can be used in network applications.
The general use of the business disk is based on the database system, although widely used in the practice, failed to meet the requirements of short, predictable response time and extremely high input rate of the application performance. Main memory is the only technology that can these features. DataBlitz is a major memory storage management product, supporting the development of high performance and failure-resistant applications, requiring competitors access to shared data. In DataBlitz, the core algorithms of competition, recovery, index management and space management are optimized if the data is the memory resident.
A order depends on query is one of the results (interpreted as multiple) of changes, if the order's input record is changed. In a stock price database, for example, the recovery of all references about a database data on a date does not depend on the order, because the collection of references does not depend on the order. On the contrary, finding a stock of five prices moving average on the trading table gives a result, which depends on the table's order. The query language based on the relative data model can process the order depending query only through the add-on. SQL:1999 for example, there is a new "window" mechanism that can classify the data in a limited part of the query.
The requirements for the broad regional distribution of the database system are significantly different from the requirements for the local regional network system. In the broad regional network (WAN) configuration, a single site is usually to different system administrators, there are different access and charging algorithms, the site is installed specific data type expansion, and there are different restrictions on service remote requests. The typical last point is the production of trading environment, fully engaged in normal working hours and cannot bear additional burden. Finally, there may be many sites involved in a WAN distributed DBMS. In this world, a single program performs global query optimization, using cost-based optimizer will not work.
More and more databases are becoming accessible through the tables-based search interface, many of these sources are e-commerce sites. providing a unified access to multiple e-commerce search engines to sell similar products is very important, allowing users to search and compare products from multiple sites is easy. A key task is to provide such ability is to integrate these e-commerce search engines network interface so that user queries can submit against the integrated interface. Currently, integrating these search interfaces are manually or semi-automatically carried out, which is inefficient and difficult to maintain. In this article, we introduce the WISE integrator - a tool to perform automatically integrated network search engines products. WISE integrator's key task is to integrate these e-commerce search engines network interface, so that the user can submit the current interface.
Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based Based
Object-Oriented Database Management Group (ODMG) is an alliance of Object-Oriented DBMS suppliers who have developed a standard interface for their products, ODMG-93. This standard includes Object-Oriented DBMS's common architecture and definition, Object-Oriented DBMS's common object model, Object-Oriented language, Object-Oriented Object-Oriented language, and C++ and Smalltalk's standardized breastfeeding language connection. Object-Oriented DBMS (according to the definition of ODMG) provides programming language connection to the data structure with direct, transparent durability, opposed to the most embedded language used in the DBMSS.
In the last question we hear Jeff Ullman, the upcoming question will include dialogue this question interview with Gio Wiederhold took place in June 2001, a few days before the festival with Gio's retirement from Stanford University.
As the cost of hardware and software decreases due to the technological progress and the scale economy, the ownership costs of database applications are increasingly dominated by the cost of managing them. Databases grow rapidly in terms of scale and complexity, while skilled database administrators (DBAs) are increasingly less and more expensive. This paper describes IBM’s DB2 Universal Database® self-management or autonomous technology to show UNIX and Windows how self-management technologies can reduce complexity, help reduce the total ownership costs (TCOs) and improve system performance.
With the popularity of XML, it is increasingly common to classify data in XML format. This point highlights an important question: taking into account the XML file S and DTDD, how to extract the data from the XML data and build another XML file T similar to the T matched XML conversion? Let’s mention this as the DT matched XML conversion. The need for this is clear, for example, data exchange: Enterprise exchanges their XML file with some pre-data based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DTD based on DT
One type of query that is often encountered in the geographical information system is to find the closest neighboring object to a specific space point. Processing these queries requires a great difference from the search algorithm of the location or range query. In this article, we introduce an effective branch and connecting R tree cross algorithm to find the closest neighboring object to a point and then generalize it to find the closest neighbor. We also discuss an optimistic and pessimistic search order strategy as well as printing.
Most modern DBMS optimizers rely on a cost model to choose the best time query implementation plan (QEP) for any query. Cost estimates are highly dependent on the optimizer estimates, the number of errors statistics and cardinal estimates for a query implementation plan, which will lead to the complex query in each step of the QEP involves many forecasts and / or operations. These estimates rely on the statistical data and the accurate estimates of the simulated data, can or may not be more real data and the accurate estimates of the data. In this paper, we introduce the LEO, the DB2 learning optimizer, as a comprehensive way to correct the errors statistics and cardinal estimates of the query implementation plan by monitoring the previous query, the LEO optimizer.
Integrated access to unusual information is an increasingly important topic, as more and more resources, independently developing, become available through the network.The structure of information and the ability of resources to answer questions may be different.Therefore, the system is necessary to be able to use the knowledge of the content and the ability of resources to break down a part of a global problem, can deal with local and reintegrate answers.
To faithfully describe the application of real life, the knowledge base must manage the overall integrity limits. In this article, we analyze the methods to effectively verify the integrity limits on the basis of updating knowledge. These methods depend on meeting the integrity limits on the basis of updating knowledge before, to simplify their assessment on the basis of updating knowledge. In the past few years, more and more publications have been dedicated to the various aspects of this issue. Because they use different formalism and different terms, they are difficult to compare. In addition, it is often complex to recognize the universality, and find whether the techniques described in different articles are different in principle. The first part of the report is designed to provide a comprehensive state-art integrity verification. It is in the general form of verification, more and more publications have been dedicated to
The implementation of multi-database queries is different from the traditional queries in this type of fusion and mixed fusion is more popular, as smooth fusion requires repeated access to external data sources. Therefore, by the traditional (e.g., system R style) multi-database queries optimizer obtained by the left deep fusion tree response time is often low optimized, related to response time, due to a type of fusion (or mixed) fusion delay producing its final result, the subdivision fusion is done. In this article, we introduce an optimization strategy, first producing the best left deep fusion tree response time, then reducing the use of simple tree conversion. This strategy guarantees the minimum of resources, response time and optimization optimization optimization optimization optimization optimization optimization optimization optimization optimization optimization optimization
The Teradata Multimedia Object Manager is a general use of content analysis multimedia server, designed for the same multi-processing and large-scale parallel processing environment. Multimedia Object Manager defines and manipulates user definitions (UDFs) and is called in parallel analysis or manipulation of multimedia objects content. Multi-calculating intensive application, this technology uses large-scale permanent data sets, including fingerprint matching, signature checking, face recognition, and language recognition/translation.
Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database
The paper introduced a relative algorithm solution that provides primary support for the three characteristics of the sequential algorithm: screenshot reduction, extended screenshot reduction, and change storage.We introduced two temporary primacy, temporary divider and temporary adjustable, and defined the use of these primacy to reduce a temporary algorithm to the rules of the operators to their non-temporary opponents.Our solutions support the three characteristics of the continuity algorithm, through interval adjustment and time compression spread.We have implemented temporary primacy and reduction rules in the PostgreSQL core to obtain local database time processing data compression support and temporary adjustable include these primacy to reduce their non-temporary operators to their non-temporary operators.
Continuous media servers provide support for storing and receiving continuous media data (e.g., video, audio) is increasingly important in guaranteeing the speed. These servers are usually relying on multiple disks services to a large number of customers, so it is very easy to suffer disks failure. We have developed two errors tolerance methods, relying on receiving control to meet continuous media requests speed guarantee. The program allows data to receive from the disks to the required speed, even if a specific disk is failing. For both methods, we introduce data placement strategies and receiving control algorithms. We also introduce the design technology to maximize the number of customers, which can be supported by continuous media servers in the end, by extending, the effectiveness of our system.
In contrast, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa, vice versa.
In this column, I will introduce myself, commentary can be published in this section of the type of software reviews, and encourage other professional considerations to take the task of reviewing statistical software packages. I first started reading statistical software reviews in this magazine when I registered in my doctoral course in Biostatistics. I read some excellent reviews and found that they contain information useful in my own research, in the classroom project, as well as in collaboration work, I did. I was interested in writing software reviews while completing my paperwork and contact Dr. Richard Goldstein, the magazine’s software reviews editor.
The view is the core component of the traditional database systems and new applications, such as the database. The very common view (e.g., the transition closed) cannot be defined in the standard language of the database system. Unfortunately, the view can often be kept continuously through the standard language. For example, the transition closed detail charts and non-oriented charts can be kept relative after one-sided insert and deletion. Many such results have been published in the theoretical database community. The purpose of the survey is to make these useful results known to the wider database research and development community.
IBM’s SMART (Self-Management and Resource Adjustment) project aims to make DB2 self-managed, that is, automated, to reduce the total ownership costs and penetrate into new markets.After multiple publications, increasingly complex SMART features will make administrative tasks like initial implementation, database design, system maintenance, problem determination and system availability and restoration easier.
The current search engine cannot properly deal with the confusion predictions defined by complex preferences. The biggest problem with the standard SQL implemented search engine is that SQL does not directly understand the preference concept. The preference SQL expansion preference model is based on a strict part command (defined in the partner paper [Kie02]), where the preference query behavior is such as soft choice limit. The multi-integrated preference type and strong Pareto operators, combined with compliance with the statement of the SQL programming style, guarantees good programming efficiency. The preference SQL optimizer provides a high-level implementation reference SQL, including a high-level implementation reference preference rule of preference rule of preference rule of preference rule of preference rule of preference rule of preference rule.
In this article, some applications of the Fuzzy setting theory to information access are described, as well as recent research results in this field of the Fuzzy setting theory are applied to information access, its main objective is to define the flexible systems, that is, the system can represent and manage false and subjective, which is characterized by the information representation and access process, which is one of the main objectives of artificial intelligence.
In this article, we have studied by group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group
We display a visually-based XML relationship database system where XML data is managed by commercial RDBMS. the query interface allows users to visually expose the stored data on the basis of query. the data statistics and a special route directory are used to rewrite the query-based route expression to an effective SQL statement, which contains a few attachments.
Management of multiple versions of XML files is an important problem as many applications from traditional, such as software configuration control, to new, such as linking permanent web files. Research management of multiple versions of XML files seeks to provide effective and solid technology (i) storage and access, (ii) exchange, and (iii) query these files. In this article, we first show that traditional version control methods, such as RCS, and SCCS, do not meet these three requirements, and discuss alternative solutions. First, we strengthen the RCS’s temporary page classification policy to the objectives (i). Then we discuss a reference-based version system to the objectives (i) and (ii) and effective support in this article, we first indicate the traditional version control methods, such as RCS, and SCCS, do not meet these three requirements.
Internet-based communication defines two main types of services, the Pull and Push services, depending on the side of the transmission information request. Unlike the Pull services, its transmission request is launched by the customer, the Push services symbolizes a type of transmission, one of which the information exchange request is launched by the publisher or the central server. The new proposed alarm notification service (ANS) represents an important implementation of these Push services, for those who do not implement it or think it is not economically worth doing this site. In this article, we introduce the details of the system functions, the building model, design, and the basic modules integrated into a full-functional system as a service. The main contribution is to design a very extended solution to isolate cloud, and dynamic, differently compatible virtual machines.
Reorganization of objects in an object database is an important component of multiple operations, such as combination, combination and graphic evolution. High availability requirements for certain applications (operations 24×7) require reorganization to be carried out online, with a minimum of interference in the transaction simultaneously.
The space index plays an important role in rapid access to space and location data.The majority of business applications enter new data in size: in a collection or series.In this article, we presented a new size input technology for R-Trees, which is fast and does not damage the quality of the result index.We presented our experience by incorporating the proposed size input strategy into Oracle 10i.The actual data set experiment shows that our size input strategy improves the performance of input operations by 50% to 90%.
In the past few years, many data mining methods have been proposed to find useful and structured information from the market base data. The association rules model has recently been proposed to find useful patterns and dependence in these data. This paper discusses a method to effectively index the similarity search of the market base data. The technology is likely to be very useful in the application, using the similarity in the customer's purchasing behavior to make the competitors' recommendations. We propose an index called a signature table, which is very flexible to support a wide range of similarity functions.
In this paper, we presented the idea of the global optimization of history that the unique characteristics of the portfolio of attributes are collectively optimized in order to reduce the total errors in the use of history. The idea is to allocate more history to historical credits, their properties are more frequently used and distributed.
Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database
Our experience with SIFT [YGM95] information transmission system (used daily by more than 7,000 users) has established an important and general transmission problem: copy information. In this article, we explain why copy information appears, we quantify the problem, we discuss why it reduces information transmission. Then we propose a copy removal module (DRM) for a information transmission system. removal copy information operates on each user, based on a file, each user read the file generates a requirement, or copy restrictions. In a broad environment, the number of restrictions of processing is very large. We consider implementing DRM, check the alternative and the structure of the data can be used to provide us with a replacement performance and responses: important design of the copy:
The problem got significant attention because of its relevance, various data management issues, such as data integration, query optimization, andining physical data independence. So far, the performance proposed algorithms got little attention, especially, their scale in the presence of a large number of views is unknown. We first analyzed two previous algorithms, Cook algorithms and reverse rules, and show their disadvantages. Then we described the MiniCon, a new algorithm to find the maximum re-written connector, using a connector connector connector can get little attention, especially in their presence of a large number of views are unknown.
Making the database system active means developing an expressive event specification language, with a precise definition of sequence, the algorithm of complex event detection, as well as the architecture of an event detector and its implementation.The Thii paper introduces the sequence of complex events, using the concept of global event history (or global event log).
Charles Schwab and Co, Inc. is a major web trader that generates a large portion of the income from the webpage. this income is based on having a website with a lot of useful facilities, as well as the execution speed, capable of dealing with the peak of demand, as well as the reliability of the website and its basic services.
Multi-level relationships, based on the current multi-level security (MLS) relationship data model, can introduce a user with difficult to explain information and may show an incoherence of the views of other users. This incoherence is due to the lack of a comprehensive approach to claim and interpret the beliefs of low-level information. In this article, we identify different beliefs that can keep the advanced user of low-level information, we introduce a new concept of microscopic double. We introduce a mechanism to claim the beliefs of all available double, including low-level double.
When implementing persistent objects on a relationship database, a major performance problem is predicting data to minimize data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data
The recent results of the Michigan University Rio Project show that it can create a major memory area that is safe, such as a disk from an operating system failure. This paper explores how to make reliable memory of the I/O interface provided by the Rio file stored into a database system. Previous studies analyzed the performance benefits of reliable memory; We focus on different designs how to influence reliability. We suggest three designs to integrate reliable memory stored in the database: unreliable database stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stored stor
The objective of our work is to provide a DBMS data model and query language capable of processing such time-dependent data models, including those constantly changing data models, describing mobile objects. Two basic abstracts are moving points and mobile areas, describing objects, only taking into account time-dependent location, or location and range of data of the system, respectively, providing a data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data
One of the key disadvantages of SQL is the lack of support for n-dimensional based calculations, which are common OLAP environments. relative OLAP (ROLAP) applications must simulate them using plugins, recently introduced SQL window features [18] and complex and ineffective CASE expressions. in SQL specified calculations are SELECT conditions, which are extremely restricted and forced users to produce queries through existing views, questions and complex plugins. In addition, SQL query optimizer is a pre-working to determine effective plugins commands and choose the best access method, and largely ignoring the complex digital formula optimization.
The chart evolution is a problem facing long-lived data. When a chart changes, the existing permanent data may become unavailable unless the database system provides the mechanism of access to the data created in the previous version of the chart. Supporting the chart evolution system most existing systems focus on changing locally to the individual type in the chart, thus limiting the changes, the database maintenance can be carried out. We have developed a model of type change involving multiple types. the chart describes the two types of changes and their impact on the data, by defining derivative rules to launch new data based on existing data.
We discussed a number of important issues, especially web cache, for the dynamic content used from database applications, and we introduce the technologies used by Oracle Web Cache to solve these problems, including: information-based content breakdown, in addition to URL, transparent meeting management, some page cache to be personalized, as well as a wide range of failure and performance guarantee effects.
Many researchers have studied the division of transactions into smaller parts to improve competitiveness. The research is usually focused on the implementation of the division provided by the database application developers, relatively less focusing on what constitutes a desired division, and how developers should obtain such division.
Databases allow quick online analysis of large databases, which is attractive in many applications. Despite several available cubic-based OLAP products, users may still face the challenges of exploring the efficiency and efficiency of the big databases as well as the big computing space of the databases. CubeExplorer is an integrated environment of the online exploration of the databases. It integrates our recently developed technology to the ice cream library calculation [2], based on the function of the cubic extract and gradient analysis [1], and makes the function of the cubic exploration efficient and efficient.
Ausgehend von einem Schemaentwurfsprozess für Data-Warehouse-Systeme werden aktuelle Entwicklungen zur Qualitätssicherung und Data-Warehouse-spezifische Qualitätskriterien wie Summierbarkeit sowie deren Verallgemeinerung zu mehrdimension Normalformen und Selbstwartbarkeit vorgestellt.
The main challenges of web mining are: (a) accurate tracking of data (because not all information is to the web server), (b) real-time acquisition of large amounts of data (4.5 million visits to Yahoo, 2 to 4GB of click flow data per hour), (c) real-time data interpretation without damaging the user's privacy (a few seconds of location and target information), (d) data visualization for policy-making.
The multimedia description standard MPEG-7 is an international standard since February 2002, it defines a huge combination of multimedia content, its creation and communication description classes. This article studies the MPEG-7 on the multimedia database system (MMDBS) and vice versa.
Microsoft.com is the world’s largest business website, in terms of website visitors and services, in general, it’s the fourth largest website for total visitors behind the U.S. online, Yahoo and web pages, we offer 250,000 pages of content that can be viewed in all major browsers (yes, we actively support the web pages), supported by three server farms internationally, and contains three hours, seven days of content updates a week.
Based on this index problem, most systems use the use of B+ wood implemented class index (CH) technology. Other technologies, such as[14,18,31], can lead to improving the average case performance of CD, but involve the implementation of the new data structure's basic size. As a special form of external dynamic two-dimensional range search, OODB index problem can be solved within the reasonable worst range of situations. Based on this view, we developed a technology called class index (CH) technology. Other technologies, such as[14,18,31], can be used as a practical instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance of instance
According to Michael Stonebraker's experience, if you choose a group of familiar database research experts and ask them to determine the field of using + requirements is critical, while requiring more significant progress, the user interface appears first in the list. This has been the last decade. Despite such a strong motivation, user interface research seems to be marginalized in the database community. Part of this community believes it is a development field rather than a research field. Researchers often feel the user interface specifications are less than collecting widgets in a certain order.
Specific requirements for the data mining application setting the classification algorithm include: the ability to find the classification embedded in high-size data subspace, scalability, the final user understand the results, not assuming any channel data distribution, as well as the insensitivity to the order of the input record. We introduce CLIQUE, a classification algorithm that meets each of these requirements. CLIQUE determines the intense classification in the maximum size of the subspace. It produces the classification description of the form of DNA expression so that it produces the same result, regardless of the input record order, and does not assume any specific mathematical form of data distribution.
The query optimizer will typically integrate the query into a best plan by taking full knowledge of all the cost parameters, such as selective and resource availability. The implementation of these programs may be low optimized when the cost parameters are ignorant compilation time or significantly changes between compilation time and running time [Loh89, GrW89]. The parameters query optimization [INS+92, CG94, GK94] optimize the query to a number of candidate programs, each of which is the best of a particular area of the parameters space.
You have many days on your head of the database technology, but you don't know where you hear the traditional melody or what song it is from. This presentation will show a query by the Humming system, which will tell you the name of the song. Most of the database studies in the pre-existing query by the Humming system using the string to match the similar melody system (e.g. [1]). the user's melody is converted into a series of different melody systems, the user's melody is converted into a series of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody of different melody
We provide an information access system, while allowing the search for text and speech files. Get the system to receive false queries and perform the best matching search to find the relevant documents. Get the system output is a list of ranked documents, in which the document on the top of the list is best to meet the user's information needs. The relevance of the document is estimated by generating data (document describing vectors). Generating data is automatically generated, it is organized so that the queries can be effectively processed. We introduce a controlled index word, both speech and text file. The new index word size is small (1000 characteristics) compared to the standard text index word size (10000 characteristics We show, based on the short-number indication, the short-number indication is similar to the short-number.
Recently, Haas and Hellerstein presented the Hash Ripple fusion algorithm in the background of online fusion. Although the algorithm quickly provides good estimates for many fusion problems cases, the fusion can be slow, if the double number of fusion forecasts is small or if there are many sets in the output. In addition, if memory transition (for example, because the user allows the algorithm to run to complete, to obtain accurate answers), the algorithm degrade to prevent Ripple fusion and performance suffering. In this article, we build on the work of Has and Hellerstein and propose a new algorithm if (a) the fusion speeds match the sample and ensure (b) the fusion speeds are good performance if the memory transition (for example, because the user allows the fusion to perform accurate ans
The Knowledge and Database Systems (KDBS) Laboratory was founded in 1992, at the National Technical University of Athens, which is internationally recognised and proves its participation as a center nod in the Esprit network of excellence IDOMENEUS. Information and data open media for users of the NTWORKS, the project aims to coordinate and improve European efforts in the development of the next generation of information environment, which will be able to maintain and spread a wide-ranging information class in an open media set.
We mentioned the need for water labelling databases to stop their pirates, identify the unique characteristics of the water labelling data, which poses new challenges and provides the necessary characteristics of the water labelling system for the water labelling data.
Abstract Many modern programming languages support basic genes, enough to implement a type of safe diversified container. Some languages have moved beyond this basic support and in doing so enables a broader, stronger form of gen programming. This paper reports on a comprehensive comparative facility of gen programming in eight programming languages: C++, standard ML, target Caml, Haskell, Eiffel, Java, C# (with its recommended gen expansion), and Cecil. By implementing a significant example in each of these languages, we explain how the basic role of gen programming can be represented in each language. We also determine eight language properties support this wide view of gen programming: Multi-concepts of genes, can be supported by multiple genes, can be supported by multiple genes.
Peer-to-Peer (P2P) systems are increasingly popular because they allow users to exchange digital information by participating in complex networks. These systems are cheap, easy to use, high-scale, and do not require central management. Despite their advantages, however, limited work has been done using the database system at the top of the P2P network. Here, we recommend PeerOLAP architecture supports online analysis processing queries. A large number of low-end customers, each containing a password with the most useful results, through a voluntary P2P network connection. If a query cannot be described as local (i.e. through the use of the computer’s password content, it is issued), it is through the network to a password that the password can be found a password answer in the line processing of the query query.
The grass (grass) is the main flow of the spinal grass in the Finnish Canadian Poria Forest, and the high population density has caused concerns about the potential negative effects of the ecosystem functions and properties, including biodiversity and wood production. We use 31 grass to eliminate in Norway to study how the forest develops clearly cut or no grass exists. We tested how the tree population, rich plant function groups, community composition and plant diversity (including Brooklyn) in the multi-level grass elimination, was eliminated for seven years, by biodiversity and we used the grass to eliminate the forest.
User Definition Collections (UDAs) provide a diverse mechanism to expand the power and availability of the object-related database (O-R DBs). in this article, we will describe the AXL system supports SQL-based language to introduce the new UDA. AXL is easy to learn and use database programming as it retains the structure, programming patterns and data types of SQL (because there is a "blocking errors" of the SQL and the programming language user definition feature currently used in O-R DBs.
Performance standards play an important role in relatively DBMS, object-related DBMS, data storage systems, and we believe that the data mining algorithm is a long-term task, and it will also play an important role in the research and development of data mining systems.
Many decision supports and continuous reference systems operate is the concept of the "influence" of the data points on the database, this concept arises from examples such as by opening a new store export location influence customer group, informing subscribers to the digital library who will find the new file added the most relevant, etc. Standard methods to determine the data point influence group involves the range of searches and the closest neighboring queries.
In short, the separation of the database is the variable in which the separation may appear in the head of the rule; the advanced version also allows the denial within the body, which can be based on the separation of the logic programming the separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation of the logic separation
On the contrary, the progress proposed in the high-speed network and multimedia properties system has made time control customers able to provide users with video on the request (VOD) service. However, it remains a challenging task to design a cost-effective VOD system, as well as the network bandwidth, which can support a large number of customers (who may have different service quality (QoS) requirements) and at the same time provide different types of VCR features. Although it has been recognized, the VCR system of space and space of space and space of space and space of space and space of space and space of space and space of space and space of space and space of space and space of space and space of space and space and space of space and space and space of space and space and space of space and space and space and space and space of space and space and space and space of space and space and
The quality of the implementation plan created by the query optimizer is related to the accuracy of its core estimates. Errors in estimates lead to poor performance, wrong behavior and user disappointment. Traditionally, the optimizer is limited to the use of tables-based statistics and derived estimates at the bottom. This method has disadvantages with processing complex queries and with rich languages such as SQL: error growth, because the estimates are at the top of the estimates, some structures are just not processed. In this article, we describe the creation and use of statistical data views on the SQL server, providing the optimizer with the statistical results of the size or relationship expression. It opens a new size of available data card and in the evaluation, we describe this optimization program by means of optimization and optimization of the number of
Today, more than 80% of the information published on the web is produced by the base of data (every access through the web portal uses the tables as a query language and HTML as a display tool) and this proportion continues to increase. but the web data source is also coded by separate HTML pages by personal hands, providing very useful information, such as comments, digestion, links, etc. For information, also exists in the base of data, the HTML interface is often the only available many will become customers.
The real world data is frequently dispersed, so effectively calculating the database in large dispersed relationships is important. We show that the current computing database in distributed relationships technology is not large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large large
XML has become a universal format of data exchange between applications. Recently, the appearance of Web services as a standard means of data publishing and access in the Web introduced a new category of XML files, which we call performance and security considerations. These files are XML files, some of which data is clearly provided, while other parts are only deliberately defined through embedded calls to Web services. When these files are exchanged between applications, there is a choice whether or not to materialize the intensity of data (i.e., reference to embedded calls) before the file is sent. This option can be affected by different parameters, such as performance and security considerations. This article solves the problem of the process we think - similar to the usual XML data diagrams and the actual XML diagrams, which can be used for the exchange of specific data, we should
We study a SQL View continuous maintenance problem facing database updates and show it is possible to reduce the total time cost of view maintenance by materializing (andining) the additional view solution, we raise the problem to determine the best group of additional view defined as an optimizing problem in the possible view set space (including the empty set). optimizing the problem is more difficult than query optimization, as it has to deal with multiple view sets, update multiple relationships, and many ways to maintain each view set for each update relationship. we develop a memory solution for this problem; the solution can implement using the expression DAG representative use of the rules based on optimizer, such as Volcano.
RTMonitor is a real-time data management system for traffic navigation applications. In our systems, mobile vehicle starts limited time navigation requests, RTMonitor calculates and communicates the best path to the customer based on the route network and real-time traffic data, we have designed a two-level traffic chart to organize real-time traffic data support requests. In the framework, the application can maintain the temporary consistency of traffic data. In order to minimize the maintenance of real-time data excess, RTMonitor uses a collaborative and distributed method, using mobile agent, can significantly reduce the communication volume and improve the system’s scalability. In order to minimize space and information excess, we have designed a two-level traffic chart to organize real-time traffic requests in the application framework, PLLO can be used.
For years, you’ve been listening and reading a new standard, all known as SQL3. designed as a major improvement to the current second generation of SQL standards, commonly called SQL-92 because it’s the year of its release, SQL3 was initially planned to release around 1996... but things didn’t go as planned.
Structured files (e.g., SGML) can benefit a lot from the database support and the more specific object-oriented database management system (OODB), which describes the natural map of the SGML file to the OODB and the formal expansion of the two OODB query languages (one similar to SQL and the other calculation) to obtain the SGML file.
Ranking queries produce the results of the order on certain calculations of the score. Usually, these queries involve joining, users are usually only interested in the top joining results. The current relationship queries processor is not effectively processing the ranking queries, especially when joining participation. In this article, we discuss support of the top joining queries in the relationship queries processor. We introduce a new ranking joining algorithm, using its input of personal orders to produce the order joining results in a user-defined score function. The idea is to gradually arrange the result joining during the joining operation. We introduce two physical queries operators based on the variable joining Ripple, implementing the ranking joining algorithm. Operators are non-blocked, can integrate to the implementation of the project solution We use the different ranking
Complex similarity queries, i.e. multifunctional multi-object queries, need to express the user's information demand for a large multimedia storage library. Even if the user initially published a single object queries a feature, the relevance feedback system will automatically produce a complex similarity queries. Relevance feedback is only useful if the response time is interactive. Therefore, this article helps important questions how to effectively evaluate this complex queries. We describe a new evaluation technique called the VA file-based General Search (GeVAS). It is built in the VA file(27), supports multiple functional types of queries, and loans an indexing structure with multiple objects queries in parallel Ciaccia etc. Our relevance feedback is useful.
1. motivation Internet search engine has been popular based on keywords search. although the relative database system provides powerfifl structured query language, such as SQL, does not support keywords search database. as a query pattern of keywords search simplicity provides compulsory value of data exploration. in particular, keywords search does not require a preliminary knowledge of the chart. above is significant because a lot of information is increasingly available in a company on its internal network. however, it is unrealistic expected users who will browse and query such information with detailed knowledge of the chart available database.
The popularity of text databases in large organizations and on the Internet makes it difficult for a person to know which databases to search for. considering the language model that describes each database content, the database choice algorithms such as GIOSS can be provided by automatic selection of the appropriate database.
We are interested in defining and query views in a huge and very unusual XML storage library (Web size). in this context, the definitions are very large, involving many sources, and there are no clear definitions of language, their size limits. This causes interesting questions that we solve in the paper: (i) how to distribute views on multiple machines without having a negative impact on the query translation process; (ii) how to quickly choose the relevant part of a query views; (iii) how to reduce the cost of making a potential query to the machine.
Network services are the unified reality of the modern network, with a huge, day-to-day computing task impact. They make the network the largest, most popular, the most active distributed computing platform. However, the use and integration of network services or applications, which is a very sensitive and conceptually unprecedented task, still does not release its full strength. A unified analytical framework, promoting the basic understanding of the network services composition of the block building concepts, models, languages, productivity support technologies and tools are necessary. This framework is necessary to effectively explore, understand, evaluate, compare, and choose the service composition of models, languages, technologies, platforms and tools.
In this sample, we set a prototype of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrator of data administrat
Adding strict security specifications to an application requirement list raises many new issues in the design and implementation of the DBMS, as well as the design, use and maintenance of the database. Strict security requirements, such as those that lead to silent hiding from the user's real information or introducing false information for query answers, also raises the basic questions about the meaning of the database and the related query language. In this article, we raise a trust-based grammatics of the use of the security database, providing a grammatics of the database that can "read" about the state of the world, or about their knowledge of the state of the world, to keep safe. This type of grammatics can be as a useful return recommendation for a "safe database model" for the "safe database" for the "safe language" for the "safe language"
The astronomical growth of astronomical data enhances the demand for extensive RDF management strategies. While cloud-based systems provide a rich platform for managing large-scale RDF data, these systems provide shared storage introduced several performance challenges, for example, disks I/O and the top of the network scanning. This paper studies SPARTI, an extensive RDF data management system. In SPARTI, the distribution of data is based on the combination pattern found in the query work. Initially, SPARTI vertically performs the part, then in the reality, the number of RDF data and the number of national data compared to the number of these systems provide shared storage provides several performance challenges, for example, the I/O disks and the top of the network scanning.
The rapid progress of the network and internet technology has led to the appearance of the "software as a service" model of corporate computing. Successful business feasible software services examples include lease distribution tables, e-mail services, general storage services, catastrophic protection services. The "database as a service" model provides users with the power to create, store, modify and obtain data from anywhere in the world as long as they have existing access to the internet. It introduces several challenges, an important issue is data privacy. In this context, we specifically deal with data privacy issues. There are two main privacy issues. First, the data owners must ensure that data stored on the service provider's website is from external data protected. Second, even the need to protect data from the service provider, if they have effective access to the internet it also introduces a
We consider the problems of computing aggregation queries in temporary databases and ofining materialized temporary aggregate views efficiently. The latter problem is challenging since a single data update can cause aggregate results to change over the entire time line. We introduce a new index structure called the SB-tree, which incorporates features from both segment-tree and B-tree. SB-tree support fast lookup of aggregate results based on time and can beined efficiently when the data changes. We extend the basic SB-tree index to handle cumulative (also called moving-window) aggregates, considering separatcases when the window size is or is not fixed in advance.
Supporting virtual state and delta is useful for a variety of database applications, including the assumption of database access, version management, simulation and active database. The Heraclitus paradigm increases the delta value is "first-level citizen" in the database programming language so that they can be clearly created, accessed and manipulated. A basic issue involves the exchange between the "precision" or "intensity" form of delta representatives, and easy to access and manipulate forms. In a end spectrum, the code blocks can be used to represent the delta value, leading to more accurate capture of intentions of updates, cost more expensive access and manipulation.
Enabling users to use simple keywords to access the database allows users to get rid of the mastery of a structured query language and understand the complex and likely rapidly developing data curve. In this tutorial, we provide an overview of the most advanced techniques for supporting structured and semi-structured keywords search, including query results definitions, ranking functions, results generate and top query processing, cutting produced, results classification, query clean, performance optimization and search quality assessment.
The Teradata Multimedia Object Manager is a general use of content analysis multimedia server, designed for the same multi-processing and large-scale parallel processing environment. Multimedia Object Manager defines and manipulates user definitions (UDFs) and is called in parallel analysis or manipulation of multimedia objects content. Multi-calculating intensive application, this technology uses large-scale permanent data sets, including fingerprint matching, signature checking, face recognition, and language recognition/translation.
Active database system support mechanisms enable them to automatically respond to events occurring within and outside the database system itself. significant efforts are aimed at improving the understanding of these systems in the past few years and presented many different recommendations and applications. This high-level activity does not give a consistent standard approach to integrate active functions with the traditional database system, but leads to a better understanding of active behavior describing language, implementing models and buildings. This survey introduces the basic characteristics of active database system, describing a collective representative system within a common framework, taking into account the consequences of the implementation of certain design decisions and discussing the development of active application tools.
Currently, genetic expression data is being produced at the rate of the phenomenon, and the overall goal is to try to better understand the function of cell tissue, in particular, a specific goal is to link genetic expression to cancer diagnosis, forecast and treatment, however, a key obstacle is the availability of the tool or its lack, which prevents the use of data, making it difficult for cancer researchers to analyze effectively and effectively.
The previous version of the OnLine DSA server has demonstrated the effectiveness of high-performance parallel data query (PDQ) technology on the SMP system. The key component of the PDQ is the multi-layer process grOUpSi desktop division, pipe hash division operator, light-level access method, and the parallel resource management onLine XPS extends to the core PDQ technology from the SMP system to the large-scale parallel group SMp or unprocessor system.
Time, space and space time queries are essentially multidimensional, combined with clear characteristics of prophecy with time(s) and space(s). in literature, there is a lot of confusion about access methods because there is no consistent comment to mention these queries. as a contribution to solving this problem, we submit a new and simple comment space time queries. the comment is designed to solve based on the choice of space time queries, usually in the literature of access methods. the comment is extensible, can be applied to a wider multidimensional queries based on choice.
From the mid-1980s to the mid-1990s, a series of research activities were conducted in the field of database drivers and restrictions, observing the development of many research recommendations and prototypes, followed by the majority of the main database products supporting restrictions and drivers, expressive restrictions specifications appearing in the SQL-92 standard, as well as restrictions and drivers in the SQL-99 standard.
Data mining in big databases is increasingly important. supporting this trend, we consider a range of building alternatives to combine mining with databases systems. These alternatives include: through a SQL processor interface to do separate mining; mining a mining algorithm in a storage program; mining data to a file system in flight and mining; closely mining, using mainly user-defined features; and SQL applications to be processed in DBMS. We thoroughly study the expression of mining algorithm options in the form of SQL query using association alternative mining as a case. We consider four options in SQL-92 and six options in SQL extended optimized objects (SQL-OR our conversion method as a conversion method of conversion algorithm).
Many business databases keep histograms summary of the content and allow effective estimation of the size of the query results and the cost of the access plan. Although in the past several types of histograms have been proposed, there has never been a system study of all the histogram aspects, each aspect of the options available, as well as the impact of these choices on the histogram efficiency. In this article, we offer a histograms of taxonomy capture all the previously proposed histogram types and show many new possibilities. We introduce new choices, several taxonomy sizes, and produce new histogram types by combining the choices in an effective way.
This technical memorandum shows how to combine some known techniques to create a method that will effectively perform the common multi-table merger. We focus on a common type of merger, called a star merger, although the proposed method will be generalized to any type of multi-table merger. A star merger is made by a central detail table, with a large cardiacity, such as a order table (where a order line contains a single purchase) with a foreign key, will be merged to the description table, such as a customer, product, and (sales) agent. The method mentioned in this memorandum uses the merger index with a compressed bit graph representative, allowing forecasting the limitation of the description of the list to determine the answer (or setting) details; using different methods in the description of the table (where a order line contains a single order one).
Superprogramming is a technology that is only in the permanent system, because the superprogramming source code contains text and links to the permanent object. a superprogramming system has been in the permanent programming language Napier88. here we report the technology transferred to an object-oriented platform, Java. the component technology requires superprogramming including language reflection, a permanent library, and a browsing mechanism, all of which are already elsewhere.
XML has become the most relevant new standard for data representation and exchange on the WWW. XML content extraction and reorganization of the novel language has been proposed, some of the traditions of the database query language (i.e. SQL, OQL), other more inspiring XML. No XML query language standards are not yet established, but discussions are carried out within the World Wide Web Alliance and many academic institutions and large companies related to the Internet.
We witnessed the rapid increase in the number of structured information sources used online, especially in WW. These sources include the business database of product information, stock market information, real estate, car and entertainment. We want to use the data stored in these databases to answer complex queries, beyond keyword search. We face the following challenges: (1) multiple information sources store interrelated data, and any query query system must understand the relationship between multiple query plans. (2) many sources are incomplete databases system, and only answer a group query their data (e.g. in WWW's table description query can (3) because the data stored in these databases is very large, the technology is necessary to make it possible in WWW's query query query query query query query query query query query query query query query qu
Any subsidiary structure, such as a bit map or a B+ wood index, referring to a chart of a sequence stored as a basic B+ wood (e.g., chart with the index of the index in Microsoft SQL Server, or index of the chart of the organization in Oracle) through their physical address will need to be updated because of the corresponding chart volatility. to solve this problem, we recommend a chart mechanism so that 1) introduce a single chart and a similar chart of the chart and a similar chart of the chart and a similar chart of the chart and a similar chart of the chart and a similar chart and a similar chart and a similar chart and a similar chart and a similar chart and a similar chart and a similar chart and a similar chart and a similar chart and a similar chart and
In this article, we propose an effective direct and indirect file transfer protocol (C2CFTP), transferring files to the client server system. The existing file transfer method uses an indirect transfer method through the server transfer files between the client or a direct transfer method, connecting a direct data channel between the client. However, in the case of indirect transfer, unnecessary file input/output (I/O) is required by the server, in the case of direct transfer, a problem occurs in the file transfer delay due to the channel management cost increased.
There are two key motivations for this work. First, the implementation of the object-oriented database has increased to a significant amount. Second, there is a need for integrated access to information from multiple data sources. Multi-database systems have been proposed as a solution for integrated access to data from multiple distributed, abnormal and independent database systems. To introduce its users to a single database illusion, Multi-database systems maintain a single global database chart, which integrates all the components of the database chart, its users will send queries and updates.
This paper introduces a temporary database of time-based merger strategies, where time is represented by time intervals. Recommended method of map time intervals to point in a two-dimensional space and sub-space. The temporary relationship is divided into a map-based merger. Therefore, when two temporary relations will be mergered in time properties, one merger in one relationship only needs to be compared with the other relationship of a forecast combination of merger.
In addition to promoting queries on the web, XML queries language can also provide a high-level structure for useful facilities that currently do not exist in the traditional DBMS. In particular, the current DBMS queries language does not allow queries to produce unusual results in the database object type. This paper encourages the usefulness of unusual queries in the traditional DBMS and studies the emerging standards of XQuery, XML queries language to express these queries.
We introduced a new combined algorithm, called Ripple Joins, to process multi-table integrated queries online in a relative database management system (DBMS).
The paper introduces a series of new technologies to parallel geospatial databases and discuss its implementation in Paradise Object Relationship Databases, the effectiveness of which has been demonstrated through multiple complex geospatial queries in a global geospatial data set of 120GB.
Extended Trading Model (ETM) is a powerful mechanism to ensure the consistency and reliability of complex business applications. However, in J2EE, there is a few implementation of ETM. The existing research lack the support range, requires some special database support. This paper explores the obstacles that J2EE supports ETM, and believes that this is due to the limitations of the XAR resource interface and the basic database of J2EE. In order to overcome the obstacles, we have proposed a new method to deal with competitive control within the J2EE application server rather than the database.
The response time is the key difference between e-commerce (e-commerce) applications. For many e-commerce applications, the web page is based on the current state of the business storage of the database system. Recently, the web acceleration topic for the database-driven web application has attracted a lot of attention, both in the study of the community and the business stage. In this article, we analyze the factors that influence the performance and extensible web application. We discuss the system architecture issues and describe the method of deploying cache solutions to accelerate the web application.
The long-term unification of the application development process is now happening, due to the development of objective technologies and standards, which will (progressively) change the way data enhances the application development, reduce the outstanding of the database in this process, and change the data management goals and participants. at the same time, the database community needs to ensure its experience is exploited, its concerns are met in the new methods and tools set. We discuss these issues and describe how they are applied to the Department of Defense (DOD) part.
Flexibility is very hopeful stream system guarantees low delays on the work load dynamics, such as increased arrival rate and data allocation volatility. The existing system achieves flexibility using the resource center methods, allocating key in parallel examples, i.e. executives, balancing work load and scale operators. However, this operating level allocating requires global sync and prohibits rapid flexibility. We propose an executive center method to avoid operating level key allocating and implementing executives as building flexibility blocks. Through this new method, we design a flexible framework with two levels of optimization: i) implementing new executives, i. physical executives to implement flexibility to implement flexibility to implement, to implement flexibility to implement, to implement flexibility to implement, to implement flexibility to implement, to implement
Over the past few years, Oracle has developed its flagship relationship database system to an object relationship system, by adding an extensible type system, object storage, an extensible query and indexing framework, supporting multimedia data types, a server-based extensible Java virtual machine, as well as improving its SQL DDL and DML language.
ODMG (Object-Oriented Database Management Group) is not a formal "standard" agency, it is a committee consisting of five first-generation Object-Oriented Database Systems (OODBs) suppliers (for years, these suppliers have provided products that are not just Object-Oriented Programming Language, but the wrong label "Object-Oriented Database Systems" has been stuck on these products on the market.
We presented an elegant technology to reduce heritage and inclusion to pure disintegration. In this article, the reduction technology allows to simulate the object-oriented database characteristics in a pure disintegration system. The inclusion has been given a formal processing, for the first time by introducing the so-called inclusive solution. In this article, the completion of the technique of the elegance of treating heritage with excess and conflict resolution, by avoiding non-singular rationalization. We show that the completion based on the reduction technology is solid and attractive, compared to any other known rewrite based on methods. We presented an object-oriented front-end language called Datalog+‘, and discussed a rewrite program to encrypt the Datalog for this language, in order to make use of this background and the completion of the technology
As WW becomes increasingly popular and powerful, how to search for the database on the web becomes an important research topic.COMMIX, a DB group developed at the University of Beijing (China), is a system to build a very large database, using data from the web page to extract information, integrate and query answers.COMMIX has some innovative features such as ontology-based packaging generated, XML-based information integration, view-based query answers, and QBE-style XML query interface.
Today, the interactive problem in the information search on the Internet is mainly solved by centralization, whether in the system or in the logic level. This method has been achieved in a certain degree success. As a new brand in the system architecture, the system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system system
Building objectively oriented application access data introduces a series of technical issues that developers are turning into C++. We describe these problems and discuss how we solve them in sustainability, an application development tool, using an automatic code inventor to combine C++ applications with relationship data.
Oracle Materialized View (MVs) is used for data storage and copying. for data storage, internal/external balance MVs can be updated on the trading boundaries, according to demand, or regularly. the copier is optimized for large loads, can be used multi-MV programming. the MVs are based on secondary in remote tables supporting double-wheeled copying. the optimization with the MVs includes transparent query copying based on cost selection methods. the MVs based on a group can rewrite a large-class query, which can be used by using Dimensions (new Oracle objects), loss of connection, functional dependence, synchronization, connectivity, connectivity, connection.
Object-oriented databases execute behavioral specification consistency rules to ensure that type security, i.e., no running time type errors may occur.When specification must progress, some specification updates may violate these rules.To maintain complete behavioral specification consistency, traditional solutions require significant changes in the code of type, type and existing methods.These operations are very expensive in the databases background.To facilitate the specification progress, we recommend supporting the specification consistency rules exceptions, without sacrificing the type security of everything.The basic idea is to detect unsafe specifications in editing time and check them in running time.
The Fifth Eastern European Conference ADBIS'2001 was held by the University of Vilnius, the Institute of Mathematics and Computer (Lithuania), the Lithuanian Computing Society and the Moscow ACM SIGMOD Chapter and the Lithuanian Law University in Vilnius, Lithuania, on September 25-28.The call for the thesis attracted 82 submissions from 30 countries.The International Course Committee, composed of 47 researchers from 21 countries, chose 25 thesis for long speeches and 19 research communications for regular meetings.
The basic idea is to calculate many integrated values for small to medium size integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integrated integr
We proposed a new dynamic method for multi-dimensional selective assessment, working accurately independent data distribution. Good selective assessment is important for query optimization and physical database design. Our method uses multi-dimensional network file (MLGF) to accurately estimate multi-dimensional data distribution. MLGF is a dynamic, sequential, balanced, multi-dimensional file structure, well adapted to non-conformity and related distribution. We show that the MLGF directory naturally represents multi-dimensional data distribution.
An important task of query optimization is a selective estimate, that is, taking into account query P, we need to estimate the score of the records in the database, meeting P. Many business databases keep history maps to approach the frequency of the value in the relationship properties.
The automatic extraction of information from unstructured sources has opened a new path to query, organization and analysis of data, by drawing a clean structured database and the wealth of unstructured data. The field of information extraction has its genetic in the natural language processing community, where the main impulse comes from competition concentrated on recognized names of entities, such as people's names and organizations from news articles. As society becomes more data-oriented, easy to access online structured and unstructured data, the new structure extraction applies to the surroundings. Now, it is interested to convert our personal desktop into structured databases, the scientific publications of knowledge into structured records, and on the Internet to find structured facts.
Microsoft® TerraServer stores Earth's air, satellite and geographical images in the SQL database available through the Internet. It is the world's largest online image, combining eight traps of image data from the U.S. Geological Survey (USGS) and SPIN-2. Internet browsers provide data with an intuitive space and text interface. Users don't need any special hardware, software or knowledge to find and browse images. This paper describes how "Internet unfriendly" geographical images are scanned and edited to millions of "Internet-friendly" images and uploaded to the SQL database.
The physical performance of the Oracle RAC hardware architecture directly affects the output performance of Oracle.In this article, we conduct performance tests on the three Oracle RAC hardware architectures, testing data through real teaching management system data at the campus.The simulation results show that the two new Oracle RAC hardware architectures are occupied by the TPM and IOPS indicators respectively, in general, stronger than the traditional external storage architecture.
For example, in the multimedia/image content settings, users may want photos with sunset; in the current systems, such as QBIC, users must provide a user-friendly, but theoretically solid method to deal with these queries and determine the relative importance of colors, shapes and structures. Worse, users may want the interrelationship between properties, such as, in a traditional, medical recording database, a medical researcher may want to find “lightly overweight patients”, where the so-called queries will be “weight/weight M4lb/inch”. Our goal is to provide a user-friendly, but theoretically solid method to deal with these queries.
With the rapid growth of XML file traffic on the network, extensible content-based XML file spread to a large, dynamic consumer group has become an important research challenge. In order to guide them to the content type, data consumers usually specify their subscriptions, using some XML mode specification language (e.g., XPath). Due to the large size of the subscribers, the system’s extension and efficiency requirements are required to be able to gather the consumer subscriptions space to a smaller content specification in order to reduce their storage space requirements, as well as accelerate the file subscriptions matching process. In this article, we offer the subscriptions portfolio of the first system study, in which there are specified wooden specifications (e.g., XPath’s important specifications).
Today’s e-commerce system is a set of complex databases, web servers, home-growing adhesive codes and network services to ensure security and scalability. The trend is to gather to larger parts in the packaging offer offered by leading software suppliers, as well as through the service provided by the company. In this article, we carefully observe IBM’s WebSphere, commercial version, as well as its packaging deployed on a major customer website.
Forecast query execution time is crucial for many databases management tasks, including receipt control, query programming and progress monitoring. Although some recent papers have studied this problem, most of the existing work is taken into account forecast of a single query, or forecast of the static work load for each query, in "static" we refer to the query model is fixed and known. In this paper we consider the more common problem is based on the dynamic current work load. Unlike the query execution time forecast for the majority of the work before, the framework we propose is based on an analysis model rather than machine learning. We first use the optimizer cost model to estimate each query I/O and CPU requirements, then use the model and the model can be purchased on the basis of the model.
Many conceptual models work involve the use of entity relationship models, in which binary relationships appear in the relationship between two entities. involving more than two entities is considered rare and therefore not obtained sufficient attention. This study provides an overall framework of analytical relationships, in which binary relationships only become a special case. The framework helps designers to identify the ground and other advanced relationships, which are often represented, often inappropriate, whether entity or binary relationships. The general rule also provides to represent superior relationships in the relationship model. This unified treatment relationship should significantly reduce the burden of the designer, by allowing it to extract more information from the real situation and in a suitable design it represents.
This article summarizes my recent job search, effectively started at the end of 2000 and ended earlier in the summer of 2001.The views I express here are mainly based on my experience, hearing and reading various sources, should be as a simple tip or recommendation for doctoral students who are about to graduate and seek a job in research-oriented academic institutions.This is meaninglessly a comprehensive job search guide: in a limited space, I only deal with issues I think more relevant or important, in the effort to provide information and insights, I think it’s not easy elsewhere.
Consider the problems of observing the thousands of time-series data flows online and making decisions according to them. In addition to single-stream statistics, such as average and standard deviations, we also want to find high relevance in all flow couples. Stock market traders may use this tool to find arbitration opportunities. This paper proposes an effective way to solve this problem, based on the distinction of four transitions and a three-level time interval. The widespread experiment on synthetic data and real-world financial transaction data shows that our algorithm hit several size commands direct calculation methods. It also improved the previous four conversion methods by allowing effective calculation of any size and any time delay.
The behavior of scientific and engineering research is becoming critical to the effective management of scientific and engineering data and technical information. The rapid progress of scientific tools, computers and communication technologies enables scientists to collect, produce, process and share unprecedented amounts of data. For example, the mission of Earth Observation Systems Data and Information Systems (EOSDIS) is to manage the data from NASA's Earth Scientific Research satellite and field measurement programmes, as well as other necessary data to explain these measurements support global change research. In addition to being able to process a traffic of 1 trabat data daily to 2000, EOSDIS also needs to provide transparent access to immaterial data stored in the files of several U.S. government agencies and national organizations.
In this article, we have developed a reversal-based algorithm called the FTP-DS (frequency time mode data flow), used to mining the frequency flow time mode while providing the general framework for the model frequency calculation, the FTP-DS algorithm has two main features, that is, a data scan on-line statistical data collection and reversal based on the micro mode display. To a data scan feature, the data division and model growth scenario are the main studies carried out from the frequency calculation department.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
XML has become common and XML data must be managed in the database.The current industry standard is to convert XML data maps into relative tables and store this information in the relative database.These maps create expressive power problems and performance problems.In the TIMBER project, we are exploring issues involving XML storage in local format.We believe that the key intellectual contribution of the system is a comprehensive timely query processing ability in the local XML store, all the standard relative query processing components, including the algorithm rewrite and cost-based optimizer.
Space data mining is finding interesting relationships and characteristics that may exist in the space database. In this article, we explore whether the classification method plays a role in space data mining. For this, we developed a new classification method called CLAHANS, based on random search. We also developed two using CLAHANS space data mining algorithms. Our analysis and experiment shows that with the help of CLAHANS, both algorithms are very effective and can lead to discovery, with the current space data mining algorithm is difficult to find.
The paper describes a parallel database load prototype for the Digital of the Rdb database product. The prototype takes a data flow method to the database parallelity. The paper includes a detector, discovering and recording a database in a collective configuration, a client CUI interface, collecting load work descriptions from the user and the Rdb directory, and an optimizer, from a best parallel implementation program, and recording it in a web page data structure. The web page describes the data operator, the data flow flow between them, the operator’s connection to the processor, the process to the processor, and the file to the disk and the disk.
With t, ha databases spread and progress in a wide range of regional networks, interests, in global data, databases interactivity has been driven. scalability and language support for this new environment is still open mission Sions. We proposed a plan where the databases nodes are dynamically divided around the current, region of ibresl. data sharing is subsequently pursuing, with any relationship informat8ion finds being fed bac,k for re-distribution. to scalability,t, he proposed architects (it is divided into two rrlntionship and illntiorl space.
Workflow consists of a set of coordinated tasks, designed to perform a precise complex process, such as directory orders, travel planning, or a business process in an enterprise. Workflow planning is a problem, finding a correct execution sequence of workflow tasks, i.e. implementing, obeying restrictions, surrounding the workflow business logic. Workflow planning studies are mainly focused on time limit, specify the correct task arrangement. Another important limit category - those generated from the resource allocation - has gained relatively less attention in the workflow model. Because the usual resources are unlimited, cannot be shared, workflow planning involves the execution of decisions as a resource, when used. In this work framework, we provide the work framework with a timely limitation, specified the proper task arrangement - another important limitation by the res
The processing of space connection can be significantly improved by using two filters, reducing the need to check the accurate polymer geology in order to find the cross polymer. The candidate couple of the near polymer use these filters to review. The result, there are three possible response sets are determined: a positive, composed of the cross polymer pair; a negative, composed of the non-cross polymer pair; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non-cross polymer composed; a non
The link between the documents has a significant impact on the importance of the document, as it can be said that important documents are directed by many documents or other important documents.Metasearch engine can be used to facilitate ordinary users to obtain information from multiple local sources (text databases).In a large-scale Metasearch engine, the content of each local databases is represented by a representative.Each user query is evaluated for the representatives of all the databases he set to determine the relevant databases (search engine) for search (search) before the link between the documents is not used to determine the appropriate databases for search (text databases).In this file, this information is used to determine the relevant document's degree and the data base's relevance correspondence correspondence correspondence correspondence correspondence correspondence
The ability to interconnect through the Internet work, wireless networks, high-band broadband satellites and cable networks added a new category of information center applications, based on a regular timetable of data transmission, forecasting the customer's requests. These applications use broadcast to transmit data to a very large customer group. We proposed the broadcast disk model [Zdon94, Acha95b] to organize a data transmission system content and manage the customer's resources to respond to such a program. Our previous work broadcast disk focuses on the "press" method where the data is sent to the broadcast channel according to a regular timetable, forecasting the customer's requests. In this article, we study how to increase the push to a model to "press" the result, using a server system that can transmit data to a customer's content system, to facilitate
For example, new features are added to SQL support OLAP type queries, in which queries are often an important aspect. Currently being taught to database students queries optimization settings or multi-group basis frameworks are increasingly inappropriate. This paper introduces an queries optimization basis, expands the existing framework to catch orders. Based on the list of relations algorithms with three gradually stronger types of algorithms rankings, specific queries conversion rules, follow different rankings, and determine which types of conversion rules apply to optimizing queries.
As we approach the next century, the landscape of the software industry is experiencing huge technological and business changes. the customer / server revolution has almost not reached its half-life, it has been closed by the Internet revolution. software development is moving from the work-intensive direction. customers are buying more pre-packed software solutions or software components that can easily be collected on the site by internal staff or system integrators. In addition to a few players such as Microsoft, Oracle and computer partners, a few of the top software companies in the 1970s and 1980s have survived to the 1990s.
PC Cluster has recently been considered to be one of the most prospective platforms, heavy data intensity applications, such as decision-making support query processing and data mining. PC hardware development time has become very short, resulting in an abnormal system, where the CPU’s clock cycle, performance/capacity of the disk drive, etc. is very prospective, in a component of the PC. Abnormality is inevitable. Basically, the current algorithm recognizes homogeneity. Therefore, if we naively apply them to an abnormal system, its performance is much lower than expected. We need some new methods to deal with abnormality. On this paper, we propose a dynamic balance method that can solve their clock cycle, the PC’s performance/capacity of the drive system is very prospective with
This paper introduces a simulation system of video requests.We introduce the video server algorithms in real-time disk programming, forecast, and bubble pool management.The performance of these algorithms is compared to the performance of simpler algorithms, such as elevator and circular disk programming and global LRU bubble pool management.
We propose a new Rtree structure, beyond all the old. The core of this idea is to be in the R-tree. This is by submitting an order of the R-tree center of the value of the R-tree. This order must be “good” in this sense, it should gather “like” data straight to reduce the area and the surrounding result of the minimum linking straight (MBR46).
In any case, they have to effectively process database queries, process a large amount of data objects, and application logic, which includes detailed object access (e.g., method calls). complex optimization technology accelerates queries processing, while queries are used to reduce the cost of application logic. queries processing and queries decisions are usually carried out in isolation, although frequently in application queries are used to identify these objects the application will be further processed. We propose a comprehensive method to accelerate these applications: We load the system queries with the relevant objects as a product queries processing that can potentially improve the performance of the application by eliminating errors in the basis of queries processing but queries can be carried out by queries.
Enterprise data analysis is often an uncertain task, which is characterized by a large amount of noise data. Therefore, enterprise data analysis must combine two types of interactive tasks: exploration and analysis. Exploration is to find the appropriate subgroup of data process analysis, analysis is to measure the process of data to provide business answers. While there are many tools available for exploration and analysis, a single tool or tool group may not provide full support for these interactive tasks. We report a project here to understand a specific business data analysis issue and create a environment to support it. This understanding result is, first, a detailed list of requirements for this task; The second group of ability to meet these requirements; The third group, implementing client server solutions to determine these requirements and many other tools may provide full support for these tasks here, we report a project to understand a specific business data analysis issue.
In recent years, the research interest is increasing, partly because XML has now become the actual standard for data exchange on the Internet. About XML storage models and query processing technologies have been a lot of work. However, some work has solved the problem of XML query optimization. In this paper, we report a challenge to us in XML query optimization: content combining size estimates. Content combining is considered an important operation of XML query processing. The estimation of its results is undoubtedly essential to produce an effective XML query processing plan. We proposed a combination of two models, interval models and location models, and based on these two models.
W3C XML Scheme language is increasingly popular to express the data model of the XML document. It is a powerful language, including the structure and data type simulation feature. Data storage in the database system has many advantages, including better query capability, optimization of updates and stronger verification. However, the loyalty of the XML document cannot be sacrificed. Therefore, the basic question faced by the database implementers is: How can the XML program map to the relative (and the object relative) database, without losing the graph or data type simulation feature? In this article, we introduce the Oracle XML DB solution to flexibly map XML map to the object related database it retains, processes, processes, processes, processes, processes.
Dynamic queries constitute a very powerful mechanism of information visualization; some data universes are visualized, and this visualization is modified on the plane when the user changes the scope of interest within the different properties of visualized information.
We are witnessing profound changes in the global information infrastructure, with potential to fundamentally affect many aspects of our lives. An important aspect of the development of the infrastructure is the wireless connection, producing continuous interaction between people and interconnected computers. One challenging field of wireless computing is the provision of integrated personal information services and applications for mobile users (PISA). In this article, the wireless client/server computing architecture will discuss the delivery of PISA.
We introduce the architecture and challenge portfolio of the database management system. These system teams build a nod (contra) network that coordinates most typical DBMS tasks in running time, such as queries, updates and data sharing. Such network work is similar to traditional multi-databases. Traditional multi-databases systems are based on critical concepts, such as a global program, central administrative authority, data integration, global access to multiple databases, permanent participation in the database, etc. On the contrary, our recommendations assume a general lack of any central authority or control, no global program, transitional participation in the database, and continuously developing the coordination rules between the database.
The performance of the main memory index structure is increasingly determined by the number of CPU memory errors that are conducted through the index. When the key is indirectly stored, as the main memory warehouse standard, the key recycling costs in the sense of memory errors can lead the cost of an index passing. However, in time and space, even the middle size of the key is directly stored in the index point. In this article, we investigate the performance of the tree structure corresponding to the OLTP work load, facing expensive memory errors and non-triple key size. We propose two index structures, pkT trees and pkB trees, which contribute to significantly reducing the memory error distribution, through the indication of the key storage part we show a small part of the key.
Time sequence analysis in financial and scientific applications requires a complex, special simulation capacity of the database features, while also a easy-to-use interface, we introduce the time sequence management system CALANDA, which combines a powerful specialized data model and intuitive GUI.
In this presentation, we will show the content-based space image recovery engine (SPIRE) implementation for multiple unstructured data. The architecture provides a framework for obtaining multiple data from large files, including images, image series, time series and parameters data. Dramatic acceleration (from 4 to 35 factors) has achieved many search operations, such as template matching, structural characteristics extract.
We proposed a new multi-properties index. Our method combines a hB tree, a multi-properties index, and a $\Pi$ tree, an abstract index, which provides effective competition and recovery method. We call the result method for hB $^\Pi$ tree. We describe several versions of hB $^\Pi$ tree, each using different nod division and index period release algorithms. We also describe a new nod removal algorithm. We have implemented all versions of hB $\Pi$ tree. Our performance results show that even if the version provides no performance guarantee, it actually performs very well, using the index, the size index(), the accurate search and overall distribution, can be distributed to different nod division and index periods we also publish a new nod deletion.
Space data mining is finding interesting relationships and characteristics that may exist in the space database. In this article, we explore whether the classification method plays a role in space data mining. For this, we developed a new classification method called CLAHANS, based on random search. We also developed two using CLAHANS space data mining algorithms. Our analysis and experiment shows that with the help of CLAHANS, both algorithms are very effective and can lead to discovery, with the current space data mining algorithm is difficult to find.
As the first step in achieving this goal, in 15, we proposed a limited-based, human-centered, various rules-based mining-supported architecture, including the association, introduced the concept of limited-frequency query (CFQ) and developed effective impulse optimization for CFQ with 1 variable (1 variable) limit.
I described an introduction to a new version of the database course in the winter of 2003, the key idea of the course was to expos students to some challenges when working and integrating data from multiple database systems and applications.
IP network operators collect integrated traffic statistics on the network interface through the Simple Network Management Protocol (SNMP). This is part of the regular network operations of most ISP; it involves multiple network management stations to request information from all the network elements and collect real-time data sources. This presentation will introduce a tool to manage the site SNMP data sources in a full-function ISP in the industry scale. The tool is mainly used to study the interconnection in network traffic, by providing user-friendly interconnection interface and a rich mix of advertising queries based on the expertise of the site operators.
Structural matching and discoveries in files such as SGML and HTML are crucial for data storage(6), version management(7), supertext writing(7), digital libraries(4) and Internet databases. For example, the users of the World Wide Web may be interested in understanding the changes in HTML documents(2), 5, 10. This change can be done by comparing the old and new versions of the document (reference document structure matching). For another example, in supertext writing(7), the user may want to find the common part in the history list of the document or in the database of the document (reference document structure discoveries). In the SIGMOD 95 presentation, we show a software pack called TreeDiff(13), comparing the two sub-periods and the reference document structure (reference document structure matching).
It provides a software architecture that allows the customer application to interact with the DBMS server in a flexible and powerful way, using direct, volatile messages, or send messages through a recoverable curve. Customer's regular requests to the server and customer's responses can be transmitted through direct or recoverable messages.
Similarity search has recently attracted a lot of research interest. This is a difficult problem because of the data’s common high size. The most promising solutions involve implementing size reduction of data and then the multi-dimensional index structure index reduction of data. Many size reduction technologies have been proposed, including the unit value decomposition (SVD), the same differentiated conversion (DFT), and differentiated conversion (DWT). In this article, we introduce a new size reduction technology, which we call adaptive component fixed proximity (APCA). Although the previous technology (e.g., SVD, DFT and DWT) chooses a common representation, even in the search database, APCA does not build the same database, APCA can support differentiated conversion.
DEVise is a data exploration system that allows users to easily develop, browse and share large table data sets (which may include or cite multimedia objects) with visual presentations.
Data mining technology gives us the new ability to identify interrelationships in big data sets, which introduces the risk when data is to be public, but interrelationships are private, we introduce a method to selectively remove individual values from the database to prevent finding a set of rules while saving data from other applications.
A new intermediate software management is emerging.These products are promised to have huge flexibility in terms of business application divisions across multiple enterprise landscapes.What factors should be considered when choosing the solution, and how the current product is upgraded?
A wide range of Web applications create a complete query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query query qu
The authors describe their method to optimize the application implementation program in XPRS, a multi-user parallel database machine based on the shared memory of multiple processors and discs series. This optimization problem is the main difficulty of editing time unknown parameters, such as the size of the bubble available and the number of free processors, as well as the potential parallel program of huge search space. The authors deal with these problems, a new two-stage optimization strategy, which significantly reduces the search space and allows the running time parameters without significant damage to the program optimization. They introduce their two-stage strategy and provide experimental evidence from the XPRS reference indicators, which suggests it almost always produces the best program.
We propose a new index structure, a tree (approximate to a tree), for similarity search for data in high sizes. The basic idea of a tree is to introduce a virtual marginal straight corner (VBR) containing and approaching MBR or data objects. VBR can represent quite compact, thus affecting the tree configuration, whether quantity and quality. First, because the tree nodes can contain a large amount of VBR input, Fanout increases, which increases the search speed. More importantly, we have a free hand in ordering MBR and VBR in the tree nodes. Therefore, through the Fet-Tree confirmation, the VA model, we can get real data of approximate and real information, and in the cost of 64 MB and data accuracy, we use its data, high size, we provide the data.
One important problem faced by many databases management systems is the "online object configuration problem" - select a disk page to keep a new assigned object problem. lack of classification standards, the goal is to maximize the use of the storage system. for the main memory base systems, simple illusions exist, providing reasonable space for use in the worst and excellent use in the typical cases. however, storage management problems for the databases include significant additional challenges, such as reducing I / O traffic, dealing with accident recovery, and charitable integration of space management with lock and logging. We investigate multiple object configuration algorithms, including technology can be found in the business and research databases system. then we offer a new object configuration algorithm, we design objects configuration algorithm, objects configuration algorithm, objects configuration
In our models, we deal with resource limitations, by putting down the form of vulnerabilities from the data flow, we first discuss the alternative building model for the data flow processing, we investigate the appropriate measures to approach a setting of assessment of the query results, then we will produce the number of results as quality measurements, we provide it with the best offline and fast online algorithms. in a thorough experimental study, synthetic and real data, we show the effectiveness of our solutions.
In recent years, the development of the Internet has made many different information systems available all over the world useful.Though there are many studies on the integration of diversified information systems, most business systems have stopped the actual integration of existing data.Data integration is the combination of multiple records representing the same real world object into a single, consistent, clean representation.
The challenge in the database in the evolutionary time series is to provide effective algorithms and access methods for query processing, taking into account the database is constantly changing as new data becomes available. Traditional access methods, constantly updated data are considered inappropriate due to significant updating costs. In this article, we use the IDC index (Incremental DFT calculation - index), an effective technique of similarity query processing in the flow time series. The index is based on a multidimensional access method, through a conversion of updating policy and an increased calculation of data conversion (DFT), it is used as a functional extract method. We focus on the range and recent type of query, because they are common application substantial indicators for modern query indicators, we have the ability to do query through query.
This article describes GnatDb, which is a built-in database system that provides protection for both accidental and malicious data corruption.GnatDb is designed to run a wide range of devices, some of which have very limited resources.Therefore, its design is strongly driven by the need to reduce resource consumption.GnatDb uses atoms and permanent updates to protect data from accidental corruption.It prevents malicious data corruption using standard encryption technologies, using log-based storage models.
We present the Iceberg-CUBE issue as a database (CUBE) problem improvement. The Iceberg-CUBE issue is only calculating the collective value of these divisions (e.g., calculating) beyond certain minimum support boundaries. The Iceberg-CUBE results can be used (1) with a condition such as HAVING COUNT(*) >= X, where X exceeds the boundaries, (2) for the mining multi-dimensional association rules, and (3) supplementing existing strategies to determine interesting CUBE subgroup pre-calculations.
Over the past few decades, CPU speed improvements have been significantly improved in the main memory and disk access rate, by size commands, allowing the use of data compression technology to improve the performance of the database system. Previous work describes the benefits of digital properties compression, data stored in the compression format on the disk. Despite the wealth of serial evaluation properties in the relative program there is little work in the compression serial properties in the database background. In addition, no previous work properly deals with the role of query optimizer: During the query execution, the data is read to the main memory, or the data is easily compressed in the main memory and only needs to be compressed in the main memory.
Capacity mature pattern (4) is an organization that defines the order of the existing software development process capabilities and sets priorities for improvement.
Current information systems need to process more complex data, about the traditional relationship data two methods of decision. Database community has proposed these types of data abstract, especially about the semi-structure data model. Half-structure model constitutes a database, essentially as a terminal-oriented label chart, its nodes represent the object, its edge constitutes the relationship between the object. Similarly, the connection query is based on the processing of the query results of the two core query language of the two methods of decision. This basic query method is based on the relationship model query language, the regular query path (RPQs) and its variables are considered to be based on the basic query mechanism, especially on the semi-structure data query, i.e. the evaluation of a query basic task, the query database should support the other
This paper studies the problem of increased merger of multiple ranked data sets, when adding conditions are defined by any user by the forecast list on the input score. This problem occurs in many important applications processing orders input and multiple ranked data sets, and requires top solutions. We use multimedia applications as motivating examples, but the problem is also applicable to traditional database applications, including the best resource allocation, planning, decision making, ordering, etc. We presented an algorithm that allows the request for orders of data sets by imposing any undefined merger forecasts. The basic version of this algorithm does not use any random access, but a variable can use the available indicators for random access based on and forecast a special case involving the combination of cases and can be supported in the basis of cases.
The physical configuration of the data is the key performance factor in the data storage. The best classification of the data on the disk to try to reduce the expected I/O, depending on the work load of the query. In practice, we often have a reasonable feeling of different categories of query, for example, 40% of query from a specific telephone number is carried out within a month. In this article, we solved finding the best record classification of a fact table on the disk, taking into account the expected work load in the query class's probability of the form of distribution.
The tutorial "Semantic B2B Integration" will provide a technical introduction to the business (B2B) integration, with a focus on the Semantic B2B integration aspects.The collection of B2B integration concepts is also introduced, and their implementation in the form of the Semantic B2B integration architecture.A set of examples is taken to describe the need to solve problems in the Semantic B2B integration project.The tutorial allows the audience to identify Semantic B2B integration problems and determine the advantages and disadvantages of various technological integration methods or B2B integration technologies.
Therefore, any particular storage device (or as a single logical device) can only serve a fixed number of client access traffic. Each storage device is also limited to the amount of video files that can be stored. Due to availability, increased growth, and abnormality, there may be multiple storage devices in a video server environment. Therefore, one or more copies of specific video can be placed on different storage devices. Because access rates to different videos are uneven, there may be unbalanced load between devices.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
The objective of our work is to provide a DBMS data model and query language capable of processing such time-dependent data models, including those constantly changing data models, describing mobile objects. Two basic abstracts are moving points and mobile areas, describing objects, only taking into account time-dependent location, or location and range of data of the system, respectively, providing a data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data
A space data mining system prototype, GeoMiner, is based on our research and development relationship data mining system, DBMiner, and our research space data mining experience for many years of design and development. Data mining power GeoMiner includes mining three rules: characteristic rules, comparative rules, and association rules, in geospatial databases, with a program expansion, including mining classification rules and classification rules. Space data (spatial and non-spatial data) building applies to simulating space databases, in which GeoMiner includes space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining, space data mining,
The appearance of big data is equivalent to the complexity of the discussion of data recurring use. The benefits of big data are in finding new trends, patterns and the possibility of relationships, by combining a very large amount of data from different sources. The current personal data protection requirements, such as data minimization and purpose specifications are potentially hostile to big data because they limit the size and use of big data. The economic and social benefits of big data can be the result of material loss. In order to avoid this, the recurring use of data can be encouraged. The recurring use of data, when done correctly, can be privacy protection and economic and social interests. In this article, we provide a right of data recurring use, from the data controller and data subject’s point of view, can be determined by the use of instances and the use of inst
Current XML data storage research focuses on XML data in the existing RDBMS or develop an original XML storage. Some original XML storage storage each XML nod in a distributed object form. Distribution, which means the physical arrangement of the object, can be an important factor in improving performance in this storage model. In this article, we propose a distributed method of storing data nods in the XML document in the original XML storage. Recommended appropriate distributed method of using the path similarity of the data nods, can reduce the page I / Os requires the query processing. In addition, we propose a query processing method, using the signature, making the distributed level of data storage from the proposed distributed method of the method that can be distributed through a method of storing the data nod in the XML document.
Reverse Neighborhood (RNN) query has been studied as a terminal, stored data set, and interested in decision-making support. However, in many applications, such as fixed wireless telephone access and sensor-based highway traffic monitoring, data reaches a stream and cannot be stored. Explore this data stream can naturally be formalized through the concept of RNN collective (RNNA), which involves the calculation of certain collectives (such as C0UNT or MAX DISTANCE) in the reverse Neighborhood "clients" associated with each "server".
Experimental research based on time sequence is a requirement for the database management system (DBMS) of data intense activity. We study a time sequence management system (TSMS) should have special characteristics. Then we show the current available solutions and related research directions are not suitable for dealing with existing problems. Therefore, we recommend developing a special purpose TSMS, which will provide special models, receiving and calculating capabilities. It will be suitable for the final user, providing a direct operating interface and allowing data exchange with various data sources, including other databases and applications packages.
Similarity recycling mechanism should use the general square distance function as well as the Oakland distance function, as the Oakland distance query parameters can be different from the user and the situation. In this article, we introduce space conversion technology, producing a new search method to adapt to the Oakland query with the square distance function. The basic idea is to convert the marginal straight corner in the primary space, where the distance from a query point is measured through the square distance function, in a new space space object, where the distance is measured by the Oakland distance function. Our method significantly reduces the cost of the CPU, because the distance is closer to the space conversion; the accurate distance assessment is to avoid access to the most close line in the original space, where the distance from a square point is measured through the space
The number, size, and user population are growing rapidly.Through the high document reach rates, users of these databases have to access the latest documents; however, the high document reach rates also make it difficult for users to keep up-to-date.
─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
Data flow queries and analysis are a very recent topic of interest, motivated by applications from the fields of network, network use analysis, sensor tools, telecommunications, etc. Many of these applications involve monitoring responses to the continuous queries of data flows to the location of physical distribution, while most of the previous methods need to flow to a single location for centralized processing. Unfortunately, the continuous transmission of a large amount of data flow to a central location can be useless or expensive. We study a useful queries class, continuously reporting the main values from the distribution of data flow ("top monitoring queries"), these applications are interesting because they can be used to reduce super monitoring, while in other types of monitoring to reduce the flow of data to a single location for centralized processing, unfortunately, the continuous transmission of
We consider the variants of the view maintenance problem: how to keep a materialized view up to date when the view definition itself occurs? can be done better than re-defining the view from the basic relationship? traditional view maintenance tries to keep the materialized view to respond to the basic relationship changes; we try to "adjust" the view to respond to the changes in the view definition.
In this paper, we study the co-author maps obtained from all the papers published in SIGMOD between 1975 and 2002.
In the epoch of electronic publication, a comprehensive website management system (WSMS) is required to provide a terminal solution, from the integration of the website to the restructuring and maintenance of a new custom page view.
Space data mining is finding interesting relationships and characteristics that may exist in the space database. In this article, we explore whether the classification method plays a role in space data mining. For this, we developed a new classification method called CLAHANS, based on random search. We also developed two using CLAHANS space data mining algorithms. Our analysis and experiment shows that with the help of CLAHANS, both algorithms are very effective and can lead to discovery, with the current space data mining algorithm is difficult to find.
Today, the database is commonly containing many megabyte and even many traps of data. Scientific experiments in the fields of high-energy physics produce enormous data sets, while in the business field, decision-making support systems and the appearance of the database lead to the organization to build enormous data sets. Integrated queries allow a person to obtain short information from such a database as they can cover many data elements while returning a small result. OLAP queries, widely used for data storage, almost entirely based on integration.
The right to protect the relationship data is increasing interest, especially considering the sensitive, valuable content to be submitted in the field. A good example is a data mining application where the data in the parts are sold to the expert mining it. The rights protection different routes are available, each has its own advantages and disadvantages. The implementation by legal means is usually ineffective to prevent the robbery of copyright works, unless the increase of digital opponents, such as water labels. The authors of recent studies introduce the issue of digital water labels, the general digital set. In this article, we extend to this basis, and through water labels introduce a solution for the relationship database rights protection. Our solution is important to attack such as data re-set, the real line of data, except the increase of the number of water labels, our data labels.
This paper describes a tool called Nodose that we have developed to accelerate the creation of a powerful packaging.Nodose allows non-programmer to build components that can convert data from the source format to XML or other genetic format.
Workflow, business processes, HPPM (HP Process Manager), data analysis, data visualization Business Process Cockpit (BPC) is a tool that supports real-time monitoring, analysis, management and optimization of business processes, developed by HewlettPackard. The main goal of business process Cockpit is to enable corporate users to carry out quality analysis, monitoring and managing business processes at the business level.
While many expanded trading models have been proposed [Elm93], there are few practical implementations, and even less can be supported by more than one model. We presented the reflective trading framework, as a practical and modular method to implement the expanded trading model. We implemented the open implementation method [Kic92] (also known as the meta-object protocol [KdRBSl]) to the design of the reflective trading framework, we achieved the practicality, through the upper implementation of a commercial transaction processing monitor. We presented the reflective trading framework, as a practical and modular method to implement existing commercial TP components, such as Encina, their functional extension to support the expanded trading function. Because our model is based on the reflective trading framework, we realized the practical implementation through the commercial
A variety of business and scientific applications need to analyze the user’s behavior on the Internet. For example, web marketing or network technology support can benefit from the web user’s classification. This is achieved by tracking the user’s page visited during a meeting (access to a specific site). For the automatic user’s meeting classification, we suggest the distance, comparing the meeting judgment of the page sequence in them and these pages. The recommended distance is based on Levenshtein measurement.
In our settings, the XML view is specified by an intermediate information system's statement query language, called SilkRoute. the intermediate information system evaluates a query by sending one or more SQL queries to the target relationship database, integrating the results of a double stream and adding XML labels. We focus on how best to choose the SQL query without controlling the target RDBMS.
We suggest multi-precision similarity matching, the image is divided into several subblocks, each with its related color histogram. We present the experimental results show that through multi-precision color histogram recording space distribution helps make the similarity matching more accurate. We also show that the sub-image query is better supporting multi-precision color histogram. To try to reduce the top, we use a filter based on 3D average color vectors. We provide a formal result proof that multi-precision color histogram is complete.
In this article, we study how we can keep the local copy of the data sources "fresh", when the source data is automatically and independently updated, we study the problem of the website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website website
In this article, we discuss a mixed media access, a information access data pattern, one of which the media type of query may differ from the data. In this article, the media type is considered to be a speech, text images and full text. Some examples of mixed media access are the keywords in the speech and image location, the speaker’s identification, the emphasis on the location of the area, as well as the location of the theme boundaries in the text.
We proposed a defined space database system as a database system, providing space data types in its data model and query language, and supporting space data types in its implementation, providing at least space index and space combination methods. space database systems provide basic database technology for geographical information systems and other applications. We investigate data models, query, data structure and algorithms, as well as the system architecture for these systems.
In this paper, we take into account the following issues: taking into account the characteristics of a chart, the domain of which is real numbers, and a query, specified a range in each dimension, finding a good close record of the number in the chart to meet the query.
We introduce the technology of computing big data flows small spaces, which are inspired by the traditional wavelet-based approach, these methods consist of specific linear projections of the basic data, we introduce the general "skate" methods to catch a variety of linear projections and use them to provide data flow scores and scores estimates, these methods use a small amount of space and every point of time, while through the data flow, and provide the accurate representation of our actual data flow experimental display.
As the importance of XML in data exchange increases, many studies have been carried out, providing flexible query facilities to extract data from structured XML files. In this article, we present ViST, a new index structure to avoid expensive query XML files. By representing XML files and XML queries in the structure coding series, we show query XML data is equivalent to finding the sequence matching. Unlike the index method, query will be divided into several subqueries, and then add these subqueries results to provide the final answer, ViST uses the tree structure as a basic query unit to avoid expensive query operations. In addition, ViST provides a unified index structure to query XML files and the content structure, it has a advantage to perform query XML data to query XML content, then add these subqueries results to provide the final answer,
Based on the analysis forecast of the characteristics of the database access from the actual reference orbit, it is very useful for the work load management and system capacity planning of the database access model, which can be easily used to calculate the appropriate allocation of the graphic space of the different database relationships, as well as to manage the mixed transaction and query environment of the graphic space. Access features can also be used to predict the graphic error effect in an effective multi-node environment, graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic
At Clio, we have gathered a series of powerful data management technologies that have been shown to be invaluable when solving these difficulties.In this paper, we present the basic topics of our approach and present a short case study.
On the Internet and elsewhere, many sources rank objects according to the results of these objects matching the original query. For example, an entity agent may rank available houses according to the user's preference location and the price matching the price. In this environment, "meta-brokers" will usually query a variety of independent, unusual sources, can use different results ranking strategies. A key issue, a meta-broker then faces is to extract the top objects of the user query from the basis, according to the meta-broker's ranking function. This problem is challenging because these top objects may not match the user's preference location and price. In this article, we discuss the solution of this problem strategic "meta-brokers" specifically, we must provide a key issue, a key issue, a key issue, a key issue, a key issue
In this article, we focus on detecting meaningful changes in the basic layer structure data, such as the basic layer object data, this problem is more challenging than the corresponding relative or tablet file data. To better describe the change, we are not only based on the traditional "atomic" insert, removal, updating operations, but also based on the movement of a whole subnode operations and copying a whole subnode. These operations allow us to describe the change in a more meaningful way. Because this change detection problem is NP hard, in this article, we provide a more challenging detection algorithm to better describe the change, we are not only based on the traditional "atomic" insert, removal, updating operations, but also based on the time operations, moving a whole node and copying a whole node allows us to describe
Previous Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post: Previous post
More than twenty years ago, DB researchers faced how to design a data-independent database management system (DBMS), i.e., a DBMS provides a suitable application programming interface (API) to the user, its architecture is open to permanent development. For this purpose, a record-oriented data-based building model based on continuous data abstract steps was proposed as a standard, later improved to a five-layer sequence DBMS model. We reviewed the basic concepts and implementation techniques of this model and investigated the system level to the presently achieved significant improvements.
The databases provide analytical processing, decision-making and data mining tools of information. With the real-time enterprise's conceptual development, the synchronousity between transaction data and databases, static implementation, has been redefined. The traditional databases systems have static structures, their charts and data relationships, so they cannot support their structure and content any dynamics. Their data are only regularly updated, because they are not prepared for continuous data integration. The real-time enterprise needs to support decision-making purposes, the real-time databases seem to be very prospective. In this article, we introduce a method to adapt to the databases charts and the user's OLAP queries to effectively support real-time data integration. Therefore, we use this technique to analyze queries to
ISDO "00" Seminar "Infrastructure for Dynamic Business to Business Services Export" 1 was held as a conference seminar on the 12th Conference on Advanced Information Systems Engineering (CAiSE *00) held in Stockholm on June 5 and 6.C. Bussler (Network Fish Technology), M. Bichler (University of Vienna Economic and Business Management) and Y. Hoffner and H. Ludwig (IBM Zurich Research Laboratory) organized the seminar, and hosted the seminar's program committee.The seminar's objective is to provide a platform to discuss the model and technology of service export, with a focus on integrating dynamic facilities, setting up and implementing service relationships, business process connectivity, and taking into account this complex process of enterprise, and to build a complex process for enterprise, and to create a complex enterprise, and to create a complex
The original B tree guesses that the existing B tree structure is variable with sequential data in the sheet blocks, which is ideal for storage organization queries involving accurate matching and/or range search in the original key. Commercially, the original B tree similar structure has been supported in DBMS, such as Compaq Non-Stop SQL, Sybase Adaptive Server, and Microsoft SQL Server. Oracle's index area organization table is similar to the original B tree; however, it differs from its commercial opponents in the following aspects: 1) storage organization does not require the entire sequence stored in the original key index.
The majority of the object-oriented database statements SQL similar query language is the right language, allowing to arbitrarily cut the expression in the selection, from, and where the phrase. From the expression in the phrase can be the basic table as well as the setting of the assessment properties. In this article, we propose a general optimized OOSQL query strategy. Like in the relative model, the translation/optimized goal is from the double-oriented query processing. Therefore, OOSQL is translated into the ADL algorithm language, and through the algorithm re-writing query is converted into as many query options as possible.
Modern applications (web portals, digital libraries, etc.) need to integrate access to a variety of information sources (from traditional DBMS to semi-structure web storage), fast deploy and low maintenance costs in a rapidly developing environment. Due to its flexibility, there is increasing interest in using XML as a medium software model for these applications. XML allows quick insert and declaration integration. However, query processing on XML-based integrated system is still punished, the lack of an algorithm with appropriate optimization properties and difficult to understand the source query capacity. In this article, we proposed an algorithm method to support effective XML query assessment. We define a general algorithm that fits the semi-structure XML query language.
This paper proposes a method of evolution of an object base based on separation of concerns.The lack of adaptability and scalability in the existing evolutionary framework is using the properties of the meta-object level to implement the link between the meta-object and the examples of adaptation of the code directly into the consequences of the classroom version.The proposed method uses a dynamic relationship that separates the connection of the code from the meta-object and aspects - the abstract is used by Aspect-Oriented programming to determine the intersection of concerns - separates the examples of adaptation of the code from the classroom version.
Healing databases are populated with a lot of human effort and updated databases, most reference works are traditionally found on the reference panels of the library - dictionaries, encyclical books, newspapers, etc. - now are healing databases, as it is now easy to publish on the web, the number of new healing databases used in scientific research has exploded, healing databases value in the organization and the quality of the data they contain.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
This standard, called sequentiality, is unacceptable for new database applications that require long-term transactions.We introduce a new trading model that allows accuracy standards more suitable for these applications.This model combines three improved standard models: wireless transactions, clear predictions, and multiple versions.These features give the new model names, wireless transactions with predictions and versions, or NT/PV.
Although these tasks gained a lot of attention in the web search literature, the Twitter background introduced a real-time "double": after significantly damaging news events, we are committed to providing relevant results within a few minutes, the paper provides a case study that describes the real-time data processing challenges of the "big data" era, we tell the story of how our system built twice: our first implementation was built on a typical Hadoop-based panel, but later replaced as it did not meet the necessary delay requirements to produce meaningful real-time results.
In many applications, users specify the target value of a specific property without the need to match accurately these values. In contrast, the results of these queries are usually ranked at the top k" score matching the specific property values. In this article, we study the benefits and limitations of dealing with the top k queries, by converting it into a single range queries, the traditional relative DBMS can be effectively dealt with. In particular, we study how to determine a range queries to evaluate a top k queries, using the available statistics of the relative DBMS, and the impact of the quality of these statistics on the recycling accuracy of the results system.
Semantic Web Enabled Web Services (SWWS) will convert the network from static information collection to distributed computing devices, based on machine processing and machine interpretation worldwide, which will allow automatic detection, selection and implementation of cross-organized business logic, making such areas as the dynamic supply chain a reality. in this article, we introduce the vision of Semantic Web Enabled Web Services, describing the requirements for the construction of Web services based on Semantic Web, and drawing the first draft of the implementation of the concept architecture of Semantic Web Enabled Web Services.
Sustainable Applications Systems (PAS) have an increasingly important social and economic significance. They have a long-living potential, at the same time accessible, and consists of big data and programs. Typical examples of PAS are CAD/CAM systems, office automation, CASE tools, software engineering environment, and patient care support systems in hospitals. The correct Sustainable Object System is designed to provide better support for the design, construction, maintenance and operation of PAS. Sustainable Abstract allows the way of creating and processing data, independent of its lifetime, thus integrating the database view with the programming language view. It provides some advantages of the view of the correct design and programming productivity, which are the principles of the Sustainable PAS principles for the Sustainable Support, Sustainable Support, Sustainable Support
In this article, we presented the pyramid technology, a new time indicator method used for high-size data space. The pyramid technology is highly adapted range query processing using maximum measurement Lmax. Unlike all other indicator structures, the performance of the pyramid technology will not deteriorate when processing range query data higher sizes. The pyramid technology is based on a special division strategy to optimize to high-size data range. The basic idea is to divide data space first into 2d pyramid sharing space center point as a top. In the second step, the single pyramid is cut into a slide with the pyramid based on the data page. In addition, we show that this part of the actual composition is based on the basis based on the data page.
The tutor's small prototype is developed in a research project financed by the German Science Foundation (DFG) "Building, Configuration and Management of Large Workflow Management Systems", which begins to develop from its former tutor (Mentor), but aimed at the simpler building. The main goal of the tutor's small is to build a heavy, extensible, adjustable workflow management system (WFMS) with small footprint and easy to use management abilities. Our method is to provide the core features within the workflow engine and consider the system component parts such as history management and workflow management to the core top extension.
Expandable label language (XML) is widely used as a format of data exchange and stored in the World Wide Web. Question XML data requires accurate selective estimates of route expression to optimize query execution plan. selective estimates of XML route expression is usually based on a summary of statistical data structure based on XML storage. All previous methods require an offline scan of XML storage database data to collect statistical data. In this article, we propose XPathLearner, a method to estimate selective of the most commonly used route expression type, without considering XML data. XPathLearner collects and improves statistical data, using query feedback online, especially for query data in the application, as it can be stored through XML data.
The core of all OLAP or multi-dimensional data analysis applications is the ability to integrate multiple groups simultaneously. computing multi-dimensional integration is the performance string of these applications. This paper introduces a fast algorithm to calculate a group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group group
Supporting the flexible method of data exchange between autonomous information systems is critical for today's increasingly unusual enterprise IT infrastructure. Updates, insert and deletion of data objects in autonomous information systems often require data changes in other autonomous systems, even if distributed systems are not integrated into a global program. We recommend solutions to this problem, based on the use of multiple XML technologies of data dissemination and conversion.
Although XML Stylesheet Language for Transformations (XSLT) is not designed as a query language, it is suitable for many query similar operations in XML files, including selection and reorganization of data. In addition, it actively performs a XML query language in modern applications and broadly supports application platform software. However, using database technology to optimize and implement XSLT only recently in the research community attention. In this article, we focus on XSL conversion will be carried out on the XML file, defined as a relative database view. For XSLT subgroup, we introduce an algorithm to form a conversion with the XML view, eliminating the implementation of XSLT needs and then we describe how to expand this algorithm to optimize and implement several additional features of XSLT.
The cost of a query program depends on many parameters, such as the forecast options and the available memory, its value may be unclear in the optimization time. Parameters query optimization (PQO) optimizes a query to a number of candidates, each of which is the best of a particular area of the parameters space. We recommend a chronic solution to the PQO problem in the case when the cost function may be nonlinear in a given parameter. This solution is the minimum of infiltration, in this sense, a existing query optimizer can be made with small changes.
Data integration often requires the full value of data from multiple sources. Despite the extensive research on the help of the user tools, data integration is still difficult, especially for the user with limited technical capabilities. To solve this obstacle, we have studied how much we can do without user guidance. Our vision is that the user should specify only two input data sets and get a meaningful integrated result.
Edit comments: Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments
The author's view of databases mining is the combination of mechanical learning technology and the performance of databases technology is introduced.Three categories of databases mining problems involve classification, association and sequence are described.These problems can be considered uniformly as the need to find the rules embedded in a large amount of data.A model and some basic operations rules of the process of detection are described.It is shown how databases mining problems are considered this model map, and how can be solved by using the basic operations.An example is the classification algorithm obtained by combining the basic rules of the operation.This algorithm is effective to find the rules and accuracy with the current ID3.
Recent studies discussed the importance of optimizing the use of L2 cache in the design of the main memory index, and presented the so-called cache awareness index, such as CSB+ Tree. However, these indicators do not take into account competitiveness control, which is crucial for running real world main memory database applications, including index updates, and using the external shell multiprocessor system to expand the performance of these applications.
On the one hand, the study studies maps between data sources in two perspectives. on the one hand, there are practical tools for the study of graphic maps generated; these emphasis are based on the user's visual specifications of maps generating algorithms. on the other hand, we have theoretical research on data exchange. how these studies generate a solution - that is, a target example - given a group of maps are usually designated as double generating dependency. however, despite the fact that the core concept of a data exchange solution has been officially defined as the best solution, there is still no map system supporting core calculations. in this paper, we introduce several new algorithms that help the gap between bridges to practical maps generating and data exchange examples.
The new database application requires storage and accessing many traps of data to reach the limitations of the disk storage system, both in terms of cost and scalability. These limitations provide a strong motivation for the database development, increasing the disk storage technology better suited to the big data volume. In particular, smooth integrated tape storage to the database system will be very valuable. Band storage is two commands size more efficient than the disk time, as this tape in time cost and physical capacity per traps traps traps traps traps traps traps traps traps traps traps traps traps traps traps traps
PAYOFF IDEA. different styles of user interfaces can significantly affect the ability of the database. In a environment that includes a lot of different databases, the goal is to choose a database management system (DBMS), providing the best design tools choices, minimizing development time, and implementing relationship rules. This article introduces a case study conducted at the University Hospital of Pennsylvania, one of which tested the database is developed to implement with three DBMS, each with a different user and program interface.
Automatic recommendation self-set (e.g., personalized product recommendation on e-commerce sites) is an increasingly valuable service related to many databases - usually online retail catalogs and web logs. Currently, a major obstacle to evaluation of the recommendation algorithm is the lack of any standard, public, real world testing suitable for task. In trying to fill this gap, we created REFEREE, a framework of existing recommendation system using ResearchIndex - a huge online digital library using computer science research paperwork - therefore, in the research community, anyone can develop, deploy and evaluate the recommendation system relatively easy and fast.
Publishing/Subscribe is a user expressing long-term interest paradigm (“Subscribe”) and some agents “Publish” events (e.g., an offer). Publishing/Subscribe software’s task is to send the event holders to meet these events’ subscriptions. For example, user subscriptions may include an interest in a type of aircraft, not exceeding a specific price. Publishing events may include an offer of aircraft with certain properties including price. Each subscription is predicted by a combination (properties, comparison operators, value).
We offer a short but complete formal definition of the XPath 1 grammar and summarize the effective algorithm processing queries in this language. our introduction is for readers who are looking for a short but comprehensive official account XPath, as well as software developers need the material to quickly implement the XPath engine.
We describe the SAMOS, an active object-oriented database management system prototype, and the SAMOS provides a powerful rule-defining language, including a small but powerful event-defining set of facilities that can automatically and efficiently detect original and complex events.
We provide the PPOST (Permanent Parallel Object Store) architecture for the main memory databases of parallel computers, which is suitable for applications with challenging performance requirements.The architecture fully exploits parallelity, large main memory and rapid exchange networks.
But, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances, in some circumstances.
The database management system (or DBMS) has been for decades, but it is still difficult to use, especially when trying to identify and correct the user's errors (or queries). we try to understand what methods have been proposed to help people dismantle the database queries, and whether these technologies are eventually adopted by the DBMS (and users). we conducted a cross-disciplinary review of 112 files and tools from the database, visualization and HCI community. To better understand whether academic and industrial methods meet the needs of the user, we interviewed 20 database users (and some designers) and found surprising results.
This paper introduces structural return as the basis for the synthesis and grammar of the semi-structure data and XML query language, we describe a simple and powerful query language based on model matching, and shows it can be expressed through structural return, it is introduced as a upward, return function, similar to the way XSL is defined on the XML tree. Based on cycle data, structural return can be defined in two similar ways: as a return function, properly evaluate the data and XML, and remember all its calls to avoid unlimited faults, or as a collective evaluation, it can be processed simultaneously, using only the traditional relationship algorithm operator that allows it to be optimized in the application form of return technology.
System development and research on parallel query processing have been focused on the "sharing everything" or "sharing what" architecture so far. Although there are several commercial DBMS based on "sharing disks" alternatives, this architecture has gained very little attention about parallel query processing. comparison between shared disks and shared nothing reveals many potential benefits of shared disks and parallel query processing.
The current comprehensive development of networks and computing has led to a technological infrastructure of the information society, a term that can be circulated in many ways, such as the immaterial computing, television and network as a huge global database independence. The paper applies to the networks of the global database shapes and supplements the immaterial and television aspects below. It should be possible to save many existing databases technologies and focus on adjusting these to the network information infrastructure. The paper explores four adjustment challenges: interactiveness due to the unusual databases, initiativity due to the authenticity of the data sources, interactiveness due to the need for short-term tasks and tasks characteristics of interaction and cooperation, and inheritance of the old systems based on the network and databases environment.
The analysis of the use of the web page is mainly focused on the site that consists of traditional static pages. However, the information available on the web page is a great deal from the database or other data collected and is presented to the user in a dynamically created page form. The search interface of these sites allows to define many search standards. The results of their creation support the results of the web pages combining cross-linked data from many sources. In order to analyze the visitor navigation behavior on these sites, we propose the web page use mines (WUM), finding the navigation pattern under advanced statistics and structural limits. Because our goal is to find interesting navigation pattern, we do not focus on access to individual pages.
A load, such as TTL type of logic network, is parallel to a multi-direct stream source, aimed atining a significantly stable operating voltage. Each source contains a control unit that will compare the load voltage with the reference level to stabilise the output voltage of the current generator. If the current of the current is below a certain minimum value, however, the control unit’s limit sensor will raise the reference level to the equivalent of the maximum possible difference between the reference level of the different control unit, thereby ensuring that all sources contribute simultaneously to the load stream.
Data defined by Interpolation is often found in new applications, involving geographical concepts, moving objects and space time data. These data lead to potential unlimited objects collection (e.g., the height of any point on the map), which is defined on the basis of sample collection and a combination of an Interpolation function. First of all, we believe that by directly accessing the sample and the manipulation of the data of the Interpolation function can easily lead to accumulation or inaccurate queries.
In this article, we study the use of low-cost PC collection methods for parallel calculation of glacier cube queries. We focus on technology for online queries big, high-size data sets, assuming the total cube has been pre-calculated. We explore the algorithmic space taking into account parallel, calculation and I/0 transactions, our main contribution is to develop and comprehensive evaluation of a variety of new, parallel algorithms. Specifically: (1) algorithm RP is a simple parallel version of BUC [BR99]; (2) algorithm BPP tries to reduce the output results in a more efficient way I/0; (3) algorithm ASLins, in which cells are parallel, calculation and I/0. Our main contribution is to develop and comprehensive evaluation of a
In data storage, a well-known challenge is in the presence of source data updates, effective maintenance of data storage. In this paper, we identify several key data representations and algorithm choices that must be done when developing data storage machines. For each decision-making field, we identify various alternatives and through a wide range of experiments to evaluate them. We show that choosing the right alternative can lead to dramatic performance profits and recommend guidelines for making the right decision in different cases. In this paper, all the issues discussed appeared in our development, a prototype storage system supports continuous maintenance.
The competitor review process is generally considered to be the core of academic knowledge progress and is also crucial to the individual career progress.
Data sources about the environment, energy and natural resources are very many worldwide. unfortunately, users often face several problems when searching for and using environmental information. in this article we analyze these issues. from a technical point of view, we describe the four main tasks of producing environmental data and describe the organization of the data generated from these tasks.
We describe the Infosphere project, which is building system software support for information-oriented applications, such as digital libraries and e-commerce. The main technical contribution is Infopipe abstract to support information flow and service quality. Using building blocks, such as programming specialization, software feedback, domain specific language and personalized information filter, Infopipe software produces code and management resources to provide specific service quality, support composition and reorganization.
In order to truly meet the requirements of Multimedia Database Management (MMDB), it requires an integrated framework for modeling, managing and collecting different media data in a unified way.MediaLand is an experimental MMDB platform that is developing in Microsoft studies different levels of user experience and expertise in Asia and searching for multimedia storage library easily, efficiently and collaboratively.MediaLand's key features include a unified data model, describing all types of media objects and their relationships, as well as a four-layer building based on this data model.In this article, a multi-layer query method of MediaLand proposes, in which multi-layer query method is based on a smooth integration of various existing query methods.
We describe the external data management component of the Lore database system as a semi-structured data.Lore's external data administrators allow voluntary, unusual external sources for dynamic access and integration in the process of query processing.The difference between Lore residents and external data is invisible for users.We introduced a flexible argumentation concept that limits the number of data obtained from external sources, we have integrated optimization to reduce the number of calls to external sources.
The database stores a lot of data, often used by decision-back applications. These applications involve complex queries. queries performance in this environment is critical because decision-back applications often require interactive queries response time. As the database is not frequently updated, it becomes possible by queries recall queries performance, in addition to queries implementation plan. In this article, we report a smart queries manager designed to queries recall queries called Watchman, which is suitable for data storage environment. Our queries manager uses two new, complementary algorithms to queries exchange and queries exchange.
Security Council Security Council Security Council Security Council Security Council Security Council Security Council Security Council
XML is rapidly becoming one of the most widely adopted technologies. as the use of XML becomes more common, we predict the development of active XML rules, that is, the rules are clearly designed to manage XML information. in particular, we think active rules provide a natural paradigm of rapid development of innovative electronic services. in the paper, we show how active rules can be defined under the XSLT background, a model-based language publishing XML document (promoed by W3C) gets strong commercial support, and Lorel, a language query XML document is quite popular in the world of research.
In the first article in the regular list of Database Standardization Activities, I provide an overview of the fields of active development to the official national and international standardization agencies, and I ask to contribute to these active topics so that standardization staff and researchers can jointly develop the most useful and highest quality database standards in the near future, before making irreversible decisions.
In these systems, the user has the right to mention the composition function instead of mentioning the original operation. Although the original operation is mentioned within the composition function, the user can only mention them through the granted function. This achieves access control at the abstract operating level. Access control using the embedded function, however, easily leads to many "security faults" through the malicious user can be through the embedded and can abuse the original operation within the function. In this article, we develop a technology to statically detect this security fault. First, we design a framework to describe the security requirements that should be met. Then, we develop an algorithm to analyze the code function and determine whether there is a security fault.
The Second International E-Commerce and Network-Based Information Systems Advanced Themes Seminar (WECWIS 2000) was held on June 8 to 9 in Crowne Plaza San Jose/Silicon Valley, California, with the aim of bringing together leading practitioners, developers and researchers to explore challenging technological issues and find feasible solutions to advance the art of e-Commerce and Network-Based Information Systems.
The concept of rotating dependency (RUD) expands functional dependency and generalization layer.RUD can be applied to OLAP and database design.RUD problems found in big databases are the center of this paper.A algorithm provided, depending on a series of theoretical results.The algorithm has been implemented; the results of two real-life data sets are provided.
The key driver of the General Enterprise Directory Services is to provide a central storage of information that is common and widely used for users, groups, network services and personal data.Lightweight Directory Access Protocol (LDAP) as a access protocol promotes the widespread integration of the network infrastructure and applications of these Directory Services.
New telecommunications services and mobile networks are introduced in the telecommunications network. Compared to the use of traditional databases, telecommunications databases must meet the very strict requirements for response time, channels and availability. ClustRa is a telecommunications databases prototype used to operate through ATM interconnected standard workstations. To meet the channels and real-time response requirements, ClustRa is a major memory database, neighboring mainly, memory storage.
Carnot Research Project (CARN, WOEL93) launched in MCC in 1990, aimed at solving the logical unified physical distribution, an unusual information within the enterprise scope. A prototype has been implemented to provide services for corporate models and models integrated to create a vision within the enterprise scope, for personal resource queries to make grammatical expansion, as well as interresource consistency management. Carnot also includes 3D visualized big information space, knowledge in the database and software application design recovery technology. Carnot Prototype software has been sponsored by Carnot project to develop a range of applications including world management, unusual access to the database, knowledge exploration and big database integrated database.
Abstract.The requirements for effective search and management of the WWW are stronger than ever. Current Web documents are classified based on their content not taking into account the fact that these documents are connected to each other by links. We claim that a page’s classification is enriched by the detection of its incoming links’ semantics. This would enable effective browsing and enhance the validity of search results in the WWW context.
Gray and Graefe's latest article in Sigmod records contains transactions between the size of the B tree and the size of the page and the cost of access to the B tree data. It aims to find the size of the page, which will lead to the lowest cost of access to each record. This transparent analysis shows how the size of the page increases allows the B tree to pass faster while increasing the number of data needed to pass.
In this article, we introduce the General Projection (G P an extended copy excluding projection, capture collection, combination, copy excluding projection (second copy save projection in a common unified framework. using G P s We extended the well-known and simple algorithms of SQL queries, using different projections to derived algorithms of queries using a collection, such as, max, min, count, and avg. We developed a powerful query re-writing rules of collective queries, unified and extended re-writing rules previously known in literature.
As a large number of text databases are already available on the Internet, it becomes increasingly difficult to find the right source for a specific query. In this article, we introduce gGlOSS, a universal Glossary-Of-Server server, which retains the statistical available databases to estimate which databases are potentially most useful for a specific query. gGlOSS expands our previous work, focusing on the database using the document obtained model to cover the database using the most complex vector-space obtained model.
Kimball recommends the creation of a database (or data mart) for each major business process. The comprehensive cohesion of the business is achieved by using another Kimball innovation, a data base standard. Understand how these two models are similar to how they are distinguishing to the reader a basic knowledge of the most basic database concept. We will also explore which organizational characteristics are best suited for each method. Mary Breslin has worked on the user and IT role, she is currently exploring the database of the University of Capella from the user side.
STREAM is a general purpose relationship data flow management system (DSMS).STREAM supports a statement of query language and flexible query implementation program. It is designed to process high data rates and large numbers of continuous queries through caution and approach answers when necessary. language design, algorithm, system design and implementation description, by the end of 2002, can be found in [3].
The parallel database machine architecture has been from using foreign hardware to a software parallel data flow architecture, based on traditional sharing - no hardware. These new designs provide an impressive speed and scale when dealing with relationship database queries.
In order to this potential, the questionnaire optimizer must know how and when to use the questionnaire's point of view. This article introduces a fast and extensible algorithm to determine that part or all of the questionnaire can be calculated from the questionnaire's point of view and describes how it can be integrated into the transformation based on the questionnaire. The current version of the processing consists of the point of view, adding and ultimately composing.
Concept models or similar data models are developed to catch the meaning of the application field, as someone perceives. In addition, the concepts used in similar data models have recently adopted object-oriented system analysis and design methods. In order to effectively use the concept model structure, their meaning must be strictly defined. However, often, the strict definitions of these structures are lacking. This situation is occurring in the case of relationship building. Experimental evidence shows that the use of the relationship is often problematic, as a way to communicate the meaning of the application field. For example, the concept model method of the user is often confused whether through the relationship, the entity or the property structure between the relationship, because the concept model of the concept model in the real model of the concept model of the concept model of the concept model of the concept model of the concept model of the concept model
The important requirement for the multimedia presentation is to be able to make up new multimedia objects from the existing, using the time relationship.When the composition of the continuous media objects is defined dynamically, the tasks of these objects are presented with new challenges.These challenges are addressed in this article.We show that in the case of a single copy of the objects, a predictive technology, a simple sliding, provides a way to reduce the delay and sliding requirements.We extend this predictive technology to simultaneously replying multiple copy of the objects.This new technology is called sliding sliding.We consider several variables of the buffer sliding algorithm.
Data storage and online analytics processing (OLAP) is becoming a key component of decision support as technology progress is improving the ability to manage and obtain a large amount of data. Data storage refers to a decision-making support technology designed to enable knowledge workers (execution, management, analysts) to make better and faster decisions " [1]. OLAP refers to performing complex analysis of data storage information. It is often managed by analysts and decision makers in a variety of functional fields, such as sales and marketing planning.
In this paper, we introduce a Network Prediction Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT), a Learning-Based Tool (WebPT)
When implementing a persistent object on a relationship database, a major performance problem is forecasting the data to try to reduce the number of circles of the database. This is difficult to navigate applications because future access is unpredictable. We recommend using an object as a future access forecast load background, where the background can be a stored relationship collection, a query result, or a complex object. When an object of O state load, the background of O is forecast similar status of other objects. We introduce a design to maintain the background and use it to guide forecast.
This dynamic environment requires fast, cost-efficient, time to the market for new or modified business processes, services and products. To support this important business demand, the next generation of DBMS must: 1. take advantage of the enormous investment in existing relationship technologies, both in data and skills groups; 2. use the flexibility, productivity and performance benefits of the OO model; and 3. integrate the powerful DBMS service quality production system. This article aims to provide a brief overview of the business SQL object database system.
The study introduced a modified B2BCRM using genetic algorithms and data mining technologies to improve decision-making.The model divides consumers into repeated and shop and go consumers.The modified data mining C5.0 and genetic algorithms are used to optimize the rules generated by decision-making algorithms.The findings show that the recommended model effectively allocates resources to the most profitable decision-making customers.
We propose a multi-resolution transmission mechanism that allows the various organizational units of the web can be transmitted and browsed, according to the amount of information obtained. We define the concept of information content, each organizational unit of the web text as an indication of its obtaining information. The concept of information content is used as a basis to define the concept of relative information content, to determine the transmission order of different units. Our mechanism allows a web client to explore more content containing parts of the web text in order to be able to end browsing irrelevant documents earlier. This program is based on our observation, different organizational units of the document helps different information quantities of the document. This multi-resolution transmission model on the mobile network is useful line line line line line line line line line line line line line line line line
We propose an indicator technology to quickly restore objects in 2D images, based on the similarity between their border shapes. Our technology is solid in the presence of noise and supports a number of important similarity concepts, including the best match, regardless of the direction and/or location changes. Our methods can also use standardized technology to deal with unchanged size matches, although optimization here is not guaranteed. We implemented our method and conducted experiments on real (handwritten numbers) data.
This article proposes an objective-oriented database system framework for an objective query optimization general method, which applies to a wide range of query categories, including the connection of repeated query expressed by a regular path and based on three components. The first is the description of logic, ODLRE, providing a type of system that can express: class description, query, viewpoint, complete query restrictions rules and conclusion techniques, such as non-conformity detection and complementary calculation. The second is the broad expansion of query, which includes the connection of repeated query expressed by a regular path, and based on three components. The first is the description of logic, ODLRE, providing a type of system that can express: class description, query, point of viewpoint, complete query and technical conclusion, such as non-conformity
With the increasing importance of XML, LDAP directory and text-based information sources on the Internet, we are increasingly needed to evaluate the queries involving (sub)string matching. In many cases, the match must be analyzed on multiple properties/size, with interrelationships between multiple dimensions. Effective queries optimization in this context requires good selective estimates. In this article, we use the predictive calculation of sufficient trees (PSTs) as a basic data structure for basic selective estimates. For 1D issues, we introduce a new technology called MO (maximum over). Then we develop and analyze two 1D estimation algorithms, MOC and MOLC, based on and one based on MO positioning, all possible integrity estimates in this background, we use the predictive calculation of sufficient trees
In order to take advantage of mature database technology, the RDF store is built on the top of the relative database, SPARQL query is maped for SQL. The use of shared non-computer groups is a way of achieving scalability by performing the top query processing of large RDF data sets in a distributed way. Considering this point, the current paper analyzes the impact of the relative graph design, when query is maped for Apache Spark SQL. A single three graphs, which is produced by the predictive division of the graph, a single wide graph covers all properties, as well as the combination of graphs based on the application model specifications, these graphs are considered to be designed for each referred method, the corresponding graphs in a single graph system use the corresponding graphs, using the corresponding graphs, using the corresponding graphs,
Data mining studies typically assume to analyze the data has been identified, collected, cleaned, and processed in a convenient form. Although data mining tools significantly improve the ability of analysts to find data-driven, most of the time for analysis is used for data recognition, collection, cleaning and processing data. Similarly, graph map tools have been developed to help automatize tasks using heritage or federal data sources for a new purpose, but assume the data source structure is easy to understand. however, data collection can be from dozens of databases, containing thousands of tables and thousands of fields, with a small reliable document attack or foreign key analysis system.We can develop a complex databases system that can develop a complex databases databases databases databases databases databases
While many of the recent work has been focused on the performance of the trading system, in the personal transaction has a deadline, our research deals with the sequence of data use in real-time applications and its integration with real-time resources management, especially the real-time data value and the real-time route and the non-state restrictions on competition control. Our research center is the idea of similarity, which is a reflective, similar relationship in a data object field. using similarity relationships, we propose a kind of effective data access policy for real-time data objects. We will also discuss a distributed real-time data access interface design.
It often occurs in practice, for example, when based on the graphic data model and query language Lorel, developed in Stanford, as the data of our work framework. The view of the semi-structure data can be used to filter the data and re-structure (or provide the structure) it. In order to the fast query response time, these views are often materialized. This paper proposes an increased maintenance algorithm of the materialized view of the semi-structure data. We use the graphic data model OEM and query language Lorel, developed in Stanford, as our work framework. Our algorithm produces a collection of query to update the computer's view of the database and develop the computer's view of the data model.
Inasmuch for speed to customers desires change and large completion that describes the day world. To guide technology and operating technology to form in general, achieving competitive advantage and special forms of design technology are the key to determining the nature and shape of the product, and what tolerable quality level of work is suitable for the product use, as well as all the characteristics and preferences are determined by design technology. For important CAD / CAM topics, we introduce in this study, providing the main components to the CAM system, and the style of this system to the detailed work steps, the detailed design steps.
Explore search requires the system to help the user understand the information space and express the progress of the search intention to perform the unusual exploration and obtaining information. We introduce the interactive intention simulation, a technology simulating the user’s progress search intention and visualizing them as the keyword interaction. The user can provide feedback from the system learning and visualizing an improved intention of estimating and obtaining information. We report the experimental comparison of the system of the variables implementing the interactive intention simulation to a control system. Data includes search logs, interactive logs, paper answers, and questionnaires showing significant improvement in the performance of the task, re-evaluating the performance of the information, the performance of the information, the user experience.
The performance of textabstract modern hardware is increasingly dependent on the proper use of memory cache series and the possibility of parallel execution in today's super-size CPUs. Recent database studies have shown that the performance of the database system has been seriously affected by the use of these resources. In previous work, we introduced the combination of algorithms, strongly accelerating the equi-join by adjusting the memory access mode to match the memory cache subsystem characteristics in comparison hardware. In order to make these algorithms applicable to the database system running on various platforms, we now introduce a encryption tool to automatically extract the relevant parameters of the memory system from any hardware.
For example, for a database representing the location of the taxi cab, a typical query may be: obtaining the current in 33 N. Michigan Avenue, Chicago within 1 mile of free cable; or for the data base of the truck company, a typical query may be: obtaining the current in 1 mile of the truck ABT312 (need help); or for the current location of the object on the battlefield, a typical query may be: obtaining a friendly helicopter in a specific area, or obtaining a friendly helicopter in the next 10 minutes.
While many researchers have studied the process of disintegrating transactions to increase the competitive steps, this study is usually focused on providing the necessary algorithms to implement the disintegration provided by the database application developers, and on what constitutes a desired disintegration or how developers should get a relatively less attention.We focus on the disintegration itself.
OOPSLA held 97 seminars on physical data management experience on Monday, October 6, 1997 at the Cobb Gallery Center in Atlanta, Georgia.
EAEcient support for setting assessment properties is likely to grow in importance because the object relationship database system, which both supports setting assessment properties, or suggests to do so quickly, begins to replace its purely relative pioneer. The setting assessment properties one of the most interesting and challenging operations is setting content portfolio as it provides a short and elegant way to express other complex queries. Unfortunately, the assessment of these portfolio is diAEcult, and the naive approach leads to the algorithm very expensive. In this article, we develop a new division based on setting content portfolio algorithm: division portfolio algorithm (JPS), using a multi-replicated division portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio portfolio
The query optimizer often restricts the access to the order of the search space, for example, by excluding the Carthage product in the subplan or restricting the plan to the deep left of the wine. This exclusion is widely assumed to reduce optimization efforts while trying to impact the quality of the plan. However, we show that the search for the full program space is cheaper than previously recognized, and the common exclusion may be few benefits. We start by introducing a Carthage product optimizer, which takes workstation time for a maximum of seconds of the search space of the Bush program of the product up to 15 relationships. Based on this result, we introduce a addition to the order optimizer, which reaches the similar performance level and retains the ability to include the Carthage product in the appropriate subplan.
Aqua is a system that provides fast, close-range integrated queries, which is very common for OLAP applications. It has been designed to run at the top of any business relationship DBMS. Aqua pre-calculates the original data (special statistics summary) and stores them in DBMS. It provides close-range answers and quality guarantees, by re-writing queries running in these combinations.
Patter is an IBM partner, winning the ACM SIGMOD Innovation Award, and also a member of the National Institute of Engineering, her doctorate from the Harvard University, so, Patter, welcome you! very much thanks. Patter, System R and INGRES show to the readers, can build a reasonable database management system with reasonable performance as part of the early system R team, you are there when a industry is born.
We introduce WSQ/DSQ (known as “wisk-disk”), a new method to combine the traditional database query facilities with the existing search engine on the network.WSQ, for Web support (database) query, shows the utility of WSQ with several interesting query and results.WSQ query results through a relative database to enhance SQL query.DSQ, for database support (Web) query, uses the database stored information to enhance and explain it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is a virtual query, it is
We propose a new method of calculation and storage of the database. Our idea is to use the Bayesian network, which can be for any query portfolio of properties and "Don't worry." a Bayesian network represents the basic common probability of the data, which is used to produce it. Through such a network, the recommended method, NetCube, uses the interrelationship between the properties.
We are developing a mobile passenger support system for public transport. Passengers can make their travel plans and buy the necessary tickets by accessing the database through the system. After the start of the journey, the mobile terminal check the travel schedule of their users by accessing multiple databases and collecting various types of information. In this application field, many types of data must be processed. Examples of these data are route information, price information, regional maps, station maps, scheduled operating schedules, real-time operating schedules, vehicle facilities, etc. According to the user's situation, different information should be provided and personalized.
In this program, the storage space is divided into two areas. one is the mirror area, which is characterized by high performance and low storage efficiency. the other is the RAID5 area, which is characterized by low performance and high storage efficiency. the hot data blocks are stored in the front area, while the cold blocks are stored in the back area.
This paper explores the use of a system call sequence to classify a system call sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence to classify a system sequence.
In this article, we introduce several new techniques to effectively build a cell-based space histograms range (window) summary to the four most important topological relationships: containing, containing, covering, and separating. We first introduce a new framework to build a multi-layer histogram composed of multiple Euler histograms and guarantees accurate summary results so that in a continuous time arranged the window. Then, we introduce an approximately algorithm in a ratio of about 19/12 to try to reduce the storage space of these multi-layer Euler histograms, although the problem is generally NP-hard to adapt to a limited space storage space, while the actual storage space is only approximately one of the Euler histograms allow storage space, it guarantees the accuracy of the storage
Introduction As the Internet has become an important component of everyday life, millions of users are now connected to the Internet. At the same time, more resources hunger and performance sensitive applications have appeared. Expandable and performance expectations have made the network infrastructure common features and copies. By guiding the work load away from the possible overload of the website server, the Web can expand and copy the address Web performance and can expand from the client and the server side, respectively. Expandable storage of a copy of data near the data consumer (e.g., in a web browser) to make the data access faster than if the content must be recovered from the server’s source.
Recent studies show that the storage awareness index exceeds the traditional main memory index, the storage awareness index focuses on better using each storage line to improve the search performance of a single search, no one uses the storage space and time location between continuous searches, we show that the traditional index, even the " storage awareness" person, suffers significant storage faults between visits.
We introduced a new model of similar time series, capturing intuitive concepts, two series should be considered to be similar, if they have enough not excessive time arranged the series of series are similar. The model allows a two series of width to any appropriate amount and its discount properly adjusted. The two series are considered to be similar, if one can be closed in one surrounded by a specific width around the other. The model also allows not matching gap in matching series. The matching series does not need along the time axis. Considering the similarity of this model, we introduce the quick search technology to find all similar series in a series of these technologies can also be found in a similar series (all of the series can be found in a similar series).
Chapter 1077 This paper focuses on describing the reliability of a property value allocation, as well as the abstractness of interesting parameters.We provide a sub-group/supergroup estimate on its statistics or relationships, we adopt a 80/20 rule, in particular, a p=(1? p) rule.This rule provides a distribution, generally referred to as ‘multifractal’ in the literature.We show how to estimate from data p (minority points and several moments) and introduce us to the experimental results of real data.
In this article, we presented an eAEcient method, called iDistance, for the nearest neighbor (KNN) search in a high-size space.iDistance separates data and chooses the reference point of each division. each division data is converted into a single-size space, based on their similarity to a reference point.
In the future mobile wireless computing environment, a large number of users equipped with a low-power radio top machine will consult the database of wireless communication channels.Palmtop-based units are usually not closed for a long time due to battery energy savings;Palmtop-based units are also frequently moving between different cells and connected to different data servers at different times.Closing of frequently accessed data projects will be an important technology that will reduce the narrow bandwidth of wireless channels closed closed closed closed closed closed closed closed closed closed closed closed closed closed closed closed closed
S3 is a prototype of the database system that supports the management and similarity of industrial CAD components. The main objective of the system is to reduce the cost of the development and production of new components by maximizing the repeated use of existing components. S3 supports the following three types of similarity queries: for example queries (the existing components in the database), scanning queries and subject similarity queries. S3 is an object-oriented system that provides the appropriate graphic user interface.
The intensity of the business query optimizer, such as DB2 comes from their ability to choose a best order by producing all the same re-orders of the binary operators. However, there is no known way to produce all the same re-orders of the SQL query containing attachments, external attachments, and portfolio collections. therefore, some re-orders significantly lower costs may be missed. using the super graphic model and a set of new identities, we propose a method to re-orders a SQL query containing attachments, external attachments, and portfolio collections. Although these operators are sufficient to catch the SQL series, which is in their re-orders, we find a strong original requirement for a dbms report.
DB-Prism is an integrated data storage system used for distributing financial and administrative control (data collection, processing and reporting) in the German Bank. It combines historical data terminal availability with high detailed reporting and planning facilities. The main components of interest include an OLAP system within the Terabyte range, as well as a meta database responsible for the entire process, from unusual data to individual OLAP reporting location and return to individual business activities.
XML is a widely recognized data exchange standard for tomorrow, as its ability represents data from a variety of sources. therefore, XML is likely to be a format through which data from multiple sources is integrated. In this paper, we study the problem of the integration of XML data sources through interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction of interaction
Effective support of XML query language data complex data optimization work is increasingly important, with the appearance of new applications, these applications can access a lot of XML data. All existing query XML (e.g., XQuery) recommendations are dependent on a model specific language, allowing route navigation and through XML data chart statistics to the authenticity of the required data elements. Optimizing these query results is critical depending on the existing short symphony chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart
Interview Stonebraker is one of Margo Seltzer, the founder of Sleepycat Software, the founder of Berkeley DB and is now the popular integrated database engine owned by Oracle.
Although there are many applications, an object-oriented data model is a good way to represent and seek data, the current object-based warehouse system cannot be processed with objects, its characteristics are uncertain. In this article, we extend the previous work by Kornatzky and Shimony to develop an algorithm to deal with the object-based uncertainty. We propose the concept of continuity for these objects base, as well as a NP integrity result, and a class of probability object base, its continuity is diversified examination of different rules. In addition, because some operations involve the connection and the division of events, and because the probability of the connection and the division of events are dependent on the original probability of events, and what is already known (if) the relationship between objects, we show all
In the future mobile wireless computing environment, a large number of users equipped with a low-power radio top machine will consult the database of wireless communication channels.Palmtop-based units are usually not closed for a long time due to battery energy savings;Palmtop-based units are also frequently moving between different cells and connected to different data servers at different times.Closing of frequently accessed data projects will be an important technology that will reduce the narrow bandwidth of wireless channels closed closed closed closed closed closed closed closed closed closed closed closed closed closed closed closed closed
In this meeting, my goal is to provide a background for such meetings, first about the topics related to the computer, second about the background of this special meeting. I think that I have the qualification of the keyboard as good as anyone: in this special meeting, I am now not directly involved in the type of computer work that we will discuss; the organization I represent, Bell Telephone Laboratories, is not in the business of making these computers.
Component-based methods are becoming more and more popular to support the Internet-based application development.The different component model methods, however, can be adopted, get different abstract levels (whether concepts or operations).In this paper, we introduce a component-based architectural application and discuss the concept of component as a building block for the development of electronic services, which are based on heritage systems.We discuss their features and applications in the Internet-based application development.
Many database applications need to store and operate different versions of data objects. In order to meet the various requirements of these applications, the current database system supports versions at a very low level. This article shows that the application independent versions can be supported at a significantly higher level. In particular, we extend EXTRA data model and EXCESS query language so that the configuration can be defined as concepts and non-programmes. We also show how the version set can be considered multi-dimensional, thus allowing the configuration to be expressed at a higher abstract level.
The sensors and embedded processors are becoming unclean.Their limited resources (CPUs, memory and/or communication bandwidth and power) present some interesting challenges.We need a strong and short "language" representing important features of data, which can (a) adapt and process regular components, including explosions, (b) requires a small amount of memory and a single data channel.
For the simulation and comparison of these index structures, it is essential for these structures to have effective cost prediction techniques. The previous technologies are either assumption of data unity or not applicable to high-size data. We recommend using samples to predict the number of index pages accessed during the query execution. The samples are independent sizes and retain the collective, which is essential for the representation of sliding data. We introduce a general model to estimate the index page configuration using the sample and show how to replace the errors. Then we offer a implementation of our model under the limited memory assumption and show it performs well under these limitations.
In this article, I first describe the background of the original ARIES recovery method, as well as its significant impact on the business world and the research community. Next, I provide a brief introduction to the different competitors control and recovery methods in the ARIES algorithm family. Next, I discussed some recent developments affect the field of trading management and what this means the future. In ARIES, the re-history concept is clearly an important paradigm. In my study of trading management in the direction of the Internet world, I observe history repeating myself in the meaning of the demands considered to be a significant main framework world (e.g., performance, availability and reliability) now becomes an important requirement for the wider information technology community.
This article describes how the database analysis system can use and use the full database storage system cost efficiency of the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active the use of the database storage system is active
From the point of view of supporting human-centric knowledge, the current mining association rules model suffer from the following serious defects: (i) lack of user exploration and control, (ii) lack of focus, and (iii) strict relationship concepts. In fact, this model as a black box, recognizes a few user interactions. In this article, we propose, a building, opening a black box, and supporting a restriction based on human-centric exploration mining association. The building is a rich restriction structure, including domain, class and SQL style integrated restrictions, allowing the user to clearly explain what association is mining.
In this article, we present the results we obtained by comparing and testing three known database intermediate software solutions. We analyze its features related to global directory and location transparency, transaction management, DML and DDL operations, SQL diameter masks, reference integrity, security, scalability, supported data sources and platforms, query optimization and performance.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
Oracle XML DB Storage is a virtual file system that shows the file directory format, this open protocol of XML storage access method can well support the knowledge base design, but can also easily manually manage the file.
The statistical method of processing natural language text has become dominant in recent years. this basic text is the first comprehensive introduction to the statistical natural language processing (NLP) appeared. this book contains all theories and algorithms required to build the NLP tools. it provides a wide but strict coverage of the mathematical and linguistic basis, as well as detailed statistical methods of discussion, allowing students and researchers to build their own implementation.
We solved the problem of finding a parallel plan for the SQL query, using two stages of methods to add orders, then parallel. We focus on the parallel phase and develop algorithms to use pipe parallelism. We develop parallel as a planning of a weighted operator tree to try to reduce response time. Our model response time captures the basic gap between the parallel execution and its communication top. We evaluate optimizing the quality of the algorithm, its performance ratio, which is the response time ratio, from the creation of the timetable to the best ratio.
DataMine is a statistical database mining system that focuses on the information produced by interactive and cute graphic representation. It also supports an offline mode discovery and provides a wide range of API, allowing users to write “mining application” as easy as daily database application, the core idea is to do the discovery with a “human in cycle” targeting system using its initial assumptions and system feedback. User can submit a rule request for a oriented base and time system can produce all the rules matching their requests.
In the web-based information storage access to the meta search engine, performance is a major problem. Effective query processing requires the appropriate query mechanism. Unfortunately, the standard page-based and double-based query mechanisms are designed for the traditional database to be ineffective web pages, where the keyword query is often the only way to obtain data. In this work, we studied the web page query series query issues and developed a web page query connection based on the signature file query mechanism. Our algorithms deal with the crossroads between the two series query and the corresponding query project.
The database research community is proud of extensible technologies, but the database system is traditionally not highlighted in an important extensible size: the extent of distribution that prevents the impact of the database technology on large-scale systems, such as the Internet.
We introduced a new database object called a cache table, which allows continuous cache of a remote database table whole or part of the content. the cache table's content is defined and in the setting time scheduled, or defined the dynamic and population demand in the query execution time. the dynamic cache table uses the characteristics of the typical trading web page application, with a large number of short transactions, simple equal forecasts, and 3 to 4 ways to join. Based on the federal query processing capacity, we developed a new set of database cache technologies: cache table, "Janus" (double head) query execution plan, cache limit, and synchronized cache population method. Our solution supports transparent content cache, as well as inter-net application response, through the network application, through the network application, through the
View is a derivative relationship based on the definition of (save) relationship. View can be achieved through a double database of a view stored. View provides quick access to data; Speed difference is essential in the application, high query rates, View complex or beyond the data in a remote database, so it is impossible to re-calculate the view of each query.
Some significant advances with multidimensional cases are not related to the algorithms, in the determination of the results of these algorithms, they have achieved a plan of accurate planning, in the last few years we have achieved a plan of accurate planning, in the last few years we have achieved a plan of accurate planning, in the last few years we have achieved a best algorithm, in which two algorithms, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one algorithm, one alg
Communication behavior represents a group of objects in the completion of tasks of dynamic evolution and cooperation. This is an important feature in the object-oriented system. We propose the concept of activity as a basic building block of declaration specifications of communication behavior in the object-oriented database system, including time arrangement of information exchange in the object communication and the performance of the activity. We formally introduce two types of activity component mechanisms: activity specialization and activity integration to abstract implementation of communication behavior. The first applies to behavior improvement of existing activities to special activities.
Key quality of the joint structure: Alisa S. Wolberg, Ph.D., F.A.H.A., Professor of Pathology and Laboratory Medicine, North Carolina University in Chapter 2. Surgery and Regional Anesthesia Management: Marc Samama, M.D., Ph.D., F.C.P., Professor and Chairman, Department of Anesthesia and Strong Care Medicine, Cochin and Hotel-Dieu University Hospital, Paris, France 3.
The extract of information from unstructured or semi-structured web documents often requires recording recognition and definition (“Recording” we refer to a entity-related information group) without the first extract of documents containing multiple records according to the record limits, recording information may not succeed. In this article, we describe a virtual method to find record limits in web documents. In our method, we catch the structure of the document as a tree of HTML labels, find the record containing the subterranean tree, identify the candidate separating labels in the subterranean tree using five independent virtualities, and choose a consensus separating labels based on a combined virtual method.
From Publisher: Mobile Computing Data Management focuses on the impact of mobile computing on data management beyond the network level. The aim is to provide a comprehensive and consistent overview of the latest progress in wireless and mobile data management. This book is written with a critical attitude. This volume shows the new issues introduced through wireless and mobile data access as well as their concepts and practical consequences. Mobile computing data management provides a single source for researchers and practitioners who want to keep the latest fields of innovation. It can also be a textbook for advanced courses in the field of mobile computing, or as a partner text for various courses, including distribution systems, database management, transaction management, or operating systems, information transfer or network computing.
Personalization, advertising and online data clarity produces a shocking amount of dynamic web content. In addition to web queries, visual materials have been shown to accelerate the production of dynamic web content. Visual materialization is an attractive solution as it separates the service’s access requests from processing updates. In the background of the web page, which visual to materialize must be determined online, we need to consider the performance and clarity of the data, we mentioned as the online visual choice problem. In this article, we define the data’s clarity measurement, providing an adaptive algorithm for the online visual choice problem and providing experimental results.
There are many applications for the availability of XML file overview data, from providing users with quick feedback on their queries, to cost-based storage design and queries optimization.StatiX is a new XML chart statistical framework that uses the structure generated by conventional expression (defining the elements in the XML chart) to highlight the location in the chart, which may be the source of the structure chart.As described below, this information can be used to build short, but accurate, statistical overview of XML data.StatiX uses standard XML technology to collect statistical data, especially the XML chart verifier, and uses the chart to summarize the structure and values in the XML document.
Many database applications require responsibility and tracking ability, which requires to maintain the previous database status. For a transaction time database supports this, select the time used for a timetable database record, determine the record is or is the current moment, must be consistent with the commands of the promised transaction series. The previous solution chooses the timetable in the promised time, chooses a time in accordance with the promised command. However, the SQL standard database may require a earlier choice, as a statement in the transaction may require "current time".
The market research company predicts a huge market, the service will be delivered to mobile users. the service includes route guidelines, point profit search, measurement services, such as road prices and parking payments, traffic monitoring, etc. We believe that no such service will be the killer service, but these integrated services are required.
The algorithm of the space data type or database system should (1) be entirely common, i.e., closed in setting operations, (2) is a formally defined sequence, (3) is defined as the final representation available in the computer, (4) provides facilities to the geological consistency of the related space objects, (5) is independent of a specific DBMS data model, but cooperates with any. We introduce an algorithm using the area as a geological field based on the space data type. A region, as a general database concept, is a final, dynamic, user defined structure, describing one or more system data types.
For the copy of the database many proposed protocols consider the central control of each transaction so that in the transaction, some sites will monitor remote data access and transaction commitments. We consider broadcasting transactions to the remote site AP forecast and processing these transactions in a complete form on each site. We consider two types of data: sharing private data and public data and showing only sharing private data transactions can be carried out under the local competitive control protocol. We take a synchronized network with the possibility of sharing failure. We show that in our plan, transaction execution can be managed with less communication delays rather than concentrated transaction control.
However, meaningful information exchange between independent design and population, dynamic, structural and semi-structural sources of information (HIS) remains a major challenge. pure Peer-to-Peer architecture naturally depends on this problem, as information sources are completely independent, in fact, a priority integration is unrecognized.
CORAL is a modular statement query language/programming language that supports the general Horn conditions, with a complex term, setup, integration, denial, and double relationship with the variables containing (generally quantitative). Supporting the permanent relationship is provided by using the EXODUS storage manager. A unique feature is that it provides a wide range of evaluation strategies and allows users to adjust the performance of a program through advanced reviews. A CORAL program is organized as a collective module, this structure is used as the basis of the expression control options.
Computing power and storage capacity increases, processing and analysis of large amounts of data plays an increasingly important role in many fields of scientific research. Typical examples of big scientific data sets include long run time dependency phenomenon simulation, regularly producing their state screens (e.g., hydromatic and chemical transport simulation to estimate the impact of pollution on the water body (4,6,20), magnetic hydromatic simulation, planetary magnetic field (32), simulation of a flame through a quantity(28), aircraft awakening simulation (21), raw materials and processing remote sensitive data files (e.g., AVHR(25), theme maps(17), MODIS(22), and medical image files (4,6,20), hydromatic simulation, image image image image image image image image image image
We consider the data map issues in the inter-data sharing system. These systems often rely on using the map lists for the corresponding values to find the data living in different couples. In this paper, we deal with the graphics and algorithms related to the use of the maps. We begin to discuss why the maps are appropriate for the data maps in the inter-data sharing system. We discuss these tables of alternative graphics, we introduce a language that allows users to specify the maps under different graphics.
In addition, the storage industry also provides significantly high data rates, the smart disk built behind reading and writing, as well as a new generation of high-speed series connections.The industry also encompasses the unprecedented (or independent) disk (RAID) technology Redundant Arrays - 1997's RAID market is expected to reach $1.3 billion.With this fast-growing market and technological base, parallel storage systems must develop beyond the RAID level 1 to 5.
Big database systems (for example, federal, warehouses) are multi-layer - that is, a combination of a database and (virtual or physical) view database.The smaller systems use a layer of view to hide the details of the physical and conceptual structure.We think most of the databases will be more effective if they are more user-centered - that is, if they allow users, administrators and application developers to work in their local view.
In order to support the dynamic settings of business processes between independent organizations, it essentially requires a formal standard format to describe business processes. ebXML framework provides a defined format called BPSS (Business Process Specification Format) which can be represented in two separate representations: a UML version and a XML version. However, the first, not to directly create business process specifications, but to define the specific elements and their relationships need to create a matching ebXML business process specifications. Therefore, it is very important to support the concept model, it is a good organization and directly matching the main model concept.
Real-time computing brings two new requirements to the database management. The first is time limit, requiring a better planning algorithm to meet the transaction period. The second requirement is temporary validity, or external consistency blocks (Lin89), data requirements: the real-time database must prevent data from being damaged, not only by carrying out current transactions, but also by delaying from sharing computing resources and real-time planning decisions. In this article, we study its external consistency requirements in the real-time database. We proposed a series-based competitive control program, prior to external consistency to traditional certification blocks.
This paper describes the design and implementation of the NAOS, an active component of the object-oriented database system 02. This work contributes to two main aspects. The first involves integrating the concept of the rule in the 02 model, providing the way of structural application. The rule is part of the plan, not belonging to a class. Program implementation and data manipulation, including method calls, can be driven by the rule. The second aspect involves how NAOS interact with the system 02 core.
Following these receiving conversations traditions, I will give me the idea where our field goes. any discussion of the information receiving (IR) study of the future, however, must be placed in the background of its history and relations with other fields. although IR has a very strong relationship with the library and information science, its relationship with computer science (CS) and its relative position as a subdiscipline of CS has been more dynamic.
Due to the size and complexity of 3D space data across the country, GIS software providers and service providers face many challenges when building a 3D space data infrastructure to efficient storage, analysis, management, and visualization of CityGML-based 3D urban models. Therefore, there is a strong demand for open and comprehensive software solutions that can provide comprehensive support for the above features. 3D urban database (3DCityDB) is a free 3D urban database solution based on CityGML.
Sybase is a leading RDBMS provider who has begun to provide OLTP systems for the client server environment and is currently mature as a data management solution provider within the enterprise. As an important part of the data management strategy within the enterprise, Sybase Replication Server supports data copying in a distributed environment. In this environment, the same data can be copied on multiple websites for rapid access to data and high data availability.
New telecommunications services and mobile networks are introduced in the telecommunications network. Compared to the use of traditional databases, telecommunications databases must meet the very strict requirements for response time, channels and availability. ClustRa is a telecommunications databases prototype used to operate through ATM interconnected standard workstations. To meet the channels and real-time response requirements, ClustRa is a major memory database, neighboring mainly, memory storage.
There is a lot of talk about how the internet will change the world economy, the companies will gather together in a way of "inserting and playing", forming a trading partner network, the virtual company will be built, a new business model can be based on access to information and agents, which can be transmitted to the world through a computer network.
In the past few years, many suggestions have been submitted on the model and query of a multi-dimensional database (MDDB). the strict classification of different types of levels remains an open question. in this article, we have submitted and discussed some different types of levels within one level. these levels are divided into different levels of integration of one level. according to them, we are discussing some OLAP operators’ characteristics, referring to the levels to maintain the data level consistency. in addition, we have proposed a group of operators to change the level structure.
In this article, we study methods to improve the performance of the algorithm to automatically store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store store
To use its inner (and often hidden) knowledge to improve business processes, the application of data mining technology is the only way to good and effective results, rather than pure manual and interactive data exploration. This paper reports a professional in Switzerland life begins mining their data resources from life insurance business. Based on the database MASY gathers all the relevant data from the OLTP system processing private life insurance contracts, established a data mining environment, which includes a series of tools for automatic data analysis, especially machine learning methods.
In this article, we study how to extend the classification method (such as OPTICS) to a very large database, using the data compression method (such as BIRCH or random sample). we propose a three-step program: 1) compressing the data to the appropriate representative objects; 2) applying the classification algorithm only for these objects; 3) restore the entire data set of the classification structure, based on the results of the data set, very efficient data set.
The University of Urm was founded in 1967, with a focus on medicine and natural sciences.In 1989, the University established two new disciplines: Engineering Sciences and Computer Sciences.This expansion takes place within the framework of the so-called scientific city of Urm.In joint efforts, the State of Baden, Industrial Companies, University and Urm City successfully established a research and development infrastructure in or near the university campus, by the University's Research Laboratory, the University's related research institutions, such as the Applied Knowledge Processing Institute (FAW) and the Industrial Research and Development Laboratory, especially the Grand Research Center of Daimler-Benz AG.
We introduce the design and implementation of the XSQ system to consult using XPath 1.0. using clean design, based on pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure pressure
The statements and opinions expressed in this supplement are by the author, not necessarily by the American Association of Osteoporosis, the JAOA, the Editor or Editor Advisory Board, or the author-related institution, unless clearly stated. The supplement is assigned to the JAOA by the American Association of Osteoporosis to carry out 2 hours of category 1B credit, the Osteoporosis Doctor who read it and returns to the test after completion.
The first command formula allows a natural description of queries and rules. Van Gelder's alternative fixed point mathematics extends the good basic mathematics of normal logic programmes to the general logic programmes with the voluntary first command formula in the regulatory institution. However, by standard translation to normal logic programmes, the implementation of the general logic programmes does not retain an alternative to fixed point mathematics. This paper introduces a direct method to target-oriented queries assessment of the general logic programmes.
In the intermediary system, query processing and optimization to get distributed non-owned sources raised many new issues. Cost-based query optimization is difficult because the intermediary has no access to the source statistics, in addition, it may not be easy to simulate the source performance. At the same time, query remote source may be very expensive because of the high connection, long calculation time, financial costs, and temporary inaccessibility. We propose a cost-based optimization technology to query the actual call to the source of statistics, so estimate the possible cost of the implementation of the program, based on statistical data query. We also provide a query result machine that allows us to effectively use the results of the query when we use the source of data query, we can use new data query results.
Int#ra-operator (or dividing) parallelism is a mechanism established for achieving high performance of the parallel database systems. However, how to use the internal operator parallelism in a multi-demand environment is not very good, the product. This paper introduces the detailed performance assessment of the internal operator parallelism in the parallel database system. A dynamic program based on the concept of matching the double traffic between the operators, showing good performance on various work loads and configurations.
Over the past few years, there has been an increasing interest in the database of mobile objects, where the movement and range of mobile objects are represented as the function of time. The paper focuses onining the constant K - the closest neighbor (k-NN) to the mobile point queries when updates are allowed. Update changes the description of the mobile point function that leads to the change of the event. The event is processed to keep the query results consistent with the point of movement. It has been shown to maintain a continuous k-NN query result for the moving point in this way can be represented by a significant decrease in the number of events processed in the present update. This point is by introducing a continuous query to the number of objects processed by a continuous query result.
The paper describes the methodology and implementation of a high-distributed system of data management, which is aimed at solving the problem of scalability and reliability of the development of the Square Postal Logistics applications, the core of which is from the Internet routing protocol, their scalability and intensity, creating a network built-in dynamic database index, and increasing the graph definition to optimize the use of the index.
This report describes the possibility of the DB/IS community to contribute to the progress of the Semantic Web, as well as the vision of the Semantic Web to the data base and information systems (DB/IS) researchers’ challenges or new research topics.The report is based on the NSF-OntoWeb invitation seminar on the DB/IS for the Semantic Web and Enterprise Research, held on April 3 to 5, 2002 in the Amicalola Falls National Park in the Northern Mountains of Georgia.
However, MapBase also shows many features of the classic information system: it provides a carefully managed, task-critical data central storage, written by customers in a variety of languages and running on a variety of hardware.
Encrypted databases, a popular way to protect data from damaged databases management systems (DBMS), using abstract threat models, capturing no real-world databases and no real-world attacks.
Time sequence appears in different applications.Many applications require time sequence to be considered continuous, where random values can be generated by a voluntary user defined exchange function from a clear value.This paper describes an extended SELECT operator, σ*, in the different user defined exchange assumption to obtain random values from a different time sequence.
We show the XISS/R system, a implementation of the XML index and storage system (XISS) at the top of a relationship database. The system is based on the XISS expansion of the booking number system, capturing the XML data's basic structure, and provides the possibility of storage and query processing independent of the data's specific structure. The system includes a Web-based user interface that allows the storage of files through the XPath query.
We have designed and implemented an object-related multimedia database. first developed a new data model, representing various types of media entities, as well as the temporary/logical relationship between these entities. then, our project proposed a three-layer object-related database infrastructure to support the proposed data model. in this article, we focus on search and query mechanisms for our database infrastructure. in particular, we discuss a new query language, developed for the SQL multimedia extension.
This work is a proposal for the database index structure, which is specifically designed to support the assessment of the XPath queries. Therefore, the index can support all XPath axes (including ancestors, followers, ancestors, descendants or self, etc.) The feature allows the index to stand out in the related work of the XML index structure, focusing on conventional path expression (equivalent to the XPath axes of children and descendants or self as well as the name test). its ability to start the transition from any background nodes in the XML document, in addition, it also allows the index to support the evaluation of the path transition embedded in the XQuery expression despite its flexibility, the new index can be performed and quoted through pure relative technology, but especially in the database, if it supports the XP
We define the content-based image indexing problem as a multi-dimensional closest search problem and develop/implement an optimistic advantage tree algorithm that can dynamically adapt to the indexing search process characteristics.Based on our performance research, the system usually only needs to touch less than 20% of the index input for good query, that is when query image is relatively close to their closest neighbors in the database.
The current paper introduces a short review of the current indicator technology, including Bitcoin's series representatives, and then introduces two methods we call Bitcoin's indicators and forecast indicators. A forecast indicator materializes a column of all values in RID order, while a forecast indicator is essentially taking a correct Bitcoin Bitcoin view of the same data. Although these concepts began with the model204 products, as well as Bitcoin and forecast indicators are now fully achieved in the Sybase IQ, this is the first rigorous review of the capacity of these indicators in the fonts that can be compared to the indicators and forecast indicators in all RID order, while a forecast indicator is essentially taking the same data.
Decision support applications involve complex queries on a very large database. as response time should be small, queries optimization is essential.
The increasing use of location-recognition devices such as GPS and RFID makes mobile object management an important task. In particular, in real-world applications required, the continuous query processing of mobile objects attracted significant research efforts. However, a small amount of attention was made to the design of continuous query processing of multiple user environments. In this article, we presented a competitive control protocol to effectively deal with the continuous query of mobile objects on the basis of the B tree.
The dynamic creation of the web pages today is immaculate, but the high demand for their resources creates a huge extensible problem on the server. The traditional web cache cannot solve this problem, because it cannot provide any guarantees for fresh cache data. A solid solution to the problem is the web page materialized, the page cache on the web server and constantly updated in the background, leading to fresh data access in cache attacks. In this work, we define the data quality measurement to evaluate how fresh data service to the user is.
These relationships can be further manipulated through relative algorithms, such as the study on the background of the "Document Score", Fagin et al. of the official information extraction framework. We study the complexity of the text query of joint query (CQs) and specific scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores scores
The study is based on the careful implementation of the algorithm, measuring the page I/O, and covers the entire transition closed algorithm as well as the partial transition closed (such as the successor of each nod in a specific source nod).We reviewed a wide range of details with the different density and "location" of the characters in the chart.We also considered the query parameters, such as the selectivity of the query, as well as the system parameters, such as the chart size and the page and the successor list replacement policy We show that there is a significant trade gap between the algorithm in this nod and determined the performance of the algorithm factors.
Relative database management system (RDBMS) is very successful in managing structured data with accurate definition of charts. However, the relative system is usually not the first choice of data management, where the charts are not predicted or must be flexible facing variations and changes. On the contrary, no SQL database system supports JSON often chooses to provide durability for these applications. JSON is a light-level and flexible semi-structured data format, they can use the corresponding data application, using the corresponding data format, supporting the corresponding structure in most programming languages. In this paper, we analyzed how to require the differentiation of the relative data management and the management of JSON data requirements. We introduce three building principles so that the RDB user can easily store in the JSON single chart, to make it easy for the JSON single chart
Therefore, if XML is to its potential, it takes some mechanisms to publish relationship data as a XML file. To this goal, a major challenge is to find a way to effectively structure and label data from one or more tables as a series of XML files. Different alternatives are possible, depending on when this processing occurs, and the number it takes place in the relationship engine. In this article, we will characterize and study the performance of these alternatives. In other things, we explore using new extensions and integrated features to build complex XML files. In this goal, we can explore a way to effectively structure and label data from one or more tables as a series of XML files. Different alternatives are possible, depending on when this processing occurs, and the number of it takes place in the relationship.
Today, Parallel Object Relationship DBMS is considered the next huge wave, but still lacks some of the effective implementation of the concept of recommended features. Therefore, one of the current objectives of Parallel Object Relationship DBMS is to move to higher performance. In this paper, we develop a framework that allows to deal with the user’s defined functionality and data parallelity. We will describe the categories of divisions that can be processed parallelly. We will also propose an extension that allows to accelerate the processing of another large category of features.
There is a lot of work in the compressed database index, but less in the compressed data itself.We review performance profits are carried out by compressing outside the index.A new compressed algorithm report that allows the processing of non-compressed data requires the combination of operations in a database built in a triple store.The simulation of the database performance with and without compressed results are given and compared with other recent work in this field.
Copy the salvation or the devil is hidden? these three achievements tell us
For a popular type of effective method (including multiple versions of Btree), data records and index entries are sometimes repeated to data separated by time. In this article, we introduce improved query processing technology in multiple versions of access methods. In particular, we solved the problem, avoiding repeat in the answer set. We first discussed the traditional methods to eliminate repeated use of hash and classification. Next, we proposed two new algorithms to avoid repeating, without using additional data structures. One of them is to do query, from one start, from one start, from one start, from one start, from one start, from one start, from one start, from one start, from one start, from one start, from one start, from one start.
In this article, we introduce a waste collection algorithm, known as transaction cycle reference calculation (TCRC), an object-oriented database. The algorithm is based on a variable reference calculation algorithm proposed by the functional programming language the algorithm keeps tracking assistant reference calculation information detection and collection cycle waste. The algorithm works correctly under the presence of current operating transactions and system failures. It does not get any long-term lock, thereby reducing the processing of interference. It uses additional calculation algorithm to detect; therefore, the editing code does not require additional reference calculation information to detect and collect cycle waste.
Information recycling (IR) engine can be ranked according to the text closer to the keywords in each document. In this article, we apply this concept to search the entire database objects are “approximate” to other related objects. Near search allows simple “focus” queries based on the overall relationship between the objects, useful interactive queries meetings. We see the database as a chart, with the data on the vertical (object) and the relationship specified the edge. Near is based on the shortest path between the objects. We have implemented a prototype search engine, using this model to search the keywords on the database, we find it is very effective to find the relevant information between the objects stored on the chart can be very expensive.
Data abstraction was initially considered a specific tool in programming. They also seem useful to explore and explain the ability and disadvantages of data definitions and operating facilities of today’s database systems. In addition, they may lead to new methods for the design of these facilities. In the first part, the paper introduces a distinctive method to determine data abstraction, and on this basis, gives accurate meaning to familiar concepts, such as data models, data types and database charts. In the second step, different possibilities to determine the data type in a data model are reviewed and described.
As the graphic data increases from the Web 2.0 availability and scale, graphic division becomes an effective pre-treatment technology to balance the work load. Because the entire graphic division costs are strictly prohibited, there are some recent attempts to work so that the flowing graphic division can run faster, easily parallel, and constantly updated.
Workflow management has recently gained more and more attention, as an important technology to improve the development of information systems in the dynamic and distributed organization. Developing a workflow application, the organization's choice of business processes is a model, optimized and defined as a workflow program, using the workflow language [2]. Workflow program is an example of workflow management system to control the execution of workflow, i.e., representing the real world of business processes [3]. The first-generation workflow management system (WFMS) is mainly developing a model and controlling the execution of business processes with a relatively static structure, performed in the same environment.
Object-related database systems allow users to define new user definitions types and features. This introduces the new optimizer and running time challenges of the database system in sharing the unparalleled architecture. In this article, we describe a new strategy that we are exploring the NCR-related multimedia database system; our focus is to guide the practical application we are seeing. In doing so, we will briefly describe the optimizer's challenges, especially the related predictions of the use of most media objects, such as video/audio clips, images and text files.
In this paper, we introduce effective and solid assessment techniques: (i) storage and access; (ii) view and exchange; and (iii) query multiple versions of XML files. First, we discussed the restrictions of traditional version control methods, such as RCS and SCCS, and then proposed new technologies to exceed its restrictions. First, we focus on managing the problem of binary storage and introducing a editing-based version program to improve the effective classification policy of RCS, based on the new page concept. However, large-scale XML files can support multiple versions of documents. First, we discussed the restrictions of traditional version control methods, such as RCS and SCCS, and then proposed new technologies in a simpler way, in a simpler way, in a simpler way, in a simpler way, in a simpler way, in a
In this article, we think databases technology can, should, provide a background for a variety of similar applications. More accurately, we present here the ActiveViews system, which, in the widespread use of the databases functionality of closed view, active rules (guides) and enhanced notification mechanisms, access control and logging/tracking user activity, provides the necessary basis for e-commerce. Based on the emerging XML standard (DOM, query language for XML, etc.), the system provides a new statement viewing specification language, describing the relevant data and activities of all participants (e.g., sellers and customers) engaged in e-commerce activities.
Since December 2000, as a small prototype designed to test XQuery static type system, the galaxy has now become a solid implementation designed to be fully compatible with the XQuery 1.0 specification family. Due to its integrity and open architecture, the galaxy is also obviously a very convenient platform for researchers interested in doing XQuery optimization experiments. We show the galaxy and its most advanced features, including supporting XPath 2.0, XML programming and static type testing. We also showed some of our first experiments, optimization.
In this work, we discussed the effective assessment of the XQuery expression of the continuous XML data flow, which is essential for a wide range of applications including monitoring systems and information transmission systems. While previous work shows that the automatic theory is suitable for the flight model recovery in the XML data flow, we found that the automated methods are not as flexibly optimized as the algorithm query systems. In fact, they implement a rigorous data-driven implementation paradigm. Therefore, we now propose a unified query model to enhance the automated processing and query-based optimization technology.
We face an active database system commitment to the results of the application developers use. The main problems faced are the lack of methodological support in analysis and design, the lack of standardization, the lack of development and management of the drivers tools, as well as low performance. We focus on performance as we found it is making users not willing to use the active rules, that is, the development of large applications. We show that using simple specific examples, optimizing large applications is separated by the transaction and the drivers and misunderstanding their delicate interaction difficulties. We believe that providing assistance to programmers, database administrators and database designers to optimize their application and the main application evolution is a strongly needed tool.
Some researchers are interested in the design of global network systems and applications. Our paper here is that the principles and technologies of the database community play an important role in the design of these systems. The starting point is the root of the database research: we generalize the concept of data independence to the physical environment outside of the storage system. We notice the similarity between the development of the database index and the new generation of structured to the right network. We observed through the database lens to describe the appearance of data independence in the network through some recent network facilities and applications.
The Active Object-Oriented Database System TriGS has been developed as part of a larger EC ESPRIT project, aimed at developing the next generation of production planning and control systems (Huem93) This paper aims to summarize the work of TriGS, which includes two aspects of the development of the active system itself, as well as guidelines on the active database design.
The big data set classification is an important data mining problem. Many classification algorithms have been submitted in literature, but research shows that so far, no algorithms are uniformly exceeding all other algorithms in terms of quality. In this paper, we introduce a unified framework called rainforest classification tree building, which classification algorithms are extensive aspects, from building the main features of the tree, determining the quality of the tree. The general algorithm is easy from literature (including C4.5, CART, CHAID, FACT, ID3 and extension, SLIQ, SPRINT and QUEST). In addition to its overallity, it is extended version of the classification algorithm, rather than a wide range of classification algorithm classification algorithm classification algorith
J2EE platform offers a variety of options to make business data sustainable using DBMS technology. However, the integration with existing background database systems has been shown to be critical for the scale and performance of the J2EE application, as the modern e-commerce system is very data-intensive. Therefore, the data access layer, as well as the link between the application server and the database server, in particular, is very likely to become a system bottle. In this article, we use the ECperf reference label as a real application example to describe the above issues and discuss how they can be approached and eliminated. in particular, we show how unsynchronized, information-based processing can be used to reduce the load of the DBMS system and improve the system performance, reliability and reliability.
These changes are the direct result of the new application requirements, such as Office Information Systems (OIS) and Computer Assistant Design (CAD). In this context, the object format requires different representativity in the disk and main memory, which is often applicable to the object reference. It is clear that these mechanisms are closely related to the object identification implementation pattern, as well as the combination of advantages of the strategy. All of these features are controlled by two object administrators. This article describes these mechanisms by implementing two object administrators' object information systems (OIS) and computer assistant design (CAD). We show these systems performance depending on their disk management and main memory format two object format can be discussed in the object format.
Abstract. supporting content in the network accessible information systems is increasingly common. we see this support with the use of ontologies and machine readable, registered documents. field simulation practice with extracting domain specific, background-related metadata also supports the use of semantics. these advances allow knowledge to find methods, defining the complex relationship between data is independently collected and managed. InfoQuilt (InfoQuilt system is one of the manifestations, as applied to geographical information as part of the NSF Digital Library II initiative is ADEPT-UGA system. the study is a part provided by the National Science Foundation, background-related metadata also supports this knowledge discovery method. these advances allow knowledge to find methods, defining the complex relationship between data is automatically collected and managed (InfoQuilt system, one of the Quilt
The next method of data storage is effective, in the last few decades, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective, the practice of the DIA is effective.
We study sliding window multiple connections processing in the data flow continuous query. some algorithms are to be continuous, increased connections, assuming that all sliding windows are suitable for the main memory. the algorithms include multiple connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections connections
In this article, we introduce a key component of the system, the information content of the system is based on the user's prophet familiar cells, and provides a background sensitive detection cells. There are three main modules of this component of the detection cells. A tracker, it is constantly tracking the part of the cell, a user access. A model, the component of the information in accessing the part of the model of the user's expected value in the invisible part. Information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information, the component of the information
In this article, we review the lessons we learned from using Bluetooth radio in the background of the sensor network, which apply to (a) the application designers choose the best radio requirements and (b) the researchers in the data management community need to make the assumptions about the sensor network.
Processor and memory performance increases and costs decrease result in the system intelligence transfer from the CPU to the surrounding area. Storage system designers are using this trend to “excessive” calculation power for more complex processing and optimization within the storage device. so far, this optimization has been at a relatively lower level of storage protocol. At the same time, the trend of storage density, mechanics and electronics is to eliminate the bottles in mobile data from the media and pressure on the connection and main processor moving data more efficiently. We propose a system called Active Disks, using the processing power on a single disk drive running application level code.
In this article, we study how to build an effective increase browser. browser selective and increase updates its index and / or local page collection, rather than regularly updating the collection pattern. increase browser can significantly improve the collection of "freshness" and bring new pages in a more timely way. we first introduce a experimental result in more than half a million web pages over 4 months, estimating how the web page develops over time. on the basis of these experimental results, we compare a variety of design choices of increase browser and discuss their transactions.
Publisher Review This chapter discusses with active XML distributed work space. the vast evolution of the network brings the need for the platform, allowing easy to deploy distributed data management applications. the current trend goes to these platforms distributed, especially to the pair of architectures. the active XML system provides the same data integrated platform, based on web standards, such as XML, and Web services. the system focuses on active XML system (AXML) file: the XML file, the content of which is part of clear XML data, while the other part is dynamically by calling to the Web service in the same or other pair of network services, the XML file has a distributed form of calculation.
In data storage methods, data from multiple sources are integrated, the information selected is extracted in advance and stored in a storage library. the data storage library (DW) can therefore be considered a collection of all content views defined by the source. when a query is submitted, it is assessed locally, using content views, without access to the original information source. using the application of the DW requires high query performance. this requirement is contrary to the need to keep the information updated in the DW.
For the purpose of solving the problem, by using the performance test system of the closed washing machine for water support work, the author designed a test bed to test the closed performance of the water cylinder in mineral water support providing a database of support for the purchase of the closed washing machine for water support manufacturers. in this article, there will be an introduction to the principles, rules and functional testing bed.
This chapter explores the basic functions of the quantum data model. It takes into account examples of the quantum model from biology and quantum physics measured. the data model in both cases is similar, but commonly used operations are quite different. in the second case, for example, a person is often interested in the behavior conditions of the local operator, such as zero flow surface, while in the biological case, a person has a mix of value conditions and geological state. In these two areas of application, this chapter emphasizes the universality and flexibility of the model. the system is based on a commercial database, adding a special function to manipulate the quantum data model.
Commercial parallel database systems such as DB2 Parallel (DB2 PE) [l, 21 provide the ability to perform complex queries on very large databases. However, the serial application interface of these databases can become an increasing list of applications, such as mail lists produced and data spread from a warehouse to smaller data labels. In this abstract, we describe the DB2 PE offers the CURRENT NODE and NODENUMBER features and show how these two features can be used to parallel queries the linear extension of the data, related to the number of nodes in the system.
The database is an integrated storage of information from distributed, independent, possibly unusual sources; in fact, the storage storage storage data is one or more substantial view; the data can be used for query and analysis of user applications; Figure 1 shows the basic architecture of the storage: the data is collected from each source, integrated with other sources and stored in the storage.
I am pleased to share with you the following three reminders, and I continue to invite unwanted contributions, please see http://www.acm.org/sigmod/record/author.html.
In this article, we show that for good performance, a primary XML query processing system should support these two processing paradigms of query programs. We describe our primary XML system and report experiments show that even for simple query, there are several methods to combine a structural combination of tree-based navigation and information-based access style, these options can have a wide range of performance.
In this article, we have proposed a new method to estimate the record selectivity of the database queries.The actual distribution of properties is adapted by using the curve matching function of the queries feedback mechanism.This method has the advantage that it does not require additional database access to collect statistics and is able to adjust the continuous distribution of values through queries and updates.The experimental results show that the estimated accuracy of this method is similar to the traditional method of statistical data collection.
Many real-time database applications occur in electronic financial services, security key facilities and military systems, implementation is key to successful business. We are here to study a new database management algorithm, providing the security of hidden channels, ensuring multi-level secrets in real-time database systems support applications for a fixed period of time. In particular, we focus on real-time database management. Our main contribution is as follows. First, we determine the importance and difficulties of providing secure database management in real-time database environment. Second, we introduce a new database management algorithm, providing the security of hidden channels.
Today, companies are looking for information solutions that enable them to compete in the global market, and in order to reduce risks, these solutions must be based on existing investments, allowing the best technology to be applied to the problem and can be managed.
The materialized chart or materialized query (MQT) is an additional chart with pre-calculated data that can be used to significantly improve the performance of the database query. The materialized query consultant (MQTA) is often used to recommend and create the MQT. The most advanced MQTA server works on a separate database server where the MQT is located on the same server space of the MQT where the base chart is located on a basic chart.
In the past few years, our detailed research efforts have been inspired by two different needs. on the one hand, the number of non-expert users access the final database is growing gap. on the other hand, the information system will no longer be described by a single of the same architecture, rather than several of the same component systems. to address these needs, we have designed a new query system, both user-oriented and multi-database characteristics models, we can use specific data description models and multi-database characteristics models, so the main components of the system are different conversion systems models, we will show a different visual interface through this model, we will provide a user with different and interchangeable interaction patterns and provide a "translation layer", which will create and provide the user with the illusion of the same structure of several of the same structures
As the size of the database increases to several hundred gigabytes or traps, the methods and tools required to automatize the process of extracting knowledge, or guide the user to the database subgroup, with special interests, are becoming highlighted. In this research paper, we explore identifying and extracting interesting knowledge issues, from the database living big data collection, using data mining technology. These technologies are able to identify models and build short models to describe data. These models can also be used to summary and approximation. We review the relevant work in OLAP, data mining, and close to questioning literature. We discuss the needs of traditional data mining technologies and adapt to the characteristics of OLAP. We can also use data mining technologies. These technologies have the ability to identify models and shorten the data model to describe these models.
We provide a principle extended SQL, called SchemaSQL, providing the ability to unify operations of data and charts in a relatively multiple database system. We develop a precise charts and charts in a way that extends the traditional SQL charts and charts and shows the following. (1) SchemaSQL keeps the taste of SQL while supporting the query of data and charts. (2) It can be used to convert the data of the database into a structure that is clearly different from the original database in which data and charts can be exchanged.
As part of the database adjustment, the index adjustment is the task of selecting and creating the index, which aims to reduce the time of query processing. However, in a dynamic environment of various advertising adjustment query, it is difficult to identify the potential useful index in advance. In this presentation, we introduce our QUIET tool to solve this problem.
Implementation of Object-Oriented Database System (OODBMS) failure recovery raises several challenging performance issues, these performance issues both arise from significant structural differences between OODBMS and traditional database systems, as well as the differences in the target application of OODBMS. This paper compares the performance of the implementation of failure recovery in a client-based OODBMS architecture. The four basic recovery techniques reviewed in the paper are the so-called page differentiation, subpage differentiation, full page logging and re-logging server. All recovery technologies are implemented in the background of the QuickStore, using the built EXODUS storage manager and using its database performance to compare.
Microsoft storage library is an object-oriented storage library, it is a component of Visual Basic (version 5.0). it includes a combination of ActiveX object architecture, a developer can use to define information model, and a storage library engine, which is the basic storage mechanism of these information models. the storage library engine is located at the top of the SQL database system. the storage library is to meet the continuous storage needs of software tools. its two main technical objectives are:. compatibility with Microsoft's existing ActiveX object architecture composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite composite compos
This note chart presents a series of published dissertations, technical reports, master's and doctoral dissertations, which study the various aspects of the performance of the object database.
Questions about XML files are typically combined with the choice of the content of the elements and through the way of expression, labelling the structural relationship between the elements. The structural portfolio is used to find all the elements that meet the original structural relationship, i.e. the parent-child and ancestor-heritage relationship. Effective support of the structural portfolio is therefore the key to effective implementation of the XML query. The recently proposed nod numbering scheme allows to catch the XML document structure using traditional indicators (such as B+ trees or R trees). This paper presents effective structural portfolio algorithms in the existence of the labelling indicators. We first use B+ trees and show how by accelerating the structural portfolio to avoid strong structural portfolio, these portfolio portfolio portfolio portfolio portfolio portfolio port
Many differences in its design include: data in order instead of storing, carefully encoding and packaging objects include main memory during query processing, storing a super-collected layer-oriented projections rather than current pricing tables and indicators, a non-traditional implementation transaction, which includes high availability and single-read transaction screenshots, as well as the widespread use of bitmap indicators to complement the B tree structure.
Data preliminary processing of data mining solves one of the most important problems in the knowledge obtained from the data process. The data obtained directly from the source may be incompatible, erroneous or most importantly, it is not ready to take into account the data mining process. In addition, the data increase in recent scientific, industrial and commercial applications calls for more complex tools to analyze it. Because of data preliminary processing, it can be impossible to convert to possible, adjusting data to meet the input requirements of each data mining algorithm.
This paper introduces double-sided samples, a new technique to estimate the size of two relationships. double-sided samples classify two groups in each relationship, divided and intense, according to the number of double-sided samples algorithms of the same combination value. Different estimates are used to focus on different combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combin
XML Stream Applications bring challenges to efficient processing of logged data. Although automatic models are naturally suitable for model matching in logged XML streams, the opposite algorithm models are accurate technologies for customized processing of self-contained modules.
Diversity, a key operation of data integration from multiple sources, is a time-consuming, working intensity and domain-specific operations. We introduce our ALIAS design using a new method to simplify this task, by limiting manual efforts to enter simple, domain-specific properties similarity features, and interactive labelling a small number of record pairs. We describe how active learning is useful to select information examples of copy and non-copy, which can be used to train a copy feature. ALIAS provides a mechanism to effectively apply this feature to the record using a new group-based implementation pattern.
The recent developments of space relations have led to their use in many applications involving space databases. This paper involves the acquisition of geological relationships on the basis of the minimum linear data structure. We study geological information, the minimum linear transmitting them to the actual objects, using the concepts of projection. Then we apply the results to the R tree and its variants, the R+ tree and the R* tree to try to reduce the disc access to the geological relationships queries. We also study the complex space conditions queries and discuss the possible expansion.
The number of databases of the Enterprise Resource Planning (ERP) systems, such as SAP R/3 is growing at a huge rate, some of which have reached a few traps size.OLTP (Online Trading Processing) this size of the database is difficult to maintain and tends to perform poorly.Therefore, most databases suppliers have implemented new features, such as level allocation, to optimize such task-critical applications. level allocation has been done in detail on the background of shared no allocated databases systems, but today's ERP systems mainly use a concentrated database and all the shared architecture.
We edited this library for our own purposes, but hopefully it may be useful for others too. all the texts appear in the following lists, generally available. we do not claim the library is complete, and covers the entire literary range of activities involved. we decide to focus on the methods, concepts, methods, and systems, to the entire space of the library, their efforts to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the library, to the entire space of the lib
More and more applications use XML data released from a relative database. For speed and convenience, these applications regularly hide this XML data and access it through a standard navigation interface such as DOM, sacrificing the DBMS for the speed provided consistency and integrity guarantee. ROLEX system is being built to expand the capacity of the relative database system, through a virtual DOM interface to provide the application with a quick, consistent and navigable XML view. This interface will convert navigation operations on a DOM tree to implement plan action, allowing a spectrum of possibility of virtualization.
MIROWeb Espris has developed a unique technology to integrate multiple data sources through the object relationship model with the semi-structure data type. In the solution, through the mixed model, it supports the relative object and semi-structure characteristics intermitted network sources and conventional relationship databases. Project data exchange format is XML, the new network standard and rotating language is XMLQL, a XML template-based query language from AT&T. The presentation will show the Oracle 8 and semi-structure charts-based intermitted data storage methods to support XML and XML query.
We presented a functional model to effectively survey the abstract collection of complex objects. The abstract collection is used for the model range, multi-value properties and indicators or query tables. Our model includes a functional language called OFL (object functional language) and a supported implementation model based on graphic channels. OFL can support any complex object algorithms and return as macro. It is a suitable target language for OCL-like query compilator. The implementation model provides a variety of strategies, including setting orientation and pipeline. OFL has been implemented at the top of an object manager.
Therefore, visual analysis is an appropriate and effective tool, so special requirements arise from the abnormality of data (different data types s, different data sources), the quality of data (lost value, error value), as well as a large amount of data. the visualization of marine data is especially imported within their geographical background and their time process. First, this paper introduces a classification of visualized space and space-related data, which is not only applicable to marine data.
ANSI SQL-92 [MS, ANSI] defines the isolation level in the sense of the phenomenon: dirty reading, unreplicable reading, and fantasy. This paper shows that these phenomena and ANSI SQL definitions cannot correctly describe multiple popular isolation levels, including the standard lock layer implementation.
The response time is the key difference between e-commerce (e-commerce) applications. For many e-commerce applications, the web page is based on the current state of the business storage of the database system. Recently, the web acceleration topic for the database-driven web application has attracted a lot of attention, both in the study of the community and the business stage. In this article, we analyze the factors that influence the performance and extensible web application. We discuss the system architecture issues and describe the method of deploying cache solutions to accelerate the web application.
Web server is increasingly used to provide dynamic content rather than static HTML pages. To create a web dynamic, the server needs to perform a script, which is usually connected to DBMS. Although CGI is the first method of the server side script, it has significant performance defects. Currently, there are many alternative server side script architectures that provide better performance than CGI. In this article, we report our experience using mod_perl, an Apache server module, which can improve the performance of the CGI script by at least one size order.
Traditionally, the database system is based on a standardized standard (e.g., Wisconsin, TPC-C, TPC-D) isolation assessment.We believe that such performance analysis is not common to reflect the actual use of DBMS in the "real world."Terminal users usually do not access a separate database system; on the contrary, they use a comprehensive application system where the database system constitutes an integrated component.To produce the actual relevance of the performance assessment of the terminal user, the application system including the database system must be based.In this article, we introduce the TPC-D reference results performed using the SAP R/3 system, an integrated business management system like many other application systems, the SAP R/3 is based on the business database related system.
In order to meet the needs of many real-world control applications, the concepts of temporary, real-time and active databases must be integrated: as the system’s data is considered to reflect the control of the environment, restoration and planning, it must be frequently updated time effectiveness; many activities, including the implementation of updates, work time restrictions; events occurring, for example, emergency events, initiation of actions. In these systems, meeting the time, predictability, and QoS guarantee requirements - through appropriate resources and load management - becomes very important.
In this article, we describe the design and implementation of OPT++, an extensible database query optimization tool, using an objective-oriented design to simplify the implementation, expansion and modification of an optimizer task. Using OPT++ building an optimizer makes it easy to expand query algorithms (adding new query algorithms and physical implementation algorithms to the system), easy to change search space and change search strategies.
Our algorithm of I/O, RAM, and CPU costs are, respectively, 4 7 5, 6, and 15, where and is the input size of the tree, is the size of the block, and. this algorithm can effectively use the residual RAM capacity to reduce the I/O cost.
From June to October, the database programming and design issues (Volume 11, Volume 6 to Volume 10) include a special series of temporary databases; the five articles of the series are copied here. Three separate case studies: a newborn intensive care unit, a commercial pet farming, and astronomical star catalogue, used to describe how temporary applications can be implemented in SQL. Effective time concepts with transaction time and current, continuous and non-continuous data processing restrictions, queries and modifications are emphasized. 1 Copying and copying This special series explores many issues that occur when trying to define and manage time data. These data are preventive. It is used to describe to prove each data data in the application, the data in each application, the data in each application, the data in each application, the data in each application.
Many applications today require a lot of real-time data processing. memory database systems have become a good alternative to these requirements. these systems keep the main copies of the database in the subjects to high input rates and low delay rates. However, the database in RAM is more vulnerable to failures than the traditional disk-oriented database. due to memory volatility. DBMS implements recovery activities (logging, checkpoint and reboot) recovery recommendations. Although recovery components look similar to the disk and memory-oriented systems, these systems implement their building components, such as data storage, indexing, competition control, query processing, sustainability and recovery. This survey aims to provide a deep data base recovery technology review for the realization of recovery activities, checkpoint and reboot goals.
OdeFS is a file-like interface to the Ode objective database. OdeFS allows the database objects to access and operate by standard commands, just as in the traditional file system. No collection is necessary so all applications can access the Ode objects. OdeFS as a network file server, using the NFS protocol.
The concept of variable independence was introduced by Chomicki, Goldin and Kuper in their PODS'96 paper as a means of adding a limited form of integration to limiting the query language whileining closed properties.
However, most of these methods may perform unnecessary checks if updates are irrelevant limitations. [Lee94] proposed a set called the relevant set that can be included in these work to reduce unnecessary checks. [Lee94] adopts a upward method and uses continuous and valuable features in limitations and valuable rules to reduce search space. In this article, we further expand this idea to use relative predictions rather than just continuous and valuable features in [Lee94].
In a subtle master copy database, transactions can be updated at one main nod a copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy copy
The paper describes the architecture and performance of ORACLE, a method to detect a unique radio from a mass of bit-like devices (the same hardware, protocol, physical address, MAC ID) using only the physical layer of IQ sample.
This paper involves finding output (exceptions) in the multi-dimensional data set. the identification of output can lead to finding truly unexpected knowledge in the fields of e-commerce, credit card fraud, etc., and even analyze the performance statistics of professional athletes. The existing method we see in the big data set to find output can only effectively process the two dimensions/properties of the data set. Here, we study the concepts of most DB (distance base) output. Although we provide form and experimental evidence to show the usefulness of the DB output, we focus on developing algorithms to calculate these output. First, we provide two simple algorithms, both with complex O(base), not based on N(base) numbers, not based on N(base) numbers, not based on N(
Physical database design is critical for query performance in a shared unparalleled database system, where the data is vertically divided between several separate nodes.We try to automatize the data division process.Given the workload of the SQL statement, we try to automatically determine how the data is divided on multiple nodes to the overall best (or close to the best) performance of the workload.The previous attempt to use the concrete rules to make these decisions.These methods cannot take into account all the mutual dependencies of query performance, usually by today's complex query optimizer model.We proposed a comprehensive solution that has been integrated closely with the business-shared parallel database system optimizer.
In this paper, we introduce a new and fast index system time sequence, when the distance function is any LP standard (p = 1; 2; : 1). One characteristic of the recommended method is that only one index structure is required for all LP standards, including the popular Euclidean distance (L2 standard). Our system achieved a significant speed beyond the art state: the best model of experimental and synthetic time. In this paper, we introduce a new and fast index system time sequence, when the distance function is any LP standard (p = 1; 2; : 1).
Recent research activities in the field of temporary databases have revealed some issues related to time definitions. in this article, we discuss the problems arising from the effective time definitions, as well as the effective time assumptions that exist in the current temporary databases methods.
Expert database systems extend the functionality of the traditional database systems by providing the facilities for creating and automatically implementing conditions of action. Although the conditions of action rules in the database systems are very strong, but due to the unstructured and unpredictable nature of the rules processing, they may also be very difficult to program. We provide static analysis rules methods; our methods determine whether a rules group guarantees completion and whether the rules implementation is confusing (there is a guarantee of a unique final state). Our methods are based on the analysis rules previous methods in the active database system. We significantly improve by providing the analysis standard previous methods: Our methods often determine a rule will end, or when the previous rules processing, our methods are based on the analysis rule, our methods are based on the analysis rule, our methods are based on the analysis rule
View selection is the choice of a set of views to be materialized on a database chart, therefore, the cost of evaluating a set of work load queries is the smallest, so that the view is suitable for a scheduled storage limit. View selection is two main applications to materialize the view in the database to accelerate the queries processing and select the view in the database to be materialized in order to answer the decision support queries. In addition, the view selection is a core issue to perform the intelligent data positioning on a wide regional network, so that data integrates applications and data management is reliable. We describe several basic results of the view selection issues. We consider the view and work load issues, including equal selection of projects and queries, and display the complexity of the view queries to make quality queries critical.
This paper describes the principles of the interactive and teaching interactive mechanisms built on the basis of the European knowledge pool system, developed by the European research project ARIADNE, which is the core characteristic of ARIADNE, distributed by a backup of the teaching documents (or learning objects) of various sizes, origin, content, type, language, etc. They are stored in their use (and reuse) of the system on the basis of electrical training or teaching courses.
This article introduces the general algorithm of competitive control of the tree-based approach, as well as a recovery protocol and a mechanism to ensure repeatable reading. The algorithm is developed in the background of the General Search Tree (GiST) data structure, an index structure supports an extended query and data type. Although developed in the tree-based background, the algorithm is generally applicable to many tree-based access methods. The competitive control protocol is based on the original B tree-based development of the linking technology and completely avoids keeping the nodes lock during I/Os.
The cost of the query program depends on many parameters, such as predictive options and available memory, whose value may be unknown at optimization time.
Commercial database does not support near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near near
In this paper, we reviewed the structure of temporary data and developed a temporary data structure independent of any traditional data model, such as relative data model or network data model, unlike many other expansion existing models that support temporary data, our goal is to describe the properties of temporary data and operators without being affected by traditional data models that are not specifically designed for temporary data models.
A method to generate an approximate answer to answer a question to a database, one of which SQL query Q in the database running a relationship R gets. Relationship R has a related histogram H. SQL query Q is translated into a query Q′ running in histogram H. Translated query Q′ in histogram H to get a result histogram.
The paper discussed the perspective issues in the web background, we introduced a set of data management and reorganization languages from the worldwide network, we introduced a specific data model, called the ARANEUS data model, inspired by the structures that are commonly present in the web page, which allows us to describe the web supertext program, in the spirit of the database, based on the data model, we developed two languages to support a complex visual definition process: first, known as ULIXES, used to build the web database views, then can be analyzed and integrated using the database technology; second, known as PENELOPE, allows to define the web supertext from the relative view.
Buffett administrators are essential to the performance, scalability and reliability of Oracle’s Universal Dam Server, high-performance data base administrators, providing powerful data management services for various applications and tools.The rich features of the Universal Data Server pose special challenges to the design of the Buffett administrators.The Buffett management algorithm must be extended and efficient within a wide range of OLTP, decision-making support and multimedia work loads, requiring very different competitiveness, channels and bandwidth requirements.
In this article, we presented an effective algorithm mining association rule, essentially different from known algorithms, compared to previous algorithms, our algorithms not only significantly reduced the top of I/O, but in most cases also lower the top of CPUs, we conducted a wide range of experiments and compared the performance of our algorithms with one of the best existing algorithms.
We describe an investigation on e-mail content mining to identify the author, or authorized the author, for the purpose of conducting a legal medical investigation.We focus on the ability to discriminate between the authors, both integrated e-mail topics, as well as various e-mail topics.Extended set of e-mail document functions, including structural characteristics and language patterns, is derived, and together with supporting Vector machine learning algorithms, used to mining e-mail content.Using various authors on a set of topics, the experiment of e-mail files, provides future results for integrated and multi-topics authorized classification.
Maintaining data consistency is known to be difficult.The latest approach depends on integrity limitations to solve problems - the correct and complete limitations of the natural work to data consistency.The most advanced data cleaning framework uses the formalism known as rejection limitations (DC) to deal with various real world limitations.Every DC expresses the relationship between the prophecy, indicating which combinations of properties are incompatible.
In this framework, the installation chart explains the operation must be installed in a stable database, if it is recoverable, this installation chart is a significantly weaker operation command, rather than conflict chart from the competitor control. We use the installation chart to explain (i) a cache management algorithm to write data from volatility cache to stable database, (ii) a RDO test specification used to select the operation in the record to replay during recovery, (iii) a strong recovery algorithm based on this test; We prove these cache management and recovery algorithm are correct. We use the installation chart to explain (i) a cache management algorithm to write data from volatility cache to stable database, (ii) a cache test specification to select the operation to replay during recovery,
While expanding to the vast and growing Internet population with unpredictable use patterns, e-commerce applications face serious costs and administrative challenges, especially databases servers, these applications’ supporters are deployed in multi-layer configurations. medium-layer databases cache is a solution to solve this problem. In this article, we introduce a simple expansion to the existing federal features in DB2 UDB, allowing a conventional DB2 example to become a DBCache without any application modification. On the application server deployed a DBCache, voluntary SQL declaration, unchanged application, designed for the backdrop databases servers, can be answered: in cache, on the backdrop databases servers, or in two ways distributed in this article, we propose a simple expansion to the existing federal features in DB2
1.S E-Commerce Officials Copyright Agent E-Commerce Services Symbolic Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive Instructive
This article discusses the performance of the distributed database system. in particular, we introduce an algorithm to dynamically copy an object in the distributed system. the algorithm is adapted as it changes the copy of the object, i.e., the combination of the processor, the copy of the object) because the change occurs in the reading of the father (i.e., the number of reading and writing issued by each processor). the algorithm continuously moves the copy of the object to the best one.
A significant feature of many advanced active database prototypes is that they support the rules caused by complex event patterns, their complex event language provides a strong primacy for the temporary rationalization of events, in fact, one of the important exceptions is that its expressive power fits and goes beyond the complex language provided by the Time Series Management System (TSMS), which is widely used for temporary data analysis and knowledge discovery.
This paper introduces the General Search Tree (GiST), an index structure that supports an extensible query and data type. GiST allows new data types to index in a way that supports the natural type query; which is contrary to the previous work of the tree extensibility, only supports the traditional equality and range forecast. In a data structure, GiST provides all the basic search tree logic requires a database system, thus combining different structures, such as B+ Tree and R Tree in a single code, and opening the search tree application to general extensibility. To show the flexibility of GST, we offer simple methods of implementation, allowing it to be like B+ Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree
Edit comments: Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments Edit comments
The availability requirements of the database systems are more rigorous than ever before, and the database is widely used as a business base. This paper emphasizes Fast-StartTM Fault Recovery, an important availability feature in Oracle, designed to quickly restore unplanned faults. Fast-Start allows administrators to set a operating system to set predictable limits within the time required for accidental recovery. For example, fast-start allows accurate controls in turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn to turn
CoDecide is a experimental user interface tool package that provides an extended to broadcast table concept, specifically aimed at supporting the multi-dimensional analysis of the data in the storage of the material. It is different from the previous recommendations, through direct support of mining/transfer analysis, without a re-designed interface; More importantly, CoDecide can connect multiple points in a storage of sync or sync cooperation by multiple analysts, through a concept model showing the size of the problem on the so-called map.
We introduced a logical formalism to define the dynamic behavior of the database. The evolution of the database is characterized by the description of the characteristics of the transition of the country and the implementation of the dynamic integrity of the transition of the country. Our formalism is based on the logical variable of the first case, the calculation state is a clear object. The integrity limitation and the transaction are uniformly defined as the expression of our language. We also point out the formalism applied to the verification and synthesis of the transaction.
As parallel computing becomes more and more common, it requires extensive and efficient way of storing and location data becomes more and more drastic. over the years, networks and cloud computing have distributed data on machines and even groups in different geographical locations (websites). however, not all websites need all data in a specific data set, or have (maybe special) processing capacity needs. These facts challenge the traditional wisdom, we should always move computing to data rather than data to computing. Sometimes the data actually needs is small.
To allow modern data intensity applications, including data storage, global information systems and e-commerce, we have to solve a source (genetic) database that is maped as a different, but fixed, target map problem.
Solid disk (SSD) provides faster random access to data than traditional hard disk drivers. Therefore, the response time of the database engine can be improved by transferring the objects of frequent random access to the SSD. Given the price of the solid disk and limited storage capacity, the database administrator needs to determine which objects (tables, indicators, materialized views, etc.) if placed on the SSD, the most will improve the performance of the system. In this article, we propose a tool called "Ob Plaject Advisor" to make a wise decision for the object positioning issue. By collecting the objects of frequent access in the work load to the SSD, the consultant provides an object list to apply to the SSD applications, such as the CD recurrence, the database administrator needs to determine the objects (which tables, which tables, tables, tables, tab
In 2001, George Mason University's Enterprise Engineering Laboratory was signed by Boeing to develop the eHub capacity of Taiwan Aerospace Provider. In the lab environment, core technology was designed, developed and tested, now Taiwan's large first-level aerospace provider is commercializing technology. The project aims to provide layer networks and application services to transport XML-based business transaction flows, multi-layer, unusual data processing environments. This paper records business scenes, eHub applications and network transportation mechanisms, used to build a n-tier center. Unlike most eHubs, this solution takes the supplier's view, according to the supplier's requirements; hence, improves the supplier's reliability.
The majority of the databases contain “name permanent” such as route numbers, personal names, and location names, equivalent to the entities in the real world. The previous work on integrating the unusual databases has been assumed that the local name permanent can be through a standardized map to a suitable global domain. However, in many cases, this assumption does not remain; determining the two names permanent should be considered the same can require detailed knowledge of the world, the user's purpose, request, or both. In this article, we reject the assumption that the global domain can be easily built, rather than the assumption that the name is provided in the natural language text.
In a big data recording and storage environment, it is often useful to provide quick, near-range query answers when possible. Before DBMS provides very accurate near-range answers, it can become a reality, it requires the development of many new data summary and estimated data response techniques. This paper introduces two new sample-based statistics summary, short samples and calculated samples, and introduces new technologies to rapidly increase maintenance, no matter how data is distributed. We measure their advantages, exce the standard sample view, from the number of additional samples to the same view size, thus providing more accurate query answers.
Building a high-performance database system, combining the best aspects of the relative and objective-oriented methods requires the design of the client server architecture, which can fully exploit the client and server resources in a flexible way. The client server query performs two dominant patterns are data transmission and query transmission We first define the limitations of these policies, they are in query optimization during the choice of the operator's website. We then study the performance transactions between them, for large-scale query processing. Although each strategy has advantages, no one in itself is effective in a variety of situations. We describe and evaluate a more flexible policy called mixed server, which can perform query client, server, or any combination of two.
We studied the issues of information selectivity in the P2P network, we introduced the classification work of data models and text information dissemination, and discussed an exciting P2P architecture that motivated our efforts, we also investigated the results of our computing complexity on the three related algorithm issues (request satisfaction, input and filtering) and provided the most critical (filter) algorithm for these issues.
While the structure is similar to the traditional histograms, these histograms infer data distributed, rather than by reviewing data or their samples, but by using the search engine to perform feedback on the actual selectivity of the range of operators, gradually improving histograms.
Lore (Lightweight Object Repository) is a DBMS designed to manage semi-structure information.Lore implementation requires re-thinking all aspects of DBMS, including storage management, indexing, query processing and optimization and user interface.This article provides these aspects of the Lore system and other new features, such as dynamic structure summary and smooth access to data from external sources.
Most of the data we encounter has a spatial (geographical location) aspect, but this is not easy to use by traditional RDBMS. Over the past five years, there has been a mixture of Geographic Information Systems (GIS) technology, RDBMS architecture and SQL standards, promoting the implementation of RDBMS internal spatial processing. This article will introduce a short spatial processing and technological evolution that leads to the development of IBM DB2 spatial extender, using IBM DB2 universal database (UDB) objects related support to implement standard-based SQL spatial capabilities.
In the context of current climate change, it is important to predict the most serious events that may occur to adapt to the energy system planning and management. However, since these events are rare, any estimation of their frequency is uncertain, a big data sample is necessary to reduce any sample uncertainty. In this article, we introduce a way to combine past observations and climate model simulation to produce a very large sample for a period of time from the nearest past to the nearest future. It is based on the temperature signal disintegration to the decisive part (humid trend and seasonal deviation in the middle and standard) and Stokster remains once the signs are disintegrated enough time (at least 30 years), the time series can be used to build climate simulation and seasonal simulation.
A Query by Humming system allows the user to find a song by humming part of the tune. No musical training is needed. Previous query by humming systems have not provided satisfactory results for various reasons. Some systems have low retrieval precision because they rely on melodic contour information from the hum tune, which in turn rely on the error-prone note segmentation process. Some systems yield better precision when matching the melody directly from audio, but they are slow because of their extensive use of Dynamic Time Warping (DTW). Our approach improves both the retrieval precision and speed compared to previous approaches.
ATLAS will be one of the four OBHC (Large Hadron Collider) particle accelerator detectors currently built in CERN, Geneva. The project is expected to start producing data collection meetings, errors and errors (15 to 20 years) to produce approximately one-fifth of the particle physical data per year. This vast amount of information will require a variety of meta data storers, which will allow the analysis and understanding of the physical data of the final user (physicians are analyzing). Data storing physical storers and tools in ATLAS can solve such problems as the logical organization of physical data, according to data collection meetings, errors and errors of data storing time (15 to 20 years).
For example, in the database of the bank’s customer, “age” and “balance” are two digital properties, while “CardLoan” is a Boolean properties. Taking a pair (age, balance) as a point in the two-dimensional space, we think a association rule form (age, balance) &isin; P) &rArr; (CardLoan = yes), which means that the bank’s customer, his age and balance falls in a balance zone P tends to use card loans with a high probability. We consider two categories of regions, straight-country and acceptable areas (i.e. connected areas and X-monone). For each class, we propose an effective calculation rule form (age, balance) &isin; &r; &r; (CardLoan = balance), which means that the bank’s age
Exchange Rate Control has gained significant attention in multiple databases because of their characteristics as abnormality and autonomy. in particular, in literature, a variety of exchange rate control protocols have been developed. in this paper, we presented a protocol that guarantees two levels of feasibility standards and is built according to the top methods.
With the appearance of XML as the existing formats of data exchange and semi-structured databases, query languages for XML and semi-structured data have become increasingly popular. Many such query languages, such as XPath and XQuery, are in the sense of navigation, their variable mandatory paradigm requires programmers uncertain route navigation through the document (ordata project).
One of the most challenging issues in this field is the wireless query, in which the embedded query can take any form, including integrated and generalized. Although there are already several proposed wireless query technologies, the majority of these technologies apply only to a few cases. We believe that the lack of a general and simple solution to wireless query is due to the lack of a unified algorithm to process all operations in the same way (including integrated and quantalized).
Weight loading refers to the process of creating an index target data set. This problem is clearly B tree, but so far, the non-traditional index structure has received moderate attention. We are interested in the fast gen weight loading technology, its implementation is using only a small interface, meeting a wide range of type of index structure. Gen technology is very attractive expansion database system, because different users implement the index structure implementation, this small interface can be optimized weight loading, without any modification of the gen code. The main contribution of the paper is the proposed two new genes and conceptual simple weight loading algorithms. These algorithm weight loading algorithms are using a new gen weight loading algorithm, referring to the same type of index, referring to the same type of index.
The new applications in the field of biotechnology have a great commitment to promoting the health and well-being of the global community, especially in developing countries. However, significant concerns about biotechnology in the multinational field have arisen, no doubt will increase concerns in the coming decades. The article aims to evaluate the strength and limitations of existing international standards and structures, aimed at solving these concerns and proposing the means to increase the existing structures to make them more effective.
For future data mining studies, a beneficial direction will be the development of technologies that include privacy issues. Specifically, we have solved the following issues. Because the main task of data mining is to develop a model on integrated data, we can develop a precise model without access to the precise information of personal data records? We take into account the specific cases of building a decision-making tree classifier from the training data, where the value of the personal record is interfered. The result data record looks very different from the original record, the distribution of the data value is also very different from the original distribution.
Many applications use sensors to monitor entities such as temperature and wind speed. a centralized database tracks these entities so that queries are processed. due to these values and database values are continuously changing and limited resources (e.g., network bandwidth and battery power), it is often incredible to store accurate values at any time. similar circumstances exist in the mobile object environment to track the constantly changing location of the object. in this environment, it is possible that database queries produce errors or incorrect results based on old data. however, if the degree of error (or uncertainty) is controlled between the actual values and database values, we can put more confidence in queries answers.
Sensors are often used to monitor the constantly changing entities, such as the location and temperature of the moving object. Sensor read reports to a centralized database system and then used to answer the query answers. Due to these values and limited resources' continuous changes (e.g., network bandwidth and battery power), the database may not be able to track the actual values of the entity and use the old values to replace. using these old values the query may produce the wrong answers. However, if the uncertainty between the database values and the database values is limited, it can be placed more confidence in the query answers. In this paper, we introduce a framework that shows the data's uncertainty.
Recent technological advances make multimedia servers available. Two challenging tasks in these systems are: a) to meet the real-time supply requirements for items in a particular bandwidth, b) to effectively serve multiple customers simultaneously. to complete these tasks and a large-scale economy related to the service of a large group of users, multimedia servers may require a large disk system. although a single disk is quite reliable, a large disk farm can have an unacceptable high disk failure possibility. in addition, due to real-time limits, the reliability and availability requirements of the multimedia systems are very strict. in this article, we study technology to provide high reliability and availability, low disk storage, disk storage and disk storage costs.
Over the past decade, the Internet has revolutionized many aspects of life, scientific publications are just one of the many companies that are connected and influenced by the World Wide Web, and now most scientific magazines have a network, so it’s still surprising that ACM generally and TODS occupied the unique capacity of the network to help spread knowledge, where I summarize the different and widespread ways of using TODS in all publication stages.
As long as there are DBMS and their applications, they are interested in performance features that these systems show. This month's column describes some of the recent work that occurred in TPC, transaction processing performance council.TPC-A and TPC-B are well-known reference indicators you may have heard of the past.TPC-C V3.5 is the current reference indicator of the OLTP system. introduced in 1992, it has been running on many hardware platforms and DBMS.
S-MAC and PAMAS are two MAC protocols, regularly placing the nodes (ractional choice) sleep to energy savings. Unlike these protocols, we propose a method in which the nodes tax cycle (i.e. sleep and awakening timetable) is based on their key. A distributed algorithm is used to find a set of winners and losers, who are then allocating the appropriate slots in our TDMA based on the MAC protocol. We introduce a nodes of energy key concept (ractional choice) sleep to energy savings. Unlike these protocols, we propose a method in which the nodes tax cycle (i.e. sleep and awakening timetable) is based on their key.
We consider to deal with the existing top N queries in a distributed environment with possibly not cooperating with the local databases system issues. For a top N queries, the problem is to find a user queries a large number of databases. therefore, it is ideal to provide a facility in which one user queries is the best, but not necessarily a completely effective way. Top N queries are popular from the corresponding databases and then search results are very useful e-commerce applications. Many companies offer the same type of goods and services to the public on the site. Many companies offer the same type of products and services to the public on the site. therefore, these collaboration methods are different. the corresponding databases can be managed to a large number of data queries.
Although the database research has made decades of progress, it is surprising that scientists in the life sciences community are still struggling in the search for the ineffective and terrible tools for the bio-data set, which work emphasizes a specific issue involving the search for a large amount of protein data set based on its secondary structure. In this paper, we define an intuitive query language that can be used to express the secondary structure query and develop several algorithms to evaluate these query. We implement these algorithms, whether in Periscope, we build a local system, or in a commercial ORDBMS. We show that the choice of algorithms can have a significant impact on the query performance as part of the Periscope implementation, we also developed a framework to optimize these query and accurate query costs, which we can estimate
P2P computing has become a very popular topic in computer science. it affects a variety of fields such as networks, distributed systems, information systems, algorithms and databases. P2P paradigm introduces a building principle of "transforming" client server computing paradigm. It is based on the concept of distributed and resource sharing. by avoiding central bottling and distributed work loads, it promotes the implementation of applications globally using P2P paradigm as a practical.
In the data transfer to the customer environment, a key consideration is that the customer can communicate with the server with a low bandwidth. Advanced applications in this environment need to read interconnected data, as well as current data. However, given the customer's immaterial communication capacity and needs in the mobile environment, the traditional reliability-based methods are too limited, unnecessary, unpractical. Therefore, we recommend using a weaker accuracy standard called updating consistency and based on this standard output mechanism, ensuring (1) the server keeps the data interconnected and the customer reads the data in the currency. Using these mechanisms, the customer can get the data is current and interconnected "air", I can contact the server without interconnected accuracy, we can say if the accuracy of the data is referred to as
Considering a big database of transactions, each transaction consists of a set of items, and a tax (i.e. a level) of items, we find the connection between items at any level of tax. For example, considering a tax saying that the jacket is the jacket, we can mention a rule, “a person who purchases the jacket tends to buy the jacket two rules.” This rule can even beined, if the rule “a person who purchases the jacket tends to buy the jacket” and “a person who purchases the jacket tends to buy the jacket” does not keep.
The rapid expansion of mobile computers, wireless data networks, vehicle navigation, multimedia and database systems has led to the development of powerful mobile information systems, which are composed of laptops (laptop, PDAs) with large storage capacity, the ability to connect wirelessly to the global information network and provide users with many features such as access to WWW, shopping, banking, booking and other transactions.
Mobile Advertising Network (MANET) is a emerging field of research. The majority of the work is focused on routing issues. This paper discusses issues related to data communications with MANET databases systems. Although data tweet and data tracking methods were previously processed in mobile networks, the proposed methods do not deal with the unique requirements related to MANET. Unlike the traditional mobile networks, all nodes within MANET are mobile and battery power. Existing wireless algorithms and protocols are mainly insufficient because they do not take into account the mobile and power requirements of the client and server.
We consider the data map issues in the inter-data sharing system. These systems often rely on using the map lists for the corresponding values to find the data living in different couples. In this paper, we deal with the graphics and algorithms related to the use of the maps. We begin to discuss why the maps are appropriate for the data maps in the inter-data sharing system. We discuss these tables of alternative graphics, we introduce a language that allows users to specify the maps under different graphics.
We offer a short but complete formal definition of the XPath 1 grammar and summarize the e-science algorithm processing queries in this language. our introduction is for readers who are looking for a short but comprehensive official account XPath, as well as software developers need the material to quickly implement the XPath engine.
In this article, we proposed a method that allows mobile customers to determine the effectiveness of their previous queries based on their current location. To this, the server, in addition to the queries results, also returns the effectiveness area around the client location, where the results remain the same. We focus on the two most common types of space queries, that is, the closest neighbor and window queries, determine the effectiveness area of each case, and submit the corresponding queries processing algorithms.
Creating a data-intense website is a complex task. Ad hoc fast prototipation methods easily lead to unsatisfactory results, for example, poor maintenance and scalability. To solve this problem, a series of model-based methods are proposed, trying to simplify the design and development of data-intense websites. However, these methods often lack expressive meta models, therefore, suffered some restrictions, for example, the lack of appropriate support to create user complex interfaces, for specifications and presentation styles, and for customization.
Relationship properties are usually not independent.Multi-properties can be an effective tool for accurate multi-properties query selective estimate.In this article, we introduce the STHoles, a "work load consciousness" hystery, which allows Burkina Faso to catch the data area with a reasonable equal double density.The STHoles hystery is created, without checking the data set, but just analyzing the query results.The Bucket is allocated to the most required places, as the work load is shown, leading to accurate query selective estimate.Our wide experiment shows that the STHoles hystery continuously produces good selective estimates, in the synthetic and real world of data sets and work loads and, in the case, many of the best technologies need to build a full hystery throughout
The rapid growth of the number of files, its diversity and terms change make it more and more difficult to manage the Federal Digital Library. The appropriate abstract mechanism requires the construction of a meaningful and extensible document collection, forming a cross-digital library information space for browsing and grammatical search. This paper discusses the above issues, proposes a distributed grammatical framework, achieving the logical distribution of the information space based on the subject area, and provides the facilities for the background and landscape of the available document settings.
View selection is the choice of a set of view to materialize a database chart, therefore, the cost of evaluating a set of work load queries is the smallest, so that the view is suitable for a prescribed storage limit. View selection two main applications are materializing the view in a database to accelerate queries processing and select the view to materialize in a database to respond to decision support queries. In addition, the view selection is a core problem, smart data placed in a wide regional network data integrated applications and data management data management costs are the smallest. We describe the few basic results of the view choice of the problem. We consider the view and work load issues by equal selection of the project queries, and display the complex view quality of the view based on the quality of the view based on the quality of the view.
Workflow management systems are one of the most interesting concepts of supporting modern organizations, with a focus on processes rather than structures. Workflow management systems provide different degrees of business flow automation. We classify workflow management systems according to their features and the type of process they support. Database systems in many ways promote the implementation of workflow management systems. They can provide the necessary features to maintain workflow related data, business data and process data.
Semantic Integration is an active research area in several disciplines, such as database, information integration, and ontologies. This paper provides a short study method of Semantic Integration developed by researchers in the ontology community. We focus on methods to distinguish ontology studies from other related areas. The paper aims to provide a reader who may not be very familiar with ontology studies, introduce this study’s main topics and guide different research items.
For example, if we find the average shoe sales in West 10 Walmart store is the same in the winter and the whole year, that means something interesting about the trend of the shoe sales in that place. In this article, we are interested in finding a short summary of the database, using the details in the least of the normal circumstances on a clear basis. We will like the summary: (i) as short as possible, (ii) for yourself to form a slow, retain a bubble/bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bubble bub
High-size data has always been the challenge of the classification algorithms because of the detail of the inner divisibility. Recent research results show that in high-size data, even the approach or classification of the concept may be meaningless. We discussed very general classification techniques that can be established classification in randomly adjusted low-size subspace. Subspace is specific classification itself. This definition is clearly more common and realistic existing technologies, limiting methods only from the original characteristic group forecast. General classification technology can also be considered as a attempt to re-definit the high-size application of classification by searching for hidden subspace classification techniques, these classifications are created by interspace classification. This definition is clearly more common and realistic, and now the method is limited only from the original characteristic
Welcome to this series of interviews installed in the SIGMOD record with the databases community. this issue interview with Hector Garcia-Molina took place in June 2001 (oriented) and October 2001 (e-mail).
Recently, through a limited set of rules proposed a strong framework to define the intermediate view, including multiple knowledge bases (24; 4, 16).We studied the substantialization of these view, by improving the definition of the view and effectivelyining the result of the intermediate view.Therefore, we considered two types of updates: the view updates and the base updates.For both cases, we provided a variety of effective algorithms to maintain the intermediate view.We improved the previous algorithms, such as the DR definition algorithm(12), and introduced a new fixed WP operator - against the definition of the view and effective maintenance of the result of the intermediate view updates.
However, its potential fatal effects on the overall database performance are being underestimated.We report the real database applications where the XML database performance is a key obstacle to the successful XML implementation.There are a lot of XML database applications often fail on a early and simple pathway: XML database.We analyze the XML database performance and measure the additional advantages of DTD and planned verification.There is no significant improvement in XML database technology, compared to the relative database performance.
1 Introduction to SQL-92 2 Start using SQL-92 3 Basic Table Creation and Data Manipulation 4 Basic Data Definition Language (DDL) 5 Values, Basic Functions and Expressions 6 Advanced Values Expressions: CASE, CAST and Row Values Expressions 7 Predictions 8 Multi-Tables Use: Relative Operator 9 Advanced SQL Question Expressions 10 Limitations, Assessments and Reference Integrity 11 Access to SQL from the Real World 12 Courses 13 Priorities, Users and Security 14 Transaction Management 15 Connection and Remote Database Access 16 DYNAMIC SQL 17 Diagnosis and Error Management 18 International SQL-92 Information Graphics 20 Based on SQL-92 Database, SQL Database Based on SQL Database, SQL Database Based on SQL Database, SQL Database Based on SQL
Today’s Internet Economy has priority to customers using a method to limit operating time and center results. But immediate results are difficult to when your customers want to a minute of information about themselves: account balance, detailed trading history on all products, instant problem solving, and recommends in future purchases. Their expectations are that they will get the same real interaction with your company, whether they can communicate with you by telephone, network, web, web, or e-mail, they can communicate with you by telephone, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network, network
Searching for models in big, real, space/time data continues to attract high interests (e.g., sales of products in space and time, models in mobile phone users; sensor networks collect operating data from cars, and even from people with portable computers). in this article, we describe a cross-disciplinary research effort to find knowledge in a large environmental database with biological and chemical sensors networks to revolutionize the quality and safety of drinking water decisions.
In many applications, users specify the target value of specific properties without precisely matching these values. On the contrary, the results of these queries are usually the "top k" score ranking, best matching the given character value. In this article, we study the advantages and limitations of the top k queries, by converting it into a single range of queries, the traditional relationship database management system (RDBMS) can be effectively processed. In particular, we study how to determine a range queries to evaluate the top k queries, using the available statistics of RDBMS, and the impact of the quality of these statistics on the return efficiency of the result system. We also report the first experimental assessment of the actual RDBMS queries, the Microsoft RDBMS queries system (RDBMS) can be effectively processed, in particular, we study how to determine
In many database applications, one of the common queries is to find a query item close to match. For example, considering the image database, you can want to obtain all the images similar to a query image. The distance-based index structure is used for the application, the distance between the objects in the data field is expensive (such as high-size data) and the distance function is counted. In this article, we consider using the distance-based index structure for the similarity of the large-size space query.
In this paper, we take into account the following issues: taking into account the characteristics of a chart, the domain of which is real numbers, and a query, specified a range in each dimension, finding a good close record of the number in the chart to meet the query.
The Internet-based e-commerce is expected to grow at a clear pace.As companies quickly deploy business to business e-commerce solutions, system designers may face new challenges when integrating and managing data.In this introduction, I recommend discussing some new business models and data management effects on the background of these ever-developing e-commerce scenarios.
VisDB system developed by the University of Munich is a complex tool for visualizing and analyzing big databases.The key idea of VisDB system is to support the exploration of big databases by using the ability of phenomena of the human visual system, able to efficiently analyze the visualization of medium to large data.VisDB system aims to provide the majority of the database visualization, making the data properties and structures in the data visible.
Many applications require space data management. Integrated large space database is an important problem, trying to find an intense population area in functional space for data mining, knowledge discovery, or effective information collection. A good integrated method should be effective and detects the integrator’s voluntary shape. It must not be sensitive to the output (luck) and input data in order. We recommend WaveCluster, a new integrated method, based on wavelet conversion, that meets all of the above requirements. Using the multi-resolution characteristics of wavelet conversion, we can effectively identify the voluntary shape integrator in different accuracy. We also show that WaveCluster is very effective time defined. The complex experimental result is very large to the data integrator we recommend WaveCluster, a new method to integrate wavelet.
The Mobile Image Expert Group (MPEG) is developing a new standard called "Multimedia Content Description Interface", also known as MPEG-7. the goal of MPEG-7 is to be able to quickly and effectively search and filter multimedia content.
With the range of database from storing pure business data to including XML files, product catalogues, e-mails and catalogues data, it has become increasingly important search database based on wild card matching: for instance, it is more common (and useful) than accurate matching for these data.
The main purpose of the network technology and database system is to provide a data-intense site, that is, the main purpose of the network application is to submit a lot of content to a variety of possible users (e.g., product directory). these sites must be a general user opportunity to browse a large data collection in a way that meets certain applications specific goals (e.g., in e-commerce, to each user to show a user's specific content so that a user's basic content, so that a user's basic content, so that a user's basic content, so that a user's basic content, so that a user's basic content, so that a user's basic content, so that a user's basic content.
Publisher Overview Support provided by enTrans can be used by users to long-term operating activities and transaction properties - activity access to one or more Lightweight Directory Access Protocol (LDAP) servers. Basic philosophy enTrans allows any standard LDAP servers. It also provides a obscure, so customizable integrity limit administrator. Guidelines are for data-intense applications, where reading is more frequent than writing, and they are mainly for standard white and yellow page applications. LDAP is an open industry standard for access to information based on guidelines. Because its natural way represents data in a series form, effective reading access and supporting unusual data, LDAP is more needed for applications such as network identity management and LDAP management policies.
We discuss the design of the purchase query processor for the collection of data in the sensor network. The purchase issue belongs to where, when and the frequency of the physical acquisition (collection) and delivery to the query processing operator. By focusing on the location and cost of the data acquisition, we are able to significantly reduce the electricity consumption of the traditional passive systems, recognizing the presence of the data. We discuss the simple expansion to SQL control data acquisition and show how the purchase issue affects the query optimization, dissemination and execution. We evaluate these issues in the background of a distributed query processor of the smart sensor device and show how the purchase technology provides a significant reduction in the electricity consumption on our sensor devices.
For many collective queries, properly built mixed (non-united) samples can provide a more accurate approach than a single sample. The best type of mixed, however, from queries to queries differ. In this article, we describe a closer queries processing technique to dynamically build a suitable mixed sample, each query, by combining the sample chosen from a non-single sample home, these samples are built in the pre-processing phase. We show that the dynamic choice of the suitable sample, the previously built samples can provide a more accurate response rather than the suitable sample.
Rules-based optimizers are extensible because they are composed of modified rules. To modify it is simple, the rules must be easily reasonable (i.e. understand and verify). At the same time, the rules must be expressive and effective (burning) rules-based optimizers are practical. Production style rules (such as in [15] ) are expressed with code and difficult to reason.
Clean the data structure and content errors are important to data storage and integration. The current data cleaning solutions involve many iterations of data “audit” to find errors and long conversions to correct them. The user needs to suffer long waiting and often write complex conversion scripts. We introduce the Porter wheel, an interactive data cleaning system, closely integrated conversion and discrepancy detection. The user gradually build the conversion to clean the data, by adding or leaving the conversion on a broadboard similar interface; the conversion effect once displayed on the screen visible records. These conversions are through simple graphic operations, or by displaying the required effect on the data background, the special wheel conversion automatic system can be used according to the data structure of the user.
The text applies (and adapts) to this successful principle, the database supports XML and XPath processing: the relationship system is the tree’s consciousness, i.e. the tree’s properties, such as the size of the underground tree, the route of the cross, containing or separating the underground tree is clear. We recommend a place to change to the database core, the staircase connection, which includes the necessary knowledge to improve the XPath performance.
Considering user data, people often want to find a close-range match in a big database. A good example of such a task is to find a similar image in a big image set. We focus on an important and technically dispersed case, each data element is high-size, or more generally, is calculated by a point in a big measurement space and distance calculation is expensive. In this article, we introduce a data structure to solve this problem called a GNAT { geological close-range access tree. It is based on the philosophy, the data structure should be as a geological model of data rather than a simple data disintegration, not using its internal geology. In the experiment, we found that the GNAT structure of the previous data in the structure of the number of data in the nearby area, the number of data in the nearby area.
The majority of the data reduction technology is closer to query processing (such as wave lines, history maps, core, etc.) usually does not apply to category data. there is some separation between the field of research and the reality of the database data; many recent studies focus on closer to query processing or digital properties, but the most of the database properties are categories: country, country, post_ title, color, gender, department, etc.
Many proposals have been issued as the last version of this column appeared six months ago, we first briefly touched some of the latest policy/legislative front on the NSF, ARPA and HPCC, and then we received the latest proposals from the ARPA, NSF, the Air Force, NASA and the Army.
In the storage database (ODB) system, the database owners publish their data through multiple remote servers, which aims to allow customers to access and request data on the network margins more effectively to do analytical assessments, because the server may be unreliable or may be damaged, query verification becomes an essential component of the ODB system, the existing solution of this problem is mainly focused on the static scene and based on the ideal characteristics of some encrypted primary material. In this work, first, we defined the various basic and practical cost measurements related to the ODB system. Then, we analyzed some different methods in finding a solution that is best suitable for all the important measurement measurement measurement measurement measurement measurement measurement measurement measurement measurement measurement
TimesTen is a memory application data manager that provides low response time and high input rate. Applications can create tables and manage them only in TimesTen, and they can optionally store in the disc relationship database that is frequently used in TimesTen. Only tables and tables administered by TimesTen can be shared in the same database.
The biggest challenge in database replication is in transactions between performance and consistency. Ten years ago, performance was only achieved by false replication at the price of transaction guarantee. The importance of strong replication method has recently increased further as it plays a role in achieving the data base layer’s flexibility and the role of the database layer, developing a replication solution. In database replication, the biggest challenge is in transactions between performance and consistency. In the last decade, performance was only achieved by false replication at the price of transaction guarantee. The importance of strong replication method has brought high costs in terms of reducing performance and limited replicability. Postgres-R combines the results of distributed systems and databases, developed a replication solution that provides a strong replication and consistency of the use of
After the first International Engineering Federal Database System Seminar (EFDBS’97) held in June 1997, the second seminar aims to gather researchers and practitioners interested in various issues related to the development of the Federal Information Systems, extending its scope to the databases and non-databases information sources (the change from EFDBS to EFIS reflects this).
The MOMIS project (Mediator envirOnment for Multiple Information Sources) has been developed over the past few years, allowing data integration from structured and semi-structured data sources. SI-Designer is a design support tool implemented in the MOMIS project, used for semi-automatic integration of unusual source charts. It is a Java application in which all the participating modules are available for the CORBA objects and interactive use of the established IDL interface. This demonstration aims to introduce a new tool: SI-Web (Source Integrator on the Web), it provides the same functionality of the SI-Designer but it has gained a huge advantage of using the Internet through a web browser.
Classification is one of the most effective means to improve the performance of the object base application. Therefore, there are many recommended algorithms for calculating good object position depends on the application’s personal data. However, in an effective object base reorganization tool, the classification algorithm is only a component. In this article, we report our object base reorganization tool, covering all stages of the object reorganization: the application’s personal data is determined by a monitoring tool, the object position is from monitoring access to statistical data calculation, using various classification algorithms, and ultimately, the reorganization tool is based on the object.
Semantic Binary Object-Oriented Data Model (Sem-ODM) provides an expression data model (like an object data model) with a well-known statement query facility - SQL (like a relative database). the benefits of using Sem-ODM include (i) more friendly and smarter genetic user interface; (ii) comprehensive implementation of integrity limits; (iii) more flexible; (iv) significantly shorter applications; (v) easier query facilities.
This paper proposes multiple versions of the two-stage lock extension of the protocol, known as EMVZPL, allowing to update the transaction using the version, while ensuring the authenticity of all transactions. The use of the protocol is limited to transactions, known as post-read transactions, consisting of two consecutive parts: a written part contains reading and writing operations in some arbitrary order, and a misuse called reading part, contains reading operations or writing operations of the data item has been locked in the writing part of the transactions. through EMVZPL, reading operations in the reading part using the version and reading lock in the writing part can be published until entering the reading part.
Computing multi-related groups and collections are one of the core operations of many online analytical processing (OLAP) applications. Recently, Gray et al. proposed the "Cube" operator, the operator calculated all possible grouping groups rather than the set of specified sizes. In this operator's importance of rapid acceptance led to a variable of the Cube was proposed as SQL standard. Therefore, it converted to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to a computer of the CAPOL to
This paper proposes a new method, a network query user interface, consisting of two stages: query preview and query improvement. This new method is based on dynamic query and close combination, guiding users to quickly and dynamically eliminate unnecessary items, reduce the data volume to managed sizes, and before submitting on the network for a two stages of dynamic query system for NASA's Earth Observation System - Data Information System (EOSDIS) was introduced.
Data mining in big databases is increasingly important. supporting this trend, we consider a range of building alternatives to combine mining with databases systems. These alternatives include: through a SQL processor interface to do separate mining; mining a mining algorithm in a storage program; mining data to a file system in flight and mining; closely mining, using mainly user-defined features; and SQL applications to be processed in DBMS. We thoroughly study the expression of mining algorithm options in the form of SQL query using association alternative mining as a case. We consider four options in SQL-92 and six options in SQL extended optimized objects (SQL-OR our conversion method as a conversion method of conversion algorithm).
The number of medical image data produced annually increases rapidly, exceeding the ability of image files and communications (PACS) systems. Image compression methods can alleviate the problem by encoding digital images to the form of more spatial efficiency. Image compression is achieved by reducing the compression rate of image data. The existing methods can reduce the compression rate of personal image. However, these methods ignore the additional compression source, these methods are based on the average information stored in multiple images and similar images (PACS) systems. We use this term "set the compression rate" to describe this type of compression rate.
The requirements for the broad regional distribution of the database system are significantly different from the requirements for the local regional network system. In the broad regional network (WAN) configuration, a single site is usually to different system administrators, there are different access and charging algorithms, the site is installed specific data type expansion, and there are different restrictions on service remote requests. The typical last point is the production of trading environment, fully engaged in normal working hours and cannot bear additional burden. Finally, there may be many sites involved in a WAN distributed DBMS. In this world, a single program performs global query optimization, using cost-based optimizer will not work.
In this article, we describe the design and implementation of OPT++, an extensible database query optimization tool, using objective-oriented design to simplify implementation, expansion and expansion tasks.
In this article, we studied two alternatives to indicator changes, i.e. OAT (one to one page moves) and BULK (centric page moves). OAT and BULK are two extremes on the spectrum of data moves. OAT and BULK are different in two aspects: First, OAT uses a little extra disk space (the majority of one additional page), and BULK uses a lot of disk space. Second, BULK uses a series of I/O to optimize I/O’s numbers during the period of change, while no A/O test, but we showed two big experiments, OAT and BULK are different in two aspects: First, OAT uses a little extra disk space (the majority of one additional page), and BULK uses a large disk space.
The rapid growth of structured data on the network creates a high demand to make this content more repeatable and consumable.The company is not only collecting structured content and making it public, but also encouraging people to reuse and benefit from it.Many companies have made its content publicly available, not only through API, but also began to broadly adopt web data standards such as XML, RDF, RDFa and microformats.This trend of structured data on the network (data network) is turning the focus of web technology to new structured data recovery models.
DB2 DataPropagator is one of the two separate applications capturing and applying unsynchronous copying solutions for the relationship data. capturing programs capture changes in source data from log files to order tabs, while applying applications change from order tabs to target data. Currently, capturing programs only support capture changes that are carried out by local transactions in a database log file. As the divided database system is increasingly deployed in the OLTP environment, it is necessary to copy operating data from the divided system.
This article describes the design and implementation of the background server, a copyable, based on the Internet, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules, a customer’s trading rules.
In the past decade, a large number of object-oriented database languages have been proposed. The early days of these languages have a minor number of object-oriented characteristics, and more and more characteristics are systematically incorporated into the continued language. However, a language has a pure logical grammatics, naturally calculating all key object-oriented characteristics, still lack of literature. This article gives us another step to solve this problem. The current lack of two characteristics is based on the rules of methods in the classroom, rather than the unconnected structure and behavior heritage with the super, conflict solving and blocking. This article introduces the synthesis of the language with these characteristics. The language is in a limited sense, we have abandoned other object-oriented characteristics, and the subject-oriented characteristics are now based on
In this article, we have proposed a 3-year design and implementation of the web base building to search for dynamic web content (i.e. data can only be extracted by filling multiple tables). the lowest layer of virtual physics, by protecting users from the data related complexity from the original web source, provides navigation independence.
In the U.S. Department of Defense (DoD), seed interactivity is a growing challenge. In this paper, we describe the basis of the reconciliation infrastructure of the relevant but heterosexual properties. It describes the three types of information that can be used to judge the character background, clearly hide the seed conflict and enable it to properly adjust the value. Through an extended example, we show how an automatic integrated agent can produce the four tasks required in a simple seed reconciliation.
The analysis of the use of the web page is mainly focused on the site consisting of traditional static pages. However, the information available on the web page is a great deal from the database or other data collected and in a dynamically created page form introduced to the user. The search interface of these sites allows to define many search standards. The results of its creation support the results of the web pages combining cross-linked data from many sources. To analyze the visitor's navigation behavior on these sites, we propose the web page use mining (WUM), finding the navigation pattern under advanced statistics and structural limits. Because our goal is to find interesting navigation pattern, we do not focus on access to individual pages.
In the past few years, the number of interactive multimedia presentations prepared by different individuals and organizations has increased significantly. In this article, we introduce the algorithms of query multimedia presentations in a database. Unlike the relative algorithms, the algorithms of interactive multimedia presentations must run on the tree, its branches reflect the different possible presentations of the family. Query language supports the choice of type of operation to find the object and the demonstration path, the user is interested, adds the type of operation, combines the presentation from multiple databases to a demonstration document, and ultimately set theoretical operation to compare different databases.
We give a simple definition of a separate vaginal relationship and defining a new normal form, accurately describing the vaginal relationship. We are based on our definition, the arbitrary combination of function and multi-value dependence, and show our definition of the standard normalization of the vaginal normal form. In addition, we give a condition that can prevent unnecessary structural abnormality in the vaginal relationship, i.e., the vaginal relationship with a maximum of one vaginal. Like other normal forms, our vaginal normal form can be designed as a database guide.
Nearly historical query processing technology emerged a very cost-efficient method to determine two speeds to deal with today's decision support system (DSS) of huge data quantities and strict response time requirements. However, the majority of the work in the field, so far, has been limited to its query processing range, usually focused on specific combination of query forms. In addition, based on entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity query entity
This paper introduces a pair of query editors to SuperSQL:SSedit and SSvisual.SSedit is a structured editor specifically for SuperSQL, mainly used to create a query statement from scratch.SSvisual is a WYSIWYG editor, mainly used to clean up the layout and visual effects of HTML.
Bipas technology, previously limited to options (KMPS94) and extended to access operations. Similar to options cases, access operators can produce two output streams - access results and its supplements, its subsequent operations sequence is individually optimized. by extending Bipas technology access, it is necessary to solve several problems. (1) an algorithm for a comprehensive search space for Bipas program needs to be developed. (2) search space for Bipas program is quite large. therefore, some exploration strategy still leads to sufficiently effective program must be developed. (3) As the access supplement can be very large, in these cases, the supplement can be limited to the supplement supplement must be found attacks. by extending Bipas technology access, it is necessary to solve several problems by providing support, by providing support, by providing support.
The paper discusses the effectiveness of two data mining technologies, analyzing and obtaining unknown behavior patterns from the data collected from the health insurance industry. In particular, the database of events (requirements) for pathological services and the general practitioners. The association rules apply to the event databases; the nervous division applies to the two databases. The results obtained from this study show that the potential value of data mining in the health insurance information system, by detecting the patological service orders patterns and dividing the general practitioners into groups that reflect the nature and style of their practice.
Author-χ is a Java-based access control system for XML documents. Author-χ implements a visible access control model specifically adapted to the characteristics of XML documents. in particular, our system allows (i) to set oriented and unoriented document protection by supporting document type and document level licenses; (ii) to support multilateral protection of objects and positive / negative licenses, to differentiate protection of document type content; (iii) to control the dissemination of licenses in the protection of objects by implementing multiple dissemination options.
Data mining is the discovery of unknown, potentially useful and hidden knowledge in the database. In this article, we focus on finding the merger rules. Many algorithms are proposed to find the merger rules in the binary properties database. We introduce the form of the merger rules, "If X is A, then Y is B", to deal with quantitative properties. X, Y is the merger combination, and A, B is the merger combination, respectively describing X and Y.
Automatic adjustment has been a breakthrough goal for a long time and has become a tension of modern electronic services. This paper reviewed and evaluated the progress achieved on this important topic in the last decade. A major conclusion is that automatic adjustment of the database technology should be based on the paradigm of the feedback control circle, but must also be built on the mathematical model and its appropriate engineering to the system components.
This qualified paper aims to review the most advanced state of the real-time database system, in a single processor and concentrated environment. Due to the abnormality of the problem, a large amount of information and space restrictions, we limit our introduction to the most important issues, the overall design, construction and progress of the real-time database system. These topics are considered to include trading these issues, the most focused on monetary control and conflict resolution protocols, because their serious role in the overall performance of the system. Other important issues, not included in our introduction include error tolerance and error recovery, predictability, most importantly, reducing conversion support; that is, the atomic relaxation and serious solutions of the real-time database. These topics are considered to include trading these issues, the most focused on monetary control and conflict resolution protocols, because
The goal of aviation security data analysis is simple: improving security. However, the path to achieving this goal is difficult to determine. What data mining methods are most suitable for this task? What data are available, how to analyze? How do we focus on the most interesting results? The answers to these questions are based on the latest research projects that we have completed. The inspiring news is that we found some aviation security offices to do valuable work, collect and analyze security-related data. But we also found some fields that data mining technology can provide new tools, whether to do analysis, previously not considered, or now can be more easily completed.
We believe a environment, distributed data sources are continuously updated to a centralized processor, monitoring the continuous queries of the distribution of data. significant communication top occurs in the existing fast upstream, we propose a new technology to reduce the top. user record continuous queries accurate requirements in the centralized flow processor, installing the filter in the remote data source. filter adapts to the conditions to try to reduce the flow rate, while ensuring that all continuous queries are still obtained the necessary updates to provide the appropriate accurate answers at all times. our method allows the application to trade accurate communications top in the subtle particles, by personalized adjustment of accurate restrictions of continuous queries on multiple work loads through physical testing to reduce the flow rate, while ensuring that all continuous queries are still obtained accurate updates
This article formalizes and studies two types of meaningful assumptions: point-based and interval-based assumptions. point-based assumptions include those using interval method assumptions, while interval-based assumptions include those involving different temporary types (time score). each assumption is considered as a way to obtain some meaningful data from the clear data stored in the database. the database system must use all clear and (may be unlimited) meaningful data to answer the user's query. this article introduces a new method to promote such query assessment. the user query is translated into a system query, the system query is the same number of user's query data (every time query).
Space database operations are usually carried out in two stages. In the filtering steps, the index and the smallest marginal rectangle (MBRs) of the objects are used to quickly identify a group of candidate objects, and in the refining stages, the actual geology of the objects is recovered and compared with the geology of the query or each other. Because of the complexity of the calculation geological algorithm involves, the CPU cost of the refining stages is usually the operation of the control of the cost of the complex geology, such as polymer. In this article, we propose a new method to solve this problem, using the effective realization and search capacity of modern graphic hardware. This method does not require expensive pre-processing data or changing the geological storage and application structure, it can be
Parallel disc systems provide two possible ways to use I/O parallelity, i.e. through inter-request and intra-request parallelity. In this article, we discuss the main issues of the performance adjustment of these systems, i.e. the adjustment and load balance, and show their relationship with response time and channels. We list a smart, self-dependent file system's main component, designed to optimize the adjustment by considering the application requirements, and through the judgmental file allocation and the dynamic redistribution of the data to balance the load when access pattern changes.
However, despite the great potential to make the indicators smaller and faster, the general compression method of application to the command data set has made a little progress. This paper shows that the vocabulary-based fast method can be applied to the command to save the compression almost the same freedom as the general situation. The proposed new technology has the same speed and compression rate only quite lower than the traditional command to the same vocabulary coding. The coding and coding table program describes similar commands related characteristics, such as the data restrictions, sensitivity and sensitivity to the characteristics, a real location, a real location, a real location, a real location, a real location.
As it broadly quotes my own article, I feel obliged to respond to the article "Domain, Relationships and Religious Wars", by R. Camps (SIGMOD Record No. 25, September 3, 1996). in this article, Camps clearly mentioned (including other things) my definition of "Domain" has changed for years. I agree that it has! but Camps continued to say: "... considering that [the date of the book one introduced to the database system] is the Bible (Camps in Italian), where most university graduates study around the world, I think the date can be partially responsible for the lack of implementation of the field (in today's SQLMS's DBMS).
The paper describes the concept of using charts in the automatic analysis and synthesis of charts in the automatic labelling of charts, which describes how charts adapt to the general framework of charts interpretation and provides examples of how to use charts in other fields.
Relative Online Analysis Processing (ROLAP) appeared as a dominant method for data storage and decision support applications. In order to improve query performance, the ROLAP method depends on the selection and substantialization of the appropriate subgroup of the combined view in the summary table, and then participate in accelerating the OLAP query. However, a direct advanced relative storage implementation of the substantialized ROLAP view is quite wasteful storage and incredibly inappropriate query performance and accelerating updating speed. In this article, we recommend using Cubetrees, a collection of packaged and compressed R tree, as an alternative to the storage and index organization of the ROLAP view, and provides an effective implementation of the algorithm in order to evaluate the view of the view of the view of the view of the view
This paper was not written as an academic work, nor as a tested course note, rather than as an invited speech translation, I gave a meeting to the researchers from the final model theory, database theory, and computer assisted verification, held in October in the Dagstuhl area long ago, the first book on this topic appeared.
The framework is based on a specific language, AIG, expanding a DTD, by (1) typing the elements with similar properties (hered and synthesized by similar concepts inspired by Attribute Grammars), (2) calculating these properties through the parameters of SQL queries about multiple data sources, and (3) integrating XML keys and containing restrictions.
Digital files are committed to the long-term preservation of electronic information and have the right to allow sustainable access, despite the rapid change of technology. Permanent files face an unusual data format, supporting applications and platforms in the file's lifelong use. This is different from the interactive challenges in which the mediator is predicted. To prevent the technological ageing time and cross-platform, a permanent file transfer method is based on the XML infrastructure recommendation. We extend existing file methods, based on standardized data format and simple integrated data mechanisms, by involving high-level conceptual models and knowledge representatives as a component of the file and input / transfer process.
We describe a comprehensive database design system tools and theories and show how they jointly support multiple concepts and logical design processes. Database design and evaluation work group (DDEW) systems use a rigorous, information content stored method to graph transformation, but combine it with illusion, guessing work and user interaction. The main contribution is to describe how the theory is adapted to a practical system, and how to improve the consistency and strength of the design system by using theory.
When we design an object-oriented database chart, we need to normalize the object class, as we do when we design a relative database chart. However, the normalization process of the object class cannot be the same as the object class, because the different characteristics of the object-oriented data model, such as the complex characteristics, the collection of data types, and the use of the object detector, rather than the relatively critical characteristics.
Literary search is an important component of any research and publication activity. In the epoch of the explosion of electronic databases and scientific publications, keywords play a huge role in mining related publications, as these keywords as "keywords" to dissolve the necessary scientific paper abstract/complete articles from the wide collection of related publications. Therefore, it is important to include and select the relevant keywords that can easily identify and search for the relevant references and filter the material that is unnecessary.
A wide range of data on the web can be used for different abnormal sources and stored in different formats. Due to the use of this abnormal data system's number increases, the importance of the data translation and conversion mechanisms increases significantly. In this paper, we introduce a new translation system, based on format matching, designed to simplify detailed data conversion tasks. We observed that in many cases, the data format in the source system is very similar to the target system. In this case, most of the translation work can be carried out automatically, based on format similarity. This saves a lot of effort to limit the number of programming required by the user. We define the common format and model, in which the format and format examples (from the common format examples) can represent a lot of data format and data format based on the format format
We introduce a new client cache consistency algorithm for the client cache database management system. The algorithm, known as non-synchronous cache consistency (AACC), provides good performance and low abortion optimization algorithm. We introduce the simulation results, comparing the AACC with two leading cache consistency algorithms: Adaptive Call Lock (ACBL) and Adaptive Cash Control (AOCC).
We describe a new data configuration program, called multi-dimensional classification, in DB2 universal database version 8. Many applications, for example, OLAP and data storage, process a table or table in a database using multi-dimensional access mode. Currently, most database systems only support a table of an organization using a major classification index.
Two observations, 1) many XML files stored in a warehouse or from the warehouse stored in a warehouse and 2) processing these files with the XSL style table processor is an important, frequently repeated task, justified a more careful observation of the current situation. Usually, XML files are obtained or built from the warehouse, exported, distributed, and then processed by a special XSL processor. This accumulated process clearly set the goal to integrate the XSL style table processing to the warehouse engine. We describe a way to this goal, through the translation of the XSL style table to the algorithm table.
In many emerging applications, Ad hoc queries and/or interconnections also require the processing of data arrived before or during the interconnection. For these applications, we have developed a system that processes Ad hoc and continuous queries the same as data processing and queries, allowing new queries to be applied to old data and new data to the old queries.
The growth center is China, but online alternative finance continues to grow in developed countries such as the United States and the UK, through the creation of new market parts and the transition from some existing financial institutions. In the case of online alternative finance, the type of information used to scan loans differs from the case of financing provided by existing financial institutions. Even sometimes credit history information is not used. Even when credit history information is used, the range of additional data used to scan loans is also expanding, including information related to e-commerce (EC) in the case of corporate loans, and personal information in the case of existing financial institutions is used to scan loans different from the case of existing financial institutions.
While multiple access control policies can be developed to control access to information, all existing authorized models and corresponding implementation mechanisms are based on a specific policy (usually closed policy).
Decomposition Storage Model (DSM) vertically divides all the properties of a specific relationship. DSM has excellent I/O behavior, when the number of properties touched in the query is small. It also has better storage footprint than most databases use N-ary storage models (NSM). However, DSM takes over the cost of rebuilding the original divide is high. We first reviewed some performance issues related to DSM. We recommend a simple indexing strategy and compared different rebuilding algorithms. The paper then proposed a new mirror system called a broken mirror, using NSM and DSM models.
In the past few years, large-scale parallel processors have been increasingly used to manage and query a large amount of data. significant performance improvements through distributed query performed in many nodes. query optimization for such a system is a challenging and important problem. In this article, we describe query optimizer in SQL server parallel database products (PDW QO). We use the existing QO technology in Microsoft SQL server to implement cost-based optimizer distributed query perform. By correct abstract data, we can easily reuse existing logical query simplification, space exploration and cardinity estimates. Unlike the previous method, it is just the best parallel series program, our optimizer considers a spatial implementation of a cost-based QO technology in Microsoft SQL server to implement a cost-based query.
One of the core tasks in the management, monitoring and mining data flow is to identify the output. statistical databases and databases have a long history of research on the various output, the latest focus is on the mining output in the data flow. here, we adopt the concept of the "output" from Jagadish et al. (1999) as the output. the output is based on a most basic statistical concept of the standard output (or variable).
Some integrated and integrated queries are concepts simple, but difficult to express in SQL. This difficulty leads to concepts and implementation of problems for SQL-based databases systems. Complex queries and views are difficult to understand and maintain. In addition, the generated code is sometimes unnecessary efficiency as we experiment to prove the use of business databases systems. In this article, we review a class of queries involving (potentially repeated) choices, integrated and integrated in the same set, and submit an extended SQL synthesis, allowing short representation of these queries. We submit a new relative algorithm operation, representing multiple levels of integration in the same set in a operating relationship. In this article, we review a class of queries involving (potentially repeated) queries, and submit a selection of queries
What analysts and decision makers use - if analysis to evaluate the assumption of e®ect. What - if analysis is currently supported by the distributed table and ad-hoc O L AP tool. Unfortunately, the previous lack of seam- with the data integration is less, the latter lack of °exibility and performance applied to O L AP applications. To solve these problems, we developed Sesames System, a model of a assumption as a list of assumption changes in the warehouse views and facts data. We provide a formal scenic merger and sequence, extended view updating sequence to meet the special requirements of O L AP. We focus on the algorithm operator suitable for the implementation of the distributed computer and then we introduce the optimizer and alternator as a model of assumption as a situation.
We describe an investigation on e-mail content mining to identify the author, or authorized the author, for the purpose of conducting a legal medical investigation.We focus on the ability to discriminate between the authors, both integrated e-mail topics, as well as various e-mail topics.Extended set of e-mail document functions, including structural characteristics and language patterns, is derived, and together with supporting Vector machine learning algorithms, used to mining e-mail content.Using various authors on a set of topics, the experiment of e-mail files, provides future results for integrated and multi-topics authorized classification.
Based on the mediation architecture, existing data integrated systems use a variety of mechanisms to describe the source’s query processing ability. However, these systems will not calculate the ability of the mediator according to their integrated source. In this paper, we presented a framework to catch the rich diversity of data sources and the mediator’s query processing ability. We introduce the algorithms to calculate the mediator’s support query combination, based on the source’s capacity limitations.
The derivative data is stored in a database system connected and summarized with the real world facts. the database changes, the derivative data need to be re-calculated. This is often caused by the writing of active rules, which are caused by the changes in the database. In a system, a rapidly changing database, a database with the standard rules system can consume most of the resources operating rules re-calculate data. This paper introduces the rules system as part of the implementation of the standard real-time information processor (STRIP). the STRIP rules system is an extended SQL3 class rules, allowing the rules to be combined together to reduce the system's overall re-calculate burden. In this paper, we describe the system's sequence and sequence rules, for example, the existing rules, to keep the rules re-calculate
The purpose of the traditional index is to optimize the nodes access in the process of query processing, but this will not necessarily reduce the total cost due to the possible large number of random access. In this article, we propose a general framework of the index adaptation to increase the total query cost. Performance profits are achieved by allowing the index nodes to contain a number of variables on the disk page.
The number of scientific and technological information is growing. therefore, the scientific community has been published in the number of new books, magazine articles and conference programs, in general, the recommended system for use online, recommends users to find interesting projects, therefore, thanks to the user and business information technology progress, significantly reduced the barriers of electronic publication and information disclosure, through the network almost anywhere in the world. therefore, the scientific community faces problems, find relevant or interesting information.
With the popularity of the world’s “Information Road”, the interest in effective document indexing technology has been recovered.In this paper, the use of new dual-structure index solves the problem of increased updates of the reversal lists.The index dynamically separates the long and short reversal lists and optimizes the recovery, update and storage of each type of lists.To study the behavior of the index, the space of engineering transactions from optimizing the update time to optimizing the query performance is described.We quantitatively explore this space by using the simulation of real data and hardware and information recovery systems.
Significant performance advantages can be by implementing a database system in a password consistent multi-shared memory multi-processor. However, when the problem occurs, avoid errors occur. a single nod (a nod indicates a processor/memory pair) collapse may need to restart the whole shared memory system. Fortunately, shared memory multi-processor to isolate a nod protocol failure is currently being developed. even if these, due to the side effects of the object of the password consistency protocol is sufficient, a transaction performing strictly in a single nod may become dependent on the effectiveness of memory many nodes, thereby causing unnecessary transaction failure.
HyperStorM Database Project (‘Hypermedia Document Storage and Modeling’) will transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform transform
budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget budget
Data in the warehouse usually has multi-dimensional interests, such as location, time, and product. It is clear that these dimensions are based on their levels, such as \store-city-state-region" for location. The standard way to model such data is with the star/snowbirth program. However, the current method will not give a grade of state size. Therefore, a meaningful category of interesting queries involves the size levels and their interaction with the facts is quite immaterial writing, difficult to read and to optimize. We propose the SQL(H) model and a natural extension to the SQL query language, it provides a state level implementation and will download it.
Last year, Pat and Betty O’Neil and Gerhard Weikum pro proposed self-adjustment improvements to the latest use (LRU) bubble management algorithm (l5) their improvements are known as LRU/k and support the preference of the bubble page in the latest conversion (standard LRU algorithm according to this term is known as LRU/l ) If the conversion of Pl is the latest access than the P2 update, then the Pl will be replaced by P2 after.
The lack of research efforts in the majority is a means of guiding the classification process and understanding the results, which is important for high-scale data. Visual technology can help solve this problem as it provides effective support for different classification paradigms and allows visual examination results. HD-Eye (high-scale eye) system shows that the close integration of advanced classification algorithms and the most advanced visual technologies is powerful to better understand and effectively guide the classification process, so it can help significantly improve the classification results.
In this article, we recommend a new operator to do advanced exploration of a multi-dimensional database. the recommended operator can automatically from a specific problem case to detailed data, and return to a broadest background of the problem occurs. this feature will be useful for analysts who are observing a problem case, saying a drop-selling product in a store, want to find the accurate range of the problem. using the existing tools, he will have to manually search the problem around the template trying to draw a model. this process is boring and inaccurate. our recommended operator can automatize these manual steps and return to a single step of the broadest and easy-to-interpretation summary, all possible maximum general, along the different rotating rotating rotating case we can now launch a cost-based framework of this type of behavior.
New models to assess dependency in data mining issues are introduced and discussed.The known concept of the Alliance Rules is replaced by a new-defined dependency value, which is a single real number that is related to a particular project group.The knowledge of dependency is sufficient to describe all dependency, having a particular data mining problem.The dependency value of the project group is the difference between the probability of the project group and the corresponding "maximum independent estimate".This can be defined as the common probability of the subgroup function by maximizing the appropriate input function.
The need to automatically extract and classify the content of multimedia data files, such as images, video and text files resulted in a major work on the collection of similarity data. Until now, the majority of the work in the field has been focused on the creation of the index structure based on similarity collection. There is a few work to develop the formalism of query multimedia databases, supporting based on similarity calculations and optimizing these queries, although it is well known that the characteristics used in media data extract and identifying algorithms are very expensive. We introduce a similarity algorithm, combining the results of similarity operators and multiple similarity applications. Search algorithm can be used to specify complex queries, combining different similarity interpretations and multi-value algorithms to calculate these algorithms on the basis
Knowledge management is becoming a key requirement for many engineering organizations. In many cases, it is difficult to catch this knowledge directly because it is hidden in the way of work, followed by a network of highly qualified professionals. In addition, the majority of this knowledge is strong background dependency, so to follow the rules must be increased by appropriate circumstances analysis. The hardware and software tools used to support these processes are strongly unusual, involving a large number of use efforts and very different types of data. In this article, we propose SURFHVV GDWD ZDUH KRXVV as a means to solve these problems. A process database, according to our method, focuses on a knowledge-based database record and drives an unusual process, supported by the engineers these processes are strongly unusual, involving a large number of and very different
Invited conversation I.- Some advances in data mining technology.- Web exploration.- Question of worldwide labels.- WWW exploration queries.- E-mail filtration strategies combining content-based and sociology filters with user types.- Interactive queries expansion in a meta search engine.- Database technology.- on optimization of queries containing regular path expressions.- Database Array Algebra for Spatio-Temporal Data and Beyond.- Dynamic relationships and its advertising and competitors in the object-oriented databases.- Tracking mobile objects using databases technology in DOMINO.- OLOG: Deductive databases language.
Again, scientists were asked to play a greater role in the political process, this time they did!We the continued debate on the national research and development policy and we also covered the funding opportunities for DoD and NSF.
Space time databases store a lot of data. They are usually used by space time OLAP systems to extract relevant information. To extract interesting information, the current user starts space time OLAP (ST-OLAP) query to navigate within the geological database (Geo-cube). It is very common to choose which part of the geological library to navigate further, thus designing the upcoming ST-OLAP query, is a difficult task. Therefore, to help the current user in the geological library to start their current query, we need a ST-OLAP query recommendation to use the geological library.
The temporary function dependency (TFD) is defined as a temporary database that includes the identity of the object. It is believed that the identity of the object can overcome certain gender divisions of certain temporary relationship data models. The practical application of the TFD is discussed on the basis of the object. The reason for the TFD is the center of this article. It is clear that the distinction between the details and the details of the graph is clearly visible. For the details of the graph, a complete detail of the graph is the end-to-end graph and provides an algorithm to determine the end-to-end graph. The same detail of the graph is proven to be complete unlimited involvement in the unlimited graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph graph
The National and International Standards Committee responsible for the database language SQL proposed a candidate extension of the SQL Permanent Storage Module (SQL/PSM). This extension aims to provide a computing full language of the declaration and reference to the SQL Storage Module and Habits. Usually, such habit is stored in a database server and performed from an application client in a client/server environment. The recommended SQL/PSM is synthesized and sorted into a variable and a sequence of data methods, function and program processing (Road) of the main conditions, condition processing, and control of the declaration of conversion and conversion. A SQL route is structured, each block is made by a local variable and condition declaration, a list of SQL routes and local conversion is made by a client/server environment.
Materialized views have been found to be very effective in accelerating queries and are increasingly supported by business databases and databases systems; however, as the amount of data entering the warehouse and the number of materialized views is rapidly increasing, the time window that can be used to maintain materialized views is shortened; these trends require effective maintenance of materialized views technology.
In many applications, new data is produced every day and often requires a data index to effectively answer questions, for example, in the warehouse you may need an index of last week’s sales record to effectively mined data, or in the web service you can provide an index of last month’s Netnews article. In this article, we present a variety of wave index where new data can be effectively added, old data can quickly disappear to keep the required window. We compare these programs based on multiple system performance measures, such as storage, query response time and maintenance work, as well as their simplicity and coding convenience.
The phrase match is a common IR technology to search for text and identify the related files in the document collection. The phrase match poses new challenges in XML as the text can interact with voluntary labels, overthrow the search technology, requiring strict dialogue or close approach to keywords. We introduce a technique to match the phrase in XML, allowing dynamic specifications, whether the phrase match and the labels are ignored. We develop an effective algorithm, our technology, using the phrase word and the XML labels reverse index. We describe the experimental results, comparing our algorithm with an indexed circular algorithm, which shows the efficiency of our algorithm.
Similarity queries are widely used for many modern applications, and disc queries are strong storage media increasingly important. In this system, similarity queries processing is the basic transaction that increased parallelity leads to higher resource consumption and lower channels, while lower parallelity leads to higher response time.
Gray discussed his childhood and education at the University of California, and he explained the influence of Sputnik, the Norbert Vienna's views on the networks and society, the social influence of computers, and Neur's work on artificial intelligence.
In this article, we carefully review selective estimates in the background of geographical information systems, managing space data, such as point, line, multi-line and polymer. In particular, we focus on the point and range queries of two-dimensional straight-country data. We presented several techniques based on the use of space indicators, historical maps, binary space divisions (BSPs) and new concepts of space divisions. Our technology carefully divides input straight-country to subgroups and accurately approaches each division. We present a detailed experimental study, comparing the proposed technology and the most familiar samples and parameters technology. We use them as substantial substantial substantial substantial substantial substantial substantial substantial substantial substantial substantial substantial substantial subst
The database server causes users to doubt that no one can fully rely on the traditional security mechanisms, against increasingly frequent and malicious attacks, no one can fully rely on an invisible DBA data management, which provides a deep analysis of existing security solutions and concludes that the traditional server is based on data confidentiality methods. through this statement, we propose a solution called the CDA (database server), because no one can fully rely on the traditional security mechanisms, against increasingly frequent and malicious attacks, no one can fully rely on an invisible DBA data management.
This second long-term planning topic on PLS-SEMin's strategic management research and practice aims to make further progress towards this goal. The magazine received 41 topical articles on PLS-SEM, 12 of which successfully completed the deep review process. Based on the number of high-quality manuscripts, the decision to separate the topic. In the first long-term planning topic, on PLS-SEM in strategic management (Hair et al., 2012a; Robins, 2012), the focus is on methodological development and its applications (Becker et al., 2012; Furrer et al., 2012; Gudergan et al., 2012; Hair et al., 2012a, b,c; Money et al., 2012; Rigdon, 2012).
We have proposed a new category of algorithms that can be used to accelerate the execution of multi-way joining queries or involve one or more joining and one composite queries. These new evaluation techniques allow to perform multiple hash-based operations in one step without allocating intermediate results. These techniques work especially well to join the level structure, for example, to evaluate the functional connection chain along the key / foreign key relationship.
Here, personalization is defined as a process of changing a system to improve its personal relevance, which may be a job or social motivation, the motivation is defined by applying to mobile phones and e-commerce websites.
The goal of the Garlic project is to build a multimedia information system that can integrate data, living in different databases systems as well as various non-databases data servers. This integration must be able to simultaneously maintain the data server independence, without creating a copy of the data. “multimedia” should be widely explained meaning not only images, videos and audio, but also text and application specific data types (e.g., CAD maps, medical items,&hellip;).
We describe a plan to divide and distribute a centralized archive. ’ This problem is by a tendency to decline and reorganize, reflecting the real, frequently distributed responsibility within the company. A major real requirement is that the existing application code must remain unchanged. We introduce the SQL extension to specify ownership and material replicability of information.
The ODMG proposal helps to focus the work of the object-oriented database (OODBs) on a common object model and query language. However, the current proposal has several disadvantages arising from the adaptation and lack of formality of the concept of object-oriented programming. In this article, we present the formalization of the ODMG model and the OQL query language used in the CROQUE project as the basis for query optimization.
This paper proposed a simple model of time-driven start and warning system. such systems can be used with the relative and object-related database system. time-driven start system has several advantages, compared to the traditional start system, test the start conditions and run the start action to respond to updates events. they are relatively easy to implement because they can use a intermediate software program, it just run SQL statement on DBMS. In addition, they can check certain types of conditions, such as "a value has not changed" or "a value has not changed more than 10% in six months. these conditions may be interested in a specific application, but cannot correctly check a event-driven start system. In addition, users can be fully happy to be notified, even a hour, even a hour, or less, because they do not depend on certain circumstances.
We describe this task as an opposite problem, determining a precise cost function, which must be optimized under the limitation. We show that our formula includes the unity and independence assumptions as a special situation, if we maximize the smoothness and unity, it can better rebuilding results. In our experiment on real and synthetic data sets, the recommended method almost consistently exceeds its competitors, increases the stock price data by 20% of the rootmean-square errors, and for more smooth data sets reaches 90%.
The model is obtained by repeated classification of data space and matching a simple predictive model in each classification. Therefore, the classification can be graphically described as a decision-making tree. The classification tree is used to depend on variables, taking the final number of non-classified values, with the predictive error in terms of the cost of error classification. The return tree is used to depend on variables, taking continuous or ordered differential values, and the predictive error is usually measured by the observation and predictive values. This article provides an introduction, by reviewing some widespread available algorithms and comparative capabilities, their intensity and intensity, in John Will Will Will Will Will Will Will Will Will Will Will Will Will Will Will Will Will Will
It’s a growing science, I think it’s part of its success. it’s already able to grow and adapt because of new input to access data, or add data in different ways.
The implementation of the multi-level security (MLS) program will improve the flexibility and effectiveness of the authorized policy in the shared enterprise databases, and will be replaced by a wide range of authorized implementation practices on the basis of a complex visual definition for each user. However, due to the progress in this field, the idea is crystallized, the synchronous weakness of the MLS databases is also emerging.
Abstract.Data placement in shared-nothing database systems has been studied extensively in the past and various placement algorithms have been proposed. However, there is no consensus on the most efficient data placement algorithm and placement is still performed manually by a database administrator with periodic reorganization to correct errors. This paper presents the first comprehensive simulation study of data placement issues in a shared-nothing system. The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies.
From the beginning to the end of the IRO DB ESPRIT project, the Sys theme is based on the ODMG standard as a circular model and language It is provided by three-layer ers local to objects with an unusual DBMS re-mote data access and interactive layer support design and query integrated view This paper describes the architecture and main design choices of the IRO DB and re-vises the experience acquired in the implementation and application It is analyzed with the necessary review and external tensions to interact with the technology developed and processed in the ESPRIT MIRO Web project.
Publisher Review This chapter provides a comprehensive and consistent overview of the key research results in the field of data flow query processing, for SQL similar to XML query language. Stream data is also naturally produced by web services, where easy-to-connect systems by exchanging large amounts of business data on XML labels, forming constant XML data flow. One core aspect of web services is to be able to effectively run these XML data flow to perform query, constantly match, extract, and convert XML data flow parts to drive heritage background business applications. Stream data manipulation presents many technical challenges these systems have just started to process in the database, systems, algorithms, networks and other computer science communities.
Unlike the traditional methods of the object-oriented program design, the object-oriented interaction is focused on the object-oriented database design should be based on the object-oriented representativity. We are more focused on the object's structure, relationships and restrictions related application series rather than the object's operation. Enhanced entity relationship (EER) model is a convenient tool to represent these series. In this paper, I discussed the concepts and methods of using the EER model to design the object-oriented database chart. The EER model promotes the object-oriented chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart chart
Companies across the globe discover, understand and manage rapid growth, enterprise global data is critical in time to make decisions and respond to changing business conditions. management and use of business information competitiveness, many companies build a decision support system around a database of topic-oriented, integrated, historical information. to understand why the database must replace the old heritage application of effective information processing, it is necessary to understand the root of difficulty in first place to obtain information. the first difficulty in obtaining information on the basis of the old application is that these old applications are formed business requirements, these applications are related twenty-five years ago.
The functionality of the extensible database server can be increased through the user-defined functionality (UDF). However, the security and stability of the server is concerned every time the new code is embedded. Recently, there is an interest in using Java as a database extensibility. This raises several questions: Does Java solve security problems? How does it affect efficiency?
Many existing databases use histogram to approach the frequency of value distribution in the relationship properties and to estimate the size of the query results and the cost of access. When choosing from different histograms, two contradictory goals must be balanced: optimized so that the estimates are minimal and practical, so that histogram can effectively be built andined.
Continuous queries are continuous queries, allowing users to obtain new results when they become available. While continuous queries systems can convert the passive network into an active environment, they need to be able to support millions of unnecessary queries because of the Internet’s size. No existing systems have reached this scale level. NiagaraCQ solves this problem, through the combination of continuous queries, based on observations, many web queries share similar structures.
The entire text information system is traditionally designed for the archive environment, they often provide little or no support for adding new documents to the existing document collection, but requires the entire collection to be re-indexed. Modern applications, such as information filters, run in a dynamic environment, need to be frequently added to the document collection. We provide this ability to use a traditional reverse file index, built on the top of a permanent object store.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
Based on the expansion of the object-oriented technology, heredity class becomes more and more complex, one of the main issues is that such class is effective search, requiring more complex algorithms to search for data, in this article we present a new method to effectively search for large heredity class, up-to-date methods using multi-dimensional data structures to indicate heredity class and effective search for data.
This paper explores the characteristics and challenges posed by medical databases and medical information systems, begins the research of medical databases/information systems, followed by the challenge list of the databases management systems generated by the needs of these systems, ends with some systems to solve these challenges, in the context of this background information, the databases community is asked to consider whether the results of the databases research will reach those who make decisions on the design and implementation of medical information systems daily.
ESPRIT project DWQ (Data Base Quality Foundation) aims to improve the quality of the DW design and operation through the system rich data storage. Logic is based on knowledge and rational technology development to control accuracy, consistency and integrity, through advanced conceptual modeling technology to integrate sources, data and multi-dimensional integration. It is by quantitative optimization of technology supplemented to visual realization, optimization of time and responsibility, without losing the conceptual method of the graphic advantage.
WebView is a web page created automatically from the base data, usually stored in DBMS. Given the database supported web server behind the multi-layer architecture, we have the opportunity to contain it on the DBMS inside, on the web server, or not, always on the aircraft (virtual). Because WebViews must be updated until now, materialized WebViews is immediately updated to the base data every update. In this article, we compared three materialized policies (materialized in DBMS, materialized on the web server and virtual) analysis, through detailed cost models, and quantitative, through a wide range of experiments on the implementation of the system. Our results show that materializing on the web server is a more extensible solution and can be easily updated to a virtual user, even within the DBMS.
In the March 1994 edition of the ACM SIGMOD record, Dr. Won Kim and SIGMOD Chairman published a "Object-Oriented Database Language Observation" (volume 23, No. 1).
The size of the query involves multiple properties of the same relationship depends on the frequency of the data distribution of these properties, i.e. the frequency of all the properties combined. to simplify this size estimate, most commercial systems make the properties value independent estimates and keep statistics (usually histograms) only on the personality. in fact, this estimate is almost always wrong, the result estimates are often very inaccurate. in this article, we propose two major alternatives to effectively approach (multi-dimensional) common data distribution. (a) using a multi-dimensional histogram, (b) using a single value distribution (SVD) from the technical line in fact, this estimate is almost always wrong, the result is often very inaccurate.
History is long, rich, full of detailed information in each stage. It includes history processes in different scientific fields, history of success and failure in approaching and compressing information, their adoption by the industry, and solutions, has given a huge variety of historical issues related. In this paper and the same spirit of history technology itself, we compress their entire history (including their "future history" as expected) in the given / fixed space budget, mainly recording the details of the periods, events, and the results of the highest (based on the individual) interests.
One of the basic aspects of information and database systems is their changes.In addition, when doing so, they develop, although this way and quality depends greatly on the mechanisms of dealing with it.Al the changes of data are well dealt with, other aspects of change, such as structure, rules, limitations, models, etc., are dealt with to a variety of complexity and integrity.
We introduce a method to effectively carry out deletion and update records when records are to be deleted or updated is scanned by a range in an index. Traditional methods involve many unnecessary lock calls and channels index from root to sheet, especially when the qualified record key expands over one page index. Customers suffer performance loss of these inefficiencies and complain of them. Our goal is to reduce the number of interactions with lock administrators, and page repair, comparative operations and, possibly, I/O. Some of our improvements come from increased synergy between query planning and data management components of the DBMS. Our patent method has been implemented in DB2 V7 to the specific requirements of the customer. Our goal is to reduce the number of lock interactions, as well as the number of page repair
Mobile computing technology is developing rapidly as the benefits of getting information through mobile devices and the need to get information in a remote location. However, many obstacles within the wireless computing discipline are not yet solved. One of the most important issues is the speed of data acquisition, directly affecting the performance of mobile database applications. To solve this problem, we have proposed an improved approach, focusing on the management of mobile transactions. This paper studies an extended on-line trading management mechanism and applies a model-based method to develop a simulation model to evaluate the performance of our methods.
In this article, we present our strategies in the BestPeer project to support more detailed data sharing, especially the relationship data sharing, in the P2P background. First, we look at the design of a comparative data management system and discuss some solutions to solve these problems. Second, we present our first prototype system design, PeerDB, and report our experience.
The current network development and the universalization of XML technology provides an enormous opportunity that can completely change the face of the network. Xyleme intends to become the leader of this revolution, by providing reference data services for the XML data of the Web. At the beginning, Xyleme is a research project, as an open, smoothly connected network of researchers. At the end of 2000, a prototype has been implemented. A startup, also known as Xyleme, is now becoming a product. The authors summarize the main research efforts of the Xyleme team. They are concerned: an extended building; effective storage of a large amount of XML data (millions of pages); XML query processing with a full text and structural index; data acquisition strategy to build and keep it up as a new data exchange.
The emergence of new database applications, such as engineering design, emphasizes the need for new functional similarities in the database system. It includes the management of multiple representatives of the database objects, long transactions and dynamic data structures. This paper introduces the methods used in CADB, a prototype database system dedicated to the CAD, management and control of the design of objects. It involves the operation of the object properties and interactive manipulation of their structure. They involve the concept of the object model, as well as the application sequence.
Many Web applications are based on the process of dynamic interactive exchange of information between Web components. This situation occurs, for example, in the mashup system or when monitoring the distribution of the independent system. Our work is on this challenging background, which has recently produced a lot of attention; see Web 2.0. We introduce the axlog formal model capture such interaction and show how this model can get effective support. The central component is the axlog widget defined as a tree-type query or more, through an active file (in active XML style) which contains some input flow updates. a widget generates a stream of updates, each query requires updates, retains the corresponding view we use a known technology: data optimization, such as differentiation, or magic query.
Information becomes an increasingly valuable asset of today's organization. therefore, it is necessary to create an integrated view of all available data sources appear. several technical issues must be in the design and implementation of a system to integrate different data sources. the main obstacles are the computing autonomy, data abnormalities and different query capabilities storage. the paper introduces the data integrated system AMOS II, based on the exchange method. the main focus of the work is the data model conversion and query processing. the following extension to the AMOS II system in this paper describes: • a framework to convert different data models to the object model AMOS II. • the roles and tasks are described in the Windows query system. the specific query system AMOS II, based on their query methods. the main focus is the data model conversion and
The database research community is correctly proud of the success of the basic research, its significant technology transfer records. now the field needs to thoroughly expand its research focus, attack capture, storage, and present a wide range of online data issues. the database research community should encompass a broader research agenda - expanding the definition of database management to include all the content of the web pages and other online databases, and re-think our basic assumptions to the technology transformation light. in order to accelerate this transformation, we recommend changing the way the research results are evaluated and presented. in particular, we support encouraging more speculation and long-term work, transferring meetings to a file format and publishing all the research literature on the web.
The big data set classification is an important data mining problem. Many classification algorithms have been submitted in literature, but research shows that so far, no algorithms are uniformly exceeding all other algorithms in terms of quality. In this paper, we introduce a unified framework called rainforest classification tree building, which classification algorithms are extensive aspects, from building the main features of the tree, determining the quality of the tree. The general algorithm is easy from literature (including C4.5, CART, CHAID, FACT, ID3 and extension, SLIQ, SPRINT and QUEST). In addition to its overallity, it is extended version of the classification algorithm, rather than a wide range of classification algorithm classification algorithm classification algorith
One challenging aspect of the scientific data storage is how to effectively explore the descriptive data directory. We met such a problem when developing the HEDC, HESSI experimental data center, built for the recently launched HESSI satellite. In HEDC, scientific users will soon face millions of couples of the directory. In this paper, we present a new technology that allows users to effectively explore such a big data space in an interactive way. Our method is to store copies of the relevant fields in the dividing and wave coding view, these customers use approximately data and adaptive coding technology, allowing users to quickly visual search space on paper, we describe how to reduce this method from the need of millions of duplicates of time.
We provide a new client-side data cache system to allow the relationship with a central server and multiple client database. The data is loaded to each client cache based on the queries carried out on the central database of the server. These queries are used to form predictions, describing cache content. The client's follow-up queries can be met in its local cache, if we can determine the query results are fully included in cache. This problem is called cache integrity. A separate problem, cache currency, processing with the client cache effect updates are carried out on the central database. We review various performance transactions and optimization problems involving the processing of cache currency problems, and using the predictions and promotional solutions, optimizing the service dynamic queries this problem is called a single cache integrity issue.
Currently there is a significant interest in the development of a multimedia digital library. However, it is clear that the existing architecture of the management system does not support the specific requirements of the continuous media type. This is an important field of service support quality. In this email, we discussed the quality of the service within the digital library and proposed a reference architecture that can support certain aspects of quality.
In this article, we discuss the use of a "game first" method to teach introduction to programming. We believe that the question about a OOO method or a program method should first be used is secondary course tasks and examples content. If the examples are not compulsory, students' interests are often behind, therefore, do OOO on the program argument motors. We believe game programming motivates most new programmers. Submitting tasks means that students are more likely to learn because they are interested, visual components allow students to see errors in their code, as in the result graphics appear. We describe our experience after re-design and provide a new introduction to the computer science series using 2D game development as a single topic. We teach the basic program concept through the two dimensions of the game developed in FlashScript, the first quarter to the student's conversion means that they
We define a warehouse manager as a database application that provides check/check, version and configuration management, notification, background management and workflow control. Because the main value of warehouse is in using its tools, we discuss the technical issues of integrated tools and warehouse. We also discuss how to implement warehouse manager, placing it in a DBMS, specifically focusing on the problem of programming interface, performance, distribution; and interactiveness.
Complex queries that include external plugins are mainly carried out by commercial DBMS products in a "like written" way. Only a few operations rearrangement are considered and the benefits of considering the comprehensive rearrangement scheme are not used. This is mainly because there are no easy-to-use results rearrangement such operations with copying and / or external plugins predictions are "simple" other. Most previous methods have been ignored by copying and complex predictions; very few people consider these aspects of the methods proposed, leading to the possible exposure number, while copying the intermediate plugins. Because the traditional query graphic model is a model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model
When different knowledge bases are used to store the knowledge provided by multiple sources, we face the integrated problem of various knowledge bases: in this case, we also face the unconsistent prospects. In this paper, we proposed a unified theoretical framework, based on record logic, to amalgamating multiple knowledge bases, when these knowledge bases (may be) contain unconsistency, uncertainty, and non-single rejection methods. We show that record logic can be used, with some modifications, to mediate different knowledge bases.
In this article, we introduce a mechanism of approximately translation of Brian queries restrictions in various sources of information. Achieving the best translation is challenging, because the source supports different limitations of submitting queries, and often these limitations cannot be accurately translated. For example, a query [point>8] may be "perfect" translated to [point>0.8] on one site, but can only be closer to them as [point=A] on another site. Unlike other work, our overall framework adopts a customizable "approximately" translation, which combines accuracy and response.
Many records in the database share the same value to multiple properties. If a person is able to identify and combine these records, share the same value to some - even not all - properties, not only a person has the opportunity to represent the data more orderly, but a person can also get useful data insights from an analysis and mining perspective. In this paper, we introduce the concept of the file. A file F(k,t) is a record subgroup, with k the same properties. A file's properties A are the same value to some - even not all - properties, if a file's range of width A value (digital properties) or digital properties (category properties) of the number (digital properties) of the number (digital properties) of the number (digital properties) of the number (digital properties) of the number
Smart cards are the most secure mobile computing devices today. They have been successfully used to involve funding applications, as well as ownership and personal data (such as banking, healthcare, insurance, etc.) As smart cards become stronger (with 32-bit CPUs and more than 1MB of stable memory in the next version) and become multiple applications, the demand for database management arises. However, smart cards have severe hardware restrictions (very slow writing, very small RAM, limited stable memory, no autonomy, etc.), making traditional database technologies irrelevant.
Multi-level transactions are a component of open transactions, in which sub-trades comply with the operations of different layers of the system architecture. They allow the use of multi-level transactions parameters implemented in the database core system (DASDBS) to improve competitiveness. Therefore, abandoning transactions requires compensation for completed sub-trades. In addition, multi-level recovery methods must be taken into account that advanced transactions are not necessarily atoms, if multiple pages are updated in the single sub-trades. This article introduces the algorithms of multi-level transactions management, these algorithms are implemented in the database core system (DASDBS). In particular cases, we show multi-level recovery can be implemented in an effective way.
The current client server object database management system uses the page server or the object server architecture.Both architectures have their respective advantages, but they have key disadvantages of the important system and work load configuration.We presented a new mixed server architecture that combines the best features of the page server and the object server architecture while avoiding its problems.The new architecture includes new or adapted data transfer, recovery and storage consistency algorithms; In this article, we only focus on data transfer and recovery problems.Data transfer mechanism allows mixed server dynamically as a page and object server compared to the performance of the object and object server, the performance of the mixed server is stronger than others.
To meet the needs of many real-world control applications, it is necessary to integrate the concepts of a temporary, real-time and active database:
We introduce a database of increased view maintenance algorithms from multiple distributed independent data sources. We start a detailed framework analysis view maintenance algorithms of multiple data sources with continuous updates. The previous view maintenance methods in the current update presence usually require two types of messages: one calculating view changes due to the initial update, the other to compensate the view changes due to interference with the current update. The algorithms developed in this article, on the contrary, by using the information already available in the database to compensate. The first algorithm, called SWEEP, ensures the full consistency of view maintenance algorithms in the database of existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing existing exist
In this context, we are studying a project, the overall goal is to develop a practical computing environment, with a human-centered frequency exploration mining, thereby having a decisive impact on the subsequent calculation, a key component is to support the dynamic mining of limited frequency objects, restrictions allowing the user to put some focus on the mining process; dynamic means that in the middle of the calculation, the user can rely on (i) change (such as proper processing or relaxing) restrictions and/or (ii) change the minimum support restrictions, thereby having a decisive impact on the subsequent calculation.
We show that adapters on the Internet can learn to use (limited) fixed strategies of the tenderers. These learning machines can be produced by adapting to a special terminal machine with evolutionary algorithms (EAs). Our method is powerful if the adapter is involved in frequently occurring microtransactions, where there is enough opportunity to allow the agent to learn online from past negotiations.
This feature is provided by the so-called (source) questionnaire (4,8) who will convert questionnaire to one or more command/ questionnaire can be understood as a basic source and convert the original result to a format of understanding application. as part of the TSIMMIS project (1,6) we have developed a hard-coded questionnaire for various sources (e.g., Sybase DBMS, WWW page, etc.) including heredity system (Folio). however, who built a questionnaire before it can prove that many efforts are being developed to write and such questionnaire in the case, it is important or can get a new source, this questionnaire is a quick questionnaire, we have developed a hard-coded questionnaire for various sources (e.g., Sybase DBMS, WWW page, etc.)
Multi-Dimensional Difference Data (MDD) is a voluntary size, size and basic type of sequence that appears in a variety of business, technology and scientific applications.RasDaMan is an effort to provide comprehensive domain-independent MDD database support.Based on the formal algorithm sequence model, RasDaMan provides a statement of the sequence operator embedded in the standard SQL; Key DBMS component is the MDD query optimizer and accurate storage manager to effectively access a huge sequence of subgroups.We introduce the RasDaMan to the MDD management methods, based on the field of medical and geographical applications of the project.
Capacity mature pattern (4) is an organization that defines the order of the existing software development process capabilities and sets priorities for improvement.
XML uses the wood structure data model, of course, the XML query time specifies the selection model for predicting the wood structure related multi-elements. in the XML database, all the phenomena of finding such double patterns are the core operation of the XML query processing. the previous work usually divides the double patterns into the binary structure (parent twins and ancestor twins) relationships, while the double matching is by: (i) using the structural combination algorithm to match the binary relationship with the XML database related multi-elements, and (ii) make these basic matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching matching match
Materialized visuals (or automatic summary tables - ASTs) are usually used to improve the performance of the integrated query through size commands. Contrary to the conventional tables, ASTs are synchronized by the database system. In this article, we introduce the technology ofining Cub ASTs. Our implementation is based on IBM DB2 UDB.
Currently, the Internet provides a large and wide range of information sources (e.g., text database, which includes technical reports on the site, directory lists) and system access to these sources (e.g., World Wide Web, Gopher, WAIS). the challenge is to provide easy, efficient, solid and secure access to this information and other types (e.g., relative and objective-oriented database). this panel aims to explore whether there are any new technical problems related to the database fields that need to be solved to such a global information system.
However, due to the rapid development of research and specific prototype implementation, in this field, the initial result of the program seems to produce a new combination of systems between. Although they can perform some advanced information integration tasks, they cannot easily communicate with each other. To understand and solve this problem, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule, a simple rule,
The objective of this demonstration is to introduce (i) Axielle, a XML storage library developed by Arendt Software(3) in O2 objective-oriented DBMS and (ii) ActiveView systems, built, distributed and restored by the Verso project in INRIA. The demonstration is based on a simple e-commerce application that will be described in Part 2. E-commerce is appearing as a major Web-supported application. It involves the processing and exchange of data (such as product directory, yellow page, etc.) and must provide (i) database features (request language, transaction, competition control, distribution and restoration) for effective management of large data and hundreds of users as (ii) standard data storage and exchange format.
XML is used to represent the synthesis and structure of information resources, but a variety of XML charts defined by independent organizations without any standards or guidelines make it difficult to share the graphic meaning of XML encoded information resources. In this article, we presented a mechanism called MSDL represents the accurate meaning of XML labels by describing the structural and graphic differences of standard data in standard data.
We describe an algorithm to estimate the number of pages captured by a part of a B tree index or a complete scan. The algorithm obtained an estimate of the number of pages captured by an index scan, when given the selected score and the number of current available LRU bubbles. The algorithm has a initial phase, before any estimate is carried out once. This initial phase, involving the LRU bubble simulation, requires to scan all the index input and calculate the number of pages captured by different bubble sizes.
In CS, the server manages the disk version of the database. After the client gets the database page from the server, it is stored in their bubble pool. The client is updated on the storage page and creates log records. The log records are stored locally in the virtual storage and then sent to the single log on the server. ARIES/CSA supports the proper recovery in the client server (CS) architecture. In CS, the server manages the disk version of the database. After the client gets the database page from the server, it is stored in their bubble pool.
Object-oriented database management system has been implemented, with no accompanying theoretical basis of limitations, query specifications and processing. The model-based object calculation submitted in this article provides the theoretical basis for the description and processing of the object-oriented database. We consider the object-oriented database as the interrelated category of network (i.e. intention) and time-changing object combination model collection (i.e. extension). Object calculation is based on the logic of the first order. It provides the object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object
As a database system magazine, it is committed to the progress of the information system architecture, the impact of emerging technologies on the information system, as well as the academic contribution to the development of new applications, it has made significant progress in the design, implementation and evaluation of the database and other information collection systems, its range ranges from the development of dedicated hardware, the design of innovative software methods, the integrated system architecture, the design analysis and performance evaluation of the new technologies to the information introduction and capture.
In July this year, the U.S. and international committee responsible for SQL standards completed a new compulsory style specification called the Call Level Interface (SQL/CLI).
HG tree is a multi-dimensional indexing tree designed by the point and is a simple modification of the Hilbert R tree to the space data index.HG tree data search method mainly uses the Hilbert index value to search for accurate data instead of using the conventional point search method, as used in most R tree paper.Hilbert curve values and MBR can reduce a space coverage of a MBR.
One of the main tasks is to combine information from an unusual source of information, which may include, for example, removing the return and solving the incoherence, which is favorable to the most reliable source. The problem becomes more difficult when the source is not structured / semi-structured, we have no complete knowledge of their content and structure. In this article, we show how many common combination operations can be defined as non-projects and short.
Publisher Review ServiceGlobe System provides a platform that electronic services (also known as services or Web services) can be implemented, stored, published, discovered, deployed, and dynamically referred to the participation of the ServiceGlobe Federal Volunteer Internet Server. the next generation of Internet applications - electronic services - are emerging. through electronic services, a person understands a separate software component, is identified by a unique resource detector (URI) unique, and can be used by using the standard Internet protocols, such as eXtensible Markup Language (XML), SOAP, or Hypertext Transfer Protocol (HTTP). electronic services can be combined with multiple applications that the user needs, such as the supply chain structure of different components for one service, but as a single software component, a single component of the software.
SQL Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database Database
We introduced a model mining algorithm that is approximately linear in the number of models embedded in the database, regardless of the length of the longest model. Compared, the longest model length is based on the previous algorithm, based on the length of the longest model.
In the past few years, within the scope of information technology distribution, the information technology industry has witnessed a multi-dimensional programming revolution. Additional data sources have proposed two connections to challenging data management solutions - processing unprecedented databases and providing real-time analysis advertising in the main production databases, without damaging the normal trading work load performance. At the same time, computer hardware systems are expanding flexibility, expanding to a complete number of processors and core, and through the existing processors and core to do a complete expansion, it is through a single extension, through a single extension, through a single extension, through a single extension, through a single extension, through a single extension, through a single extension, through a single extension, through a single extension, through a single extension, through a single extension
A common query for big proteins and genetic sequence data sets is to find a target similar to the input query series. The current collection of popular search tools, such as BLAST, uses silicon to improve such search speed. However, this silicon sometimes misses the target, in many cases it is unnecessary. Alternative BLAST is to use a precise algorithm, such as Smith-Waterman (S-W) algorithm. However, these precise algorithms are calculated very expensive, which restricts their use in practice.
Genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm, genal algorithm is genal algorithm.
The next few years will be witnessing significant advances in wireless communication and positioning technology. Therefore, the tracking of the changing location of the objects that are able to continue to move becomes more and more possible and necessary. This article proposes a new, R*-based indexing technology that supports effective queries of the current and expected future location of these moving objects. The technology is able to index the objects that are moving in one, two and three-dimensional space.
Although there is a lot of research on data mining, so far there are few research prototypes or business systems that support comprehensive query-oriented mining, which promotes data interactive exploration. Our paper is that the restriction structure and the optimization they cause play a key role in mining query, thereby significantly improving the usefulness and performance of mining systems.
For most decision support systems (DSSs), there are many common data manipulation requirements; these requirements are currently being solved by the use of complex decision support engines working with RDBMS. Since RDBMSS is the most popular data storage layer, there is the opportunity to provide many of the most common data manipulation requirements in the database.
In this article, we introduce a mechanism for translation of restricted queries, i.e. Polish restricted expressions, cross-normal information sources. Integrating these systems is a difficult part because they use a wide range of restrictions as a graphic queries vocabulary. We describe the algorithm, applying the user's map rules, will queries restrictions translate into those understood and supported in another background, for example, it uses the appropriate operator and value format. We show that the translation queries are the least original. In addition, the translation queries are also the most compact possible.
Many business databases keep histograms summarizing the content of the big relationship and allow effective estimation of the query results size for query optimizer. Delayed transmission databases updates to histograms often introduce erroneous estimates. This article introduces new methods based on samples to increase the maintenance of close histograms. By programming up to histograms based on databases updates, our technology is the first to effectively keep histograms up to date and avoid calculating excess when not necessary. Our technology provides very accurate approach to histograms belongs to equal and compressed classes. The experimental results show that our new methods provide more accurate estimates than expected. By using these important aspects, we can use new histograms to update our databases, our technology is the first to effectively keep histograms up to date.
In this paper, we proposed a unified formalism, based on the past temporary logic, determining the conditions and events in the active database system rules. This language allows to define the characteristics of the database system in many different periods. It also allows to define the temporary collective. We proposed an effective increase algorithm to detect the conditions specified in this language.
Similarly, today’s majority of research and decision-making definitions are based on the fact of acceptance, knowledge and insight can be from the analysis and background of the “open” or “real” data of a huge (continuously growing) amount of data. This concept, today’s enormous number of available data sources promotes the analysis of the mixture of unusual information of data, the data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data data
In this article, we introduce a new multimedia access model to innovate large-scale search for unusual multimedia data. being able to return different media types of results, for example, using query images from different data sources to obtain relevant text files or images. using widely available data and images from different data sources, for the current user's needs, while receiving a result list, containing multiple data types, to get a comprehensive understanding of query results. to allow large-scale multimedia access, we presented a new medium query (IMH) model to explore the interrelationship between different data sources and solve the scale of the problem.
Over the past few years, the connection between the database theory and the database practice has been weakened, and we believe that the new challenges presented by XML and its applications are strengthening this connection, and we show examples of the three theoretical issues arising from our own research.
We have proposed a new category of algorithms that can be used to accelerate the execution of multi-way joining queries or involve one or more joining and one composite queries. These new evaluation techniques allow to perform multiple hash-based operations in one step without allocating intermediate results. These techniques work especially well to join the level structure, for example, to evaluate the functional connection chain along the key / foreign key relationship.
In the past few years, some single-way glass algorithms have been developed for data flow problems. Although these methods solve the scale problem, they are usually blind to the evolution of data and do not solve the following problems: (1) the quality of the glass is poor when the data develops significantly over time. (2) a data flow glass algorithm needs to be more functional in discovering and exploring the different parts of the glass.
Space data mining is finding interesting relationships and characteristics that may exist in the space database. In this article, we explore whether the classification method plays a role in space data mining. For this, we developed a new classification method called CLAHANS, based on random search. We also developed two using CLAHANS space data mining algorithms. Our analysis and experiment shows that with the help of CLAHANS, both algorithms are very effective and can lead to discovery, with the current space data mining algorithm is difficult to find.
However, these search engines often suffer from a problem that a relevant web page may not contain keywords in its web page text system by using the link structure of web documents, such as HITS, is also proposed to use the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm of the algorithm.
In the Business Database Management System (DBMS), BTree remains a common indexing technology. Integrating new indexing methods into the existing database core is a very complex and expensive task. Exceptions exist because our experience integrates the UB-Tree into TransBase, a commercial DBMS, shows. UB-Tree is a very promising multi-dimensional indexing, it has shown its superiority over the traditional access methods, in different scenarios, especially in the OLAP application. In this article, we will discuss a main issue of the UB-Tree integration. As we see, this task’s complexity and cost reduction is equivalent to the actual fact that the UB-Tree depends on the TransBase, a commercial DBMS, shows that the UB-Tree is a very prominent multi-dimensional index because we are in the business of the Tree
In search intensive database applications, such as decision-making support and data mining, the use of vertical dividing systems has significant performance advantages. To support this dividing data model relative or object-oriented application requires a flexible but powerful intermediate language. This problem has been successfully solved in Monet, our group developed the modern extensible database core. We focus on design choosing in Monet translation language (MIL), its algorithm query language, and explain its strategy optimizing the concept of how to improve and simplify complex query optimization.
Therefore, if XML is to its potential, it takes some mechanisms to publish relationship data as a XML file. To this goal, a major challenge is to find a way to effectively structure and label data from one or more tables as a series of XML files. Different alternatives are possible, depending on when this processing occurs, and the number it takes place in the relationship engine. In this article, we will characterize and study the performance of these alternatives. In other things, we explore using new extensions and integrated features to build complex XML files. In this goal, we can explore a way to effectively structure and label data from one or more tables as a series of XML files. Different alternatives are possible, depending on when this processing occurs, and the number of it takes place in the relationship.
Today’s information systems managers desperately need to improve the productivity of the programmes and reduce the cost of software maintenance. They need a variety of short databases to meet unexpected changes (see [Yankee 94]). Adaptivity is the main requirement for most companies’ information systems efforts. Management change is one of the key computing concepts of the 1990s. Object-oriented tools and development frameworks begin to provide the existing goals of productivity and flexibility. These next-generation products now need to be combined with a relatively short database to investment and promote access to business data.
A solution to this problem is to store the web content on the server side and marginal storage so that it can be quickly delivered to the end user. However, for many e-commerce sites, the web page is based on the current state of the business process, represented in the application server and database. Because the application server, database, web server and cache are independent components, there is no effective mechanism to change the database content reflected on the web page. Therefore, most application server must dynamically create the web page as a non-cache web page.
If alternative options are used in the external portfolio to produce initial operation, personal records are deleted and inserted in the type of operation work space. Variable length records introduce the potential complex memory management and the need for additional copy records. Therefore, a few systems use alternative options, although it produces a longer run than the algorithm used. We experimentally compared several algorithms and variables to manage this work space. We found that the simple best suitable for the algorithm to memory using 90% or better, running the length of more than 1.8 times the work space size, no additional copy records and other very few top, for a wide record size and wide memory size.
We propose a new resource allocation framework, based on the concept of microeconomy. Specifically, we solve the difficulty of managing resources, in a more demanding environment, the broker can the best performance through planning and resource allocation decisions to maximize profits. In addition, the broker adopts dynamic technology and by changing all previous resource requirements, while conducting the main element of the survey is the resource broker to profits, using a performance-based "currency" competitor.
Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution Distribution
In this paper, we studied the effective updating algorithms of structural index, we studied two types of updates - adding a font designed to represent the addition of a new file to the database and adding a marginal, representing a small increase in change. We focus on the examples of three structural index, based on the graphic duality concept. We propose the algorithms to update the double division of two types of data structures and show how they expand to these indicators. We studied the two real world data settings updating algorithms - adding a font designed to represent the addition of a new file to the database and adding a marginal to represent a small increase in change. We focus on the three structural index examples, based on the graphic duality concept.
Many web users monitor dynamic data, such as stock price, real-time sensor data and traffic data for online decisions. These data instances can be seen as data flow. In this article, we consider creating a sustainable and efficient content distribution network so that this dynamic change of traffic data. We solved the problem of consistency of dynamic data elements: the data distributed to a storage is filtered by the storage and distributed to the storage depends on it. Our method is sustainable link errors and storage errors. This durability means the data loyalty does not even lose, when the storage (or communication path) through the user obtains the data experience experimental assessment, the use of the error case, the data distributed to a storage is filed by the storage and distributed to the storage depends on it.
This second special topic provides a theme forum that shows the usefulness of PLS-SEM, with a strong impact on the application of this method in the field of strategic management. therefore, the theme is dedicated to two audiences: academics and practitioners involved in the field of strategy and management, such as consultants.
Skyline requests a series of interesting points from potential big data points. If we are traveling, for example, a restaurant may be interesting, if no other restaurant is closer, cheaper, there is better food. Skyline requests receive all such interesting restaurants so that users can choose the most prospective one. In this article, we introduce a new online algorithm, calculating Skyline. With most existing algorithms, calculating Skyline in a package, this algorithm immediately returns the first result, producing more results continuously, and allows the user to run the time of the algorithm so that the user can control what type of result is the next (e.g., cheaper, or closer to the restaurant).
Government and industry are investing major resources in new technologies to obtain text-based sources of information, including text-based institutions, structural data, images, geospatial data, audio, video, and so on.
The main objective of this third seminar is to combine new insights of academic research with industrial-oriented development and prospects in the field of information systems.
While "now" is expressed in SQL and CURRENT_TIMESTAMP, this value cannot be stored in the database. as usual, this concept of increasing current time value is reflected in some temporary data models, including the database residence variables, such as "now" "to date changed, "**", "@", and "-". time variables are very desirable, but their use also leads to a new database type, composed of variables, called the variable database.
The goal of this work is to support copying network services, receiving updates to digital records from multiple square locations. Due toining a solid consistency peak, many copying services can tolerate the differences in their shared data as long as digital errors are limited. Target distribution services include copying stock services, online auction, distribution sensor systems, square resource accounting and load balance of copying servers. We introduce two algorithms to a smartly connected absolute error using only local information. Distribution weight AE individual boundaries increase and decrease, while combined weight AE combines them together. The two algorithms can be combined to provide good performance and low space.
The subject of the paper is created by listing and organizing certain subtitles of the knowledge base of all web events, we focus on subtitles, these subtitles are the subtitles of the web phenomenon, such as the intensive focus topic community, subtitles, tax trees, Kerets, etc. For example, the subtitles of the web page is a central page, with a double-directional link to other pages, we develop new algorithms, such a list of issues. A key technical contribution is to develop a model, the evolution of the web map, based on experimental observation of the web page screens. We believe our algorithm works effectively in this model and use the model to explain some statistical web phenomenon, in our experiments, we design the web page, we design the web page, we design the web page, we
Microsoft's strategic interest in the database field began in 1993, as well as the efforts of David Vaskevitch, who is now Vice President of the Microsoft Database and Trading Processing Product Development Group. David's vision is that the world needs millions of servers, which provides a great opportunity for companies like Microsoft to sell software at high and low prices. Database systems played an important role in the Vision of Vaskevitch, in fact, in Microsoft's current product plan.
At the end of May 2008, a group of database researchers, architects, users and researchers met in the Claremont Resort in Berkeley, California to discuss the situation in the field of research and its impact on practice. This is the seventh meeting in the past twenty years and is distinguished by broad consensus that we are at a turning point in the history of the field, due to data explosions and scenes of use, as well as significant transformations in computer hardware and platforms. Given these strengths, we are at an opportunity to influence research and have an extraordinary influence on computer, science and society. This report detailed the discussion and emphasized the Group's consensus on new fields, including new database architecture, programming, programming, programming, programming, programming, programming, programming, programming, programming, programming, program
The digital library is being built on a solid basis for the previous work as a high-end information system in the future. a component building method is becoming popular and has good support for key components, such as stock, especially through open file initiatives. we consider digital objects, data transfer, harvest, index, search, rights management, links, and strong interfaces. flexible interaction will be possible through a variety of buildings, using buses, agents and other technologies. the whole field is experiencing rapid growth, supporting advances in storage, processing, networks, algorithms and interaction. there are many initiatives and developments, including supporting these educations that will benefit Latin America.
In large modern enterprises, it is almost inevitable that different parts of the organization will use different systems to produce, store and search for their key data. However, only by combining these different systems of information, enterprises can the full value of the data they contain. Database Alliance is a method of data integration, in which intermediate software, consisting of a relative database management system, provides equal access to multiple unusual data sources. In this article, we describe the basis of the database alliance, introducing several styles of the database alliance and listing the conditions that each type of alliance should use.
In the traditional database system integration is carried out in an integrated mode: a query submitted, the system processed a lot of data for a long period of time, and in the end, the final answer is returned. This archaeological method is disappointing users and has been abandoned in most other computing fields. In this article, we presented a new online integrated interface that allows users to observe their integrated query progress and control performs on the aircraft. After completing a system supporting the integrated use and performance requirements, we introduce a series of technologies, expanding a database system to meet these requirements.
The Internet community has recently focused on the opposite systems, such as Napster, Gnutella and Freenet. The great vision - a dispersed machine community combines its resources for the interest of all - is forced for many reasons: scale, intensity, lack of management needs, even anonymity and resistance to review. The existing opposite (P2P) systems focus on specific application fields (such as music files) or provide file systems similar abilities; These systems ignore the sequence of data. The database community is an important question of how data management can be applied to P2P, and what we can learn and contribute from the P2P field. We deal with these issues, identifying the potential research ideas in the data management between P2P and existing P2P systems, and some of the basic data management systems describe our initial work.
Since the last episode of this list six months ago, there have been many interesting program announcements, some of which have expired, we will pass these announcements, hoping that they will make readers better prepared for future financing opportunities, but first, we will talk about the Congress' continued budget struggle, as well as the recent NASA rebellion.
In fact, the development of web services, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services automation, by web services
The appearance of the Internet-based electronic marketplace and increasingly popular, in its various forms, presented the universal challenge of exploring the market design. In this paper, we presented a domain-specific software architecture, describing an abstract component of the general market, and determining the limitations of control and data flow, as well as a framework that allows the appropriate component to implement specific market policies. The framework is implemented in the GEM system. GEM provides infrastructure services, allowing market designers to focus on market issues.
Oracle’s business mission is to realize the vision of the information age, to provide everyone with broader access to information, as well as the potential to increase power and productivity.The technical impact of the network computing vision is to access smaller databases through low-cost devices, through professional management of the network, in accordance with the open Internet work protocol.The latest release of the Oracle data server, Oracle8 provides a new technology to manage very large databases, containing rich and user-defined data types, and continues to develop to make it economically efficient for all forms of digital information stored in the database.
In many applications, from telephone fraud detection to network management, data reaches a stream, which requires a variety of statistical summary information about a large number of customers online. Currently, such applications maintain a basic integration, such as running extreme values (MIN, MAX), average values, standard deviations, etc., can be calculated through data flow in a limited space in a simple way. However, many applications require more complex knowledge integration about different properties, the so-called related integration. as an example, a person may be interested in calculating the percentage of international telephone, above the average time of domestic telephone.
We describe a technology that can use the relative database management system to store and manage semi-structure data. Our technology depends on the map between the semi-structure data model and the relative data model, expressed in a query language called STORED. When a semi-structure data example is provided, a STORED map can automatically produce using data mining technology. We are interested in applying STORED to XML data, which is a semi-structure data example. We show how a document type descriptor (DTD), when there is, can further improve performance.
The author proposed a declaration image query language (known as PQL), capable of expressing query in an object-oriented geographical database, describing the characteristics of query. These characteristics refer to the classic characteristics of a geographical environment (geological zero, geological point, geological polymers and geological areas) and define the letter of the above language.
In this article, we describe new technologies that enable it to build an industrial intensity tool to automatize the selection of indicators in a SQL database of physical design. The tool is taken as the input of a workload SQL query and proposed a suitable combination of indicators. We ensure that the chosen indicators are effectively reducing the workload costs, by keeping the indicators to select the tool and query optimizer "steps". must evaluate the number of indicators set to find the best configuration is very large. We reduce the complexity of this problem using three technologies. First, we remove a large number of stimulating indicators from consideration, taking into account the two query combinations and cost information. Second, we introduce the optimization, which makes it possible to make a cheap assessment of the "good" indicator third, we will process a complex method for achieving a
The existing community served by SIGMOD is based on the research direction: people producing research results (academics and research laboratories members), people using research results (DBMS, intermediate software and tool providers), and people interested in the fire (prognostic users and consultants).
These advantages are not free.The challenge of this architecture (such as any integrated or distributed architecture) is to provide data consistency to the system's independent users.The way to do this is to use lock.Oracle uses multiple levels of lock: trading levels of sequence lock, examples lock in examples, and global lock in examples.
This paper introduces the General Search Tree (GiST), an index structure that supports an extensible query and data type. GiST allows new data types to index in a way that supports the natural type query; which is contrary to the previous work of the tree extensibility, only supports the traditional equality and range forecast. In a data structure, GiST provides all the basic search tree logic requires a database system, thus combining different structures, such as B+ Tree and R Tree in a single code, and opening the search tree application to general extensibility. To show the flexibility of GST, we offer simple methods of implementation, allowing it to be like B+ Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree, R-Tree
In the graphic interface used to display the database objects, the dynamic display is automatically updated as the changes in the database objects occur.Based on the expansion of the database system or interface code provides the appropriate communication, making the interaction between the two systems complex, and subsequent updates become complex.In this article, the methods based on the active rule are presented.The statement of the active rule and the modular description allow the active display to support the changes in the database or its graphic interface.Even though this method has been used to support the link between the database system and its graphic interface, it can easily adapt to the dynamic interaction between the active database system and other external systems.
Businesses today need to store data in a variety of systems, ideally through a single advanced query interface.We introduce a query optimizer design for Garlic [C 95], a intermediate software system designed to integrate data from a wide range of data sources, with very different query capabilities.Garlic optimizer expands the rule-based method [Loh88] working in an unusual environment, by defining the genetic rules of the intermediate software and using the encryption provided for the ability to include each data source.
Some researchers are interested in the design of global network systems and applications. Our paper here is that the principles and technologies of the database community play an important role in the design of these systems. The starting point is the root of the database research: we generalize the concept of data independence to the physical environment outside of the storage system. We notice the similarity between the development of the database index and the new generation of structured to the right network. We observed through the database lens to describe the appearance of data independence in the network through some recent network facilities and applications.
UniSQL/X unified relationship and object database systems are designed to support the development of applications in the standard host programming Ianguagc (such as C) or object programming language (such as C++ or Smalltalk). in particular, C++ programmers can take advantage of all the features in the C++ programming style, using UniSQL/X C++ Interfi or C programmers can access the UniSQL/X database using the embedded SQL/X (object SQL) Preprocessor and/or UniSQL/X AH (known as levcl interface).
TPC Benchmark&trade; C (TPC-C) is a modern standard for measuring OLTP performance. running TPC-C, Tandem shows a large-scale parallel configuration of 112 CPUs that reaches 10 times the performance of any other system before (and is still a better factor today).
Recently, broadcasts have attracted significant attention as a means of disseminating information, both wireless and wireless settings. In this article, we use the version to increase the competitiveness of customer transactions in the presence of updates. We consider three alternative media storage versions: (a) air: the old version is broadcast along with the current data, (b) the customer's local storage: the old version isined in storage, (c) the customer's local database or storage: a part of the server's database isined in the form of the client's multiple version of the material view.
DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool for a particular work load, automatically recommending physical design features, these features are indexed by any subgroup, materialized query tables (also known as materialized query tables), shared database divisions, and multi-dimensional group tables. Our work is the first industrial intensity tool, covering the design of four different features, a significant progress to the existing tools, supporting not only the index and materialized views. Building such tools is challenging because not only the interaction between the features introduced by the large search space, but also requires expansion to support the additional features of the tool in the future demand, we can use a "new designer" to re-adjust our tools
The panel will explore the influence of the current XML revolution on the network on the research and development of the database systems.
This paper introduces an Intelligent Information Introduction System (IIPS) framework, which provides Intelligent Interface Introduction support for data-intense web applications, using ontologies to drive the website generation and maintenance process, IIPS defines a series of ontologies to simulate navigation structures, compose structures and data-intense web user interfaces, and provides a set of supporting web site creation, maintenance and personalized tools.
The Association Rules of Mining has recently attracted strong attention. in general, a series of data projects is available. users are interested in a wide range of Association Rules, which cover a series of different levels, as sometimes more interesting rules can be produced by considering a series.
We are increasingly becoming a data-oriented society with a great deal of information demand and more and more online data sources. ETH Database Group’s research activities focus on exploring and managing COSMOS data’s architecture and technology research, its spread and data diversity, as well as its internal abnormality. Our key goal is to provide a range of data connections that can interact and integrate through data sources and application systems at different levels.
Recovery activities, such as logging, checkpoint and reboot, are used to restore a database to a consistent state, after the system collapse occurs. Recovery related top is problematic in a main memory database where I/O activity is the sole purpose of execution to ensure the sustainability of data. In this article, we introduce a recovery technology of the main memory database, its advantages are as follows. First, the disk I/O is by logging in to the disk the advantages are recorded only during normal execution. The disk the advantages are recorded only during normal execution.
Central to any XML query language is a path language, such as XPath, running in the wooden structure of XML documents. We prove in this article that the wooden structure can be effectively compressed and manipulated, using the technology produced from the symbol model check. Specifically, we first show that the short representation of the underground tree structure of the document is very effective. Second, we show that the compressed structure can be directly and effectively query, through a process of manipulation of the choice of the nodes and partial decompression. We study the the theory and experimental characteristics of this technology and provide the algorithm query our compressed examples using the nodes to choose the path query language, such as XPath. We believe that the ability to store and process the majority of the structure of the document is very important in the XML memory and the
The current web service standard allows to publish the description of the service and find the correspondence of the service based on the standards, such as method signature or service category. However, the current method does not provide the basis for choosing a good service or comparing service rating. We describe a concept model of reputation use, reputation information can be organized and shared, service choice can be convenient and automated.
Capacity mature pattern (4) is an organization that defines the order of the existing software development process capabilities and sets priorities for improvement.
Editor's Note: For this topic of "from editing" observations, I invite Robert Gafat about his observations, as a world-famous quality writer himself, Bob in an excellent location to provide observations, how writers can increase their quality research receiving publication opportunities at A!vII in the past two and a half years, I have developed a huge respect for Bob's hot eye assessment of quality research submitted, and especially appreciated quality research, he gave the writer a shocking suggestion on how to improve his work. as a world-famous quality writer himself, Bob in an excellent location to provide observations, how writers can increase their quality research receiving publication opportunities at AMI's three e-mail discussions and quality research results, Thomas, I concluded
Space data mining, that is, finding interesting characteristics and patterns that may be randomly present in the space database, is a challenging task, because the huge number of space data and the new conceptual nature issues, must be responsible for the problem of space data mining. Classification and regional-oriented queries are common issues in this field. Several methods have been submitted in the past few years, all of these require at least one scanning of all individuals (point). Therefore, the complexity of calculation is at least a linear proportion of the number of objects for each query. In this article, we propose a new conceptual information network approach based on the problem of space data mining to further reduce the cost. This idea is with the common issue of space-related statistical information cells in this field.
Due to its expression capacity, conventional expressions (REs) are rapidly becoming a component of several important application scenarios of language specifications. Many of these applications have to manage a huge database of RE specifications and need to provide an effective matching mechanism, as a input chain, fast identifying search database of REs, suitable for it. In this article, we presented a new indicator structure for large database of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs specifications of REs
In the TelegraphCQ project, we implement these applications using a general continuous query (CQ) engine, performing long-term data flow. In order to expand the performance of these data streams, we parallel them in a workstation collection. For these key applications, penetration or rejection of service detection, click flow processing and expansion are important goals. These goals are challenging to a collection because the machine is connected to failure, load imbalance may occur. In this design, we can develop a Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux, Flux,
This article describes how the Internet and the world’s networks influence the databases and data storage and the continuous impact of these areas.
XQuery Formatting is the W3C XML Query Working Group is ongoing efforts to determine XQuery's accurate format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format format
Extensibility is one of the main advantages of the object relationship database management system.We have used this system property to implement a StateMachine module within the object relationship database management system.The module allows to check the dynamic integrity limits and perform the active behavior specified by UML.Our method shows that extensibility can be effectively applied to integrate the dynamic aspects specified by UML into the object relationship database management system.
In this article, we introduce a new index structure, called Δ Tree, to accelerate the processing of the high size of the K nearest neighbor (KNN) queries in the main memory environment. Δ Tree is a multi-level structure, each level represents data space in different sizes: the number of sizes increases to the surface level, which includes data in its full size. The remaining size is obtained through the main component analysis, it has the required properties, the first sizes capture the majority of the information in the database. Each level of the tree serves to scan the search space in a more efficient way, because the reduced size can better use the small storage space size. More, the calculation distance in the low size is lower cost.
Abstract Spatial-Query-by-Sketch is the design of the query language of the geographical information system. It allows users to create a space query by drawing the necessary configurations on a touch-sensitive computer screen with a pen and convert this query to a symbolic representation that can be processed by the geographical database. Because the query configuration is usually not fully matching the query, it requires relaxing the space limitation of the query. This article describes the representation of the query and describes the design of the limitation relief method used in query processing.
Many commercial XML data accompanies XSD specifications. In many scenarios, "dividing these XML data into a relative storage is a popular paradigm. Optimizing XML data for XPath queries requires careful attention to the same logical and physical design of the relative databases in which XML data is distributed. No existing solution has taken into account the physical design of the relative databases produced. In this article, we studied the interaction of logical and physical design and came to the conclusion, 1) independently solving them leads to low optimization performance and 2) has significantly exceeding logical and physical design: some known logical design transformations produce the same geographical and physical design. In addition, there is an existing search algorithm in the search space for logical design and search space for logical design in this article, we find the conclusion that
Expanded transaction models in the database are motivated by complex applications such as CAD and software engineering. Transactions in these applications have a variety of requirements, for example, they may be long-lived and may need to cooperate. We describe the ASSET system that supports expanded transactions. ASSET consists of a set of transaction primates, allowing users to define customized transaction parameters to meet the needs of a specific application. We show how transaction primates can be used to define a variety of transaction models, including dividing transactions, dividing transactions and details.
For example, in the database of a bank client, age and balance are two digital properties, while CardLoan is the Brain properties.
This paper introduces a model of the Normandy intermediate software system that supports a temporary consistent structure related to XML documents. Specifically defined advanced operations interchange in most cases, reducing the number of transactions and increasing the system availability.
While expanding to the vast and growing Internet population with unpredictable use patterns, e-commerce applications face serious costs and administrative challenges, especially databases servers, these applications’ supporters are deployed in multi-layer configurations. medium-layer databases cache is a solution to solve this problem. In this article, we introduce a simple expansion to the existing federal features in DB2 UDB, allowing a conventional DB2 example to become a DBCache without any application modification. On the application server deployed a DBCache, voluntary SQL declaration, unchanged application, designed for the backdrop databases servers, can be answered: in cache, on the backdrop databases servers, or in two ways distributed in this article, we propose a simple expansion to the existing federal features in DB2
In the Business Database Management System (DBMS), BTree remains a common indexing technology. Integrating new indexing methods into the existing database core is a very complex and expensive task. Exceptions exist because our experience integrates the UB-Tree into TransBase, a commercial DBMS, shows. UB-Tree is a very promising multi-dimensional indexing, it has shown its superiority over the traditional access methods, in different scenarios, especially in the OLAP application. In this article, we will discuss a main issue of the UB-Tree integration. As we see, this task’s complexity and cost reduction is equivalent to the actual fact that the UB-Tree depends on the TransBase, a commercial DBMS, shows that the UB-Tree is a very prominent multi-dimensional index because we are in the business of the Tree
The seminar explores the most advanced methods for storing and obtaining multimedia data from a big database. recording (= file) may include formatted fields, text, images, voice, animations, etc. 4 samples queries we want to support is "in the collection of 2D color images, finding images similar to the sunset." images and other media index is a new, active field of research; the seminar will introduce the latest methods and prototypes for 2D and 3D medical image databases, 2D color image databases and LD time series databases.
We describe an algorithm to estimate the number of pages captured by a part of a B tree index or a complete scan. The algorithm obtained an estimate of the number of pages captured by an index scan, when given the selected score and the number of current available LRU bubbles. The algorithm has a initial phase, before any estimate is carried out once. This initial phase, involving the LRU bubble simulation, requires to scan all the index input and calculate the number of pages captured by different bubble sizes.
The forecast queries on space time data have proved to be critical in many location-based services, including traffic management, travel sharing and advertising. In the past few years, one of the most interesting work on space time data management is forecast queries. In this article, we review the current research trends and present the relevant applications in the field of forecast space time queries processing. Then we discuss some of the basic challenges arising from new opportunities and open issues. The paper aims to catch interesting fields and future work under the shadow of forecast space time data queries.
Activities are the core of reaction and active applications, which are becoming more and more popular in many areas.
Multi-dimensional database technology is a key factor for decision-making purposes to carry out a large amount of data interactive analysis. Unlike previous technologies, these databases view data as a multi-dimensional database, these databases are suitable for data analysis. Multi-dimensional models classify data as facts with the relevant digital measurement or as the description of the text size. Question collective measurement values over a series of size values to provide results, such as monthly total sales of a product. Multi-dimensional database technology applies to distributed data and new data types, the current technology is often unable to properly analyze. For example, classic technologies such as pre-configuration cannot ensure fast response time, when the data display is from the sensor to the sensor to the sensor to the sensor to the sensor to the sensor to the sensor to the sensor
Dividing transactions into fragments is beneficial for performance, but it can lead to irregular implementation.Many researchers react to this by inventing new competitive control mechanisms, weakening the regularity, or both.
A decade ago, the connection between objects and databases was new, and a series of different explorations were carried out within our community. By the concept, the management of traditional business data is mainly a problem-solving, the project is studying ideas such as adding abstract data types to the relative databases and building extended databases systems, objects databases systems, and tools packages to build a dedicated databases system. In addition, the work was carried out elsewhere in the computer science research community by extended programming language and databases inspired features such as durability and transactions. In this article, we look at our field is a problem a decade ago, now it is the databases support objects (in the contrary). We are all studying the project and business databases systems. We share our vision and vision, our databases
Continuous query (CQ) systems usually use the universality of query expressions to improved efficiency through shared processing. Recently proposed CQ systems have introduced window specifications to support unlimited data flow. However, a few surveys have been conducted on the sharing of window query operators. In this article, we discussed the shared implementation of the window connection, which is the core operator of the CQ system. We show that the strategy used in the system has so far had an unreported performance error that can negatively affect the query with a relatively small window. Then we presented two new shared connection implementation strategies.
This is the Lord, this is the Lord, this is the Lord, this is the Lord, this is the Lord, this is the Lord, this is the Lord, this is the Lord, this is the Lord.
The Brazilian Database System Seminar (SBBD) is a traditional Brazilian conference, sponsored by the Brazilian Computer Association, and the SBBD's technical program includes the following activities: the competitors review the full technical thesis, the invitation of the conversation, the tutorial (invited and selected from the submission), the discussion of the panels and the introduction of the tools.
Keywords indicators, thematic catalogues and link-based rankings are used to search and structure today’s fast-growing web pages. Surprisingly, rare use is carried out by millions of browsing experiences and concentrated groups of interests. In fact, this information is an artificial difference between the browser’s history and the intentional icons. Even the intentional icons are stored in a passive and isolated way. All of this is against Vannevar Bush’s dream Memex: an enhanced add-on of personal and community memory. We suggest to show a “Memex” for the beginning of the web page: a browsing assistant for the individual and group’s attention.
From Publisher: Learn how to easily set up and manage a business site with Microsoft's latest up-to-date Web Solutions Pack, Web Server 3. With accurate instructions and various images, Web Expert Brad Harris shows all the tools you need to run on the Internet. Covering from background management to e-commerce to website performance analysis, Microsoft Web Server 3 is a step-by-step resource to build a successful Web business.
We provide a principle extended SQL, called SchemaSQL, providing the ability to unify operations of data and charts in a relatively multiple database system. We develop a precise charts and charts in a way that extends the traditional SQL charts and charts and shows the following. (1) SchemaSQL keeps the taste of SQL while supporting the query of data and charts. (2) It can be used to convert the data of the database into a structure that is clearly different from the original database in which data and charts can be exchanged.
We describe the SORAC concept model, a data model system developed at Rhodes University. SORAC supports two similar objects and relationships, and provides a tool that the model database requires a complex design field. SORAC's collection of built-in similar relationships enables the designers to determine the implementation rules, keep the objects and the relationship type limitations. SORAC then automatically produces C++ code to keep the specified implementation rules, producing a graph that is compatible with Ontos. This promotes the task of the graph designers who no longer need to ensure that all the objects category methods properly maintain the required limitations. In addition, the specific implementation rules of the implementation rules enables the automatic analysis of the implementation rules.
Graphic matching is the task of finding the sequence matching between the two graphic elements. It is required in many database applications, such as web data sources integration, database loading and XML message map. To reduce the user’s effort as possible, the automatic method of combining several graphic technologies is necessary. Although such graphic method has recently found a significant interest, how best to combine different graphic algorithms issues still require further work. We therefore developed the COMA graphic matching system as a platform to combine multiple graphics in a flexible way. We offer a wide range of personalized graphics, especially a new method designed to repeat the use of previous graphic operations results and use multiple mechanisms to combine the graphic performance results as a graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graphic graph
WebView is a web page created automatically from the base data, usually stored in DBMS. Given the database supported web server behind the multi-layer architecture, we have the opportunity to contain it on the DBMS inside, on the web server, or not, always on the aircraft (virtual). Because WebViews must be updated until now, materialized WebViews is immediately updated to the base data every update. In this article, we compared three materialized policies (materialized in DBMS, materialized on the web server and virtual) analysis, through detailed cost models, and quantitative, through a wide range of experiments on the implementation of the system. Our results show that materializing on the web server is a more extensible solution and can be easily updated to a virtual user, even within the DBMS.
The method builds a sequence of combined index and converts the sequence of indicator tracking operations into a simple search in the combined index file, thus accelerating the navigation of the object-oriented database. The method expands the combined index structure that is studied in the relative and spatial databases, supports the forward and back navigation between objects and classes, and finds up-to-date spread in the schedule. Our performance studies show that some combined index-oriented processing is beyond several other index mechanisms.
About a year and a half, an informal and open group meeting considered how to use the Java programming language and the relationship database can be used together. Initially known as JSQL, later known as SQLJ, the group of companies involved was Compaq (Tandem), IBM, InfoMix, Micro Focus, Microsoft, Oracle, Sun, and Sybase. When it was formed, the group intended to advise and review each other’s ideas, meeting quite frequently, see where there is a common understanding and protocol on synthesis and grammar, and ultimately provide one or more formal standards of the work began a suggestion on how SQL statement can be embedded in Java, by Oracle in advance.
In this article, we describe the design and implementation of ParSets, which is the means of using parallelism in SHORE OODBMS, we use ParSets parallel OO7 OODBMS's graphic cross-finals and provide parallel SHORE running these cross-finals, these cross-finals on a set of standard Ethernet-connected cargo workstations. for someOO7 cross-finals, SHORE achieved excellent speed and scale; for otherOO7 cross-finals, only the speed and scale occur.
Over the past decade, the database research community has achieved valuable achievements in the simulation and access of space objects, for example, within a time frame, GIS representation and management plays an important role in data manipulation and query, in fact, time integration may lead to consideration of these very time-dependent space data (e.g., temperature, land use coverage, popular information related to a specific area, etc.) and moving objects (e.g., changing the relative space location of entities).
The number of processor storage errors has a critical impact on the performance of the large main memory configuration of the server running DBMS. On the contrary, the use of the database storage system is very dependent on the record in the main memory of the physical organization. The recently proposed storage model, known as PAX, has significantly improved the performance of the serial file scanning operations compared to the usual implemented N-ary storage model. However, the PAX storage model can also be used for other common operations, such as index scanning, showing poor storage.
Approximately every five years, a group of database researchers meet to carry out self-evaluation of our community, including thinking about our impact on the industry and the challenges we are facing in the community.
Infomaster is an integrated information system that provides integrated access to multiple unusual information sources on the Internet, thus giving an illusion of a central, uniform information system. We say Infomaster creates a virtual database. The core of Infomaster is a promoter, dynamically determining an effective way to respond to the user's requests to use as little as possible sources, and coordinating the unusual between these sources. Infomaster processes the structure and content translations to solve multiple data sources and multiple applications gathered data. Infomaster connects to various databases using plugins such as Z39. SQL database, through ODBC, EDI transactions, and other Wide Web directory (Wide Web directory) has several unusual users in these sources.
Java programming language has been published as a web programming language since its establishment. Many programmers have developed simple applications such as games, clocks, news icons and library icons to create information, innovative websites. However, it is important to note that Java programming language has greater capacity. Language components and structures are originally designed to improve Java’s functionality, as a web-based programming language can be used in a wider range. Java provides a developer with tools that enable the creation of innovative network, database, and graphic user interface (GUI) applications. In fact, Java and its related technologies, such as JDBC API [11,5], JDBC drivers [12], threats and AWT provides programming components to help develop a platform that can create an independent database.
The current state of health promotion Health promotion has made very impressive advances over the past twenty years. it is from an innovative idea that created conceptual meaning, but without scientific support to a mature field that supports more than 1,000 experimental studies, proves positive health and financial effects of the program, and practices almost all major employers in the United States. despite this progress, health promotion is not part of major medicine. only a small part, less than 1%, per year spends on health care for health promotion. despite our development of health promotion science, it is not recognized as a mature science by any scientific group.
In the spring of 2003, Joe Hellerstein of Berkeley and Natassa Ailamaki of CMU collaborated to design and run a parallel version of the database course, which exposed students to developing code in the core of a full-functional database system.
The research work in the programming language and database systems is addressing the same issues of scale, change and complexity. This paper explores the current difficulties of data related to the constantly changing program. It describes the current interface on the programming methods and the algorithm design. It recognizes the need for new language primitivity to include the database concepts and has studied some outstanding primitivity.
We propose a product specification database, suitable for product evolution, simulating product specification as an object. In this database, we propose a behavior limit to maintain consistency. In addition, this database can manage visual specifications, such as operating specifications, which is difficult to process. We have developed visual CASE: an object-oriented software development system for domestic equipment. Visual CASE is a visual prototype system, based on the object model we propose. In this article, we show that product specifications are easy to review, using visual prototypes. We also discuss the application of the database issues to the home software development process.
The previous study on the rules of the mining association found the rules of a single conceptual level, however, the rules of the mining association at multiple conceptual levels could lead to finding more concrete and concrete knowledge from the data. In this study, a advanced rule of the mining association was developed from a large trading database, by expanding some existing rule of the mining technology.
Similarity recycling mechanism should use the general square distance function as well as the Oakland distance function, as the Oakland distance query parameters can be different from the user and the situation. In this article, we introduce space conversion technology, producing a new search method to adapt to the Oakland query with the square distance function. The basic idea is to convert the marginal straight corner in the primary space, where the distance from a query point is measured through the square distance function, in a new space space object, where the distance is measured by the Oakland distance function. Our method significantly reduces the cost of the CPU, because the distance is closer to the space conversion; the accurate distance assessment is to avoid access to the most close line in the original space, where the distance from a square point is measured through the space
This paper shows some of the new introduced parallel implementation methods in Oracle RDBMS. These methods provide highly extensible and adaptative assessments for the most common SQL operations - connectivity, composition, rolling/circle, group and window features. The innovations of these technologies are that they use multi-stage parallel modeling, adapting errors, as well as running time parallelization and data allocation decisions. These parallel programs adapt on the basis of the statistics collected on the real data in query implementation time.
All existing queries for XML proposals (e.g., XQuery) depend on a model specification language, allowing (1) route navigation and through XML data chart labelling structure branches, (2) forecasting the value of specific route/ branch nodes to the required data elements. Optimizing such queries depends mainly on the existing short synthetic structure, allowing accurate compilation time selective estimation of complex route expression, through the chart structure of XML data. In this article, we extended our previous work, through the structure of XSKETCH synthesis, and proposed a (added) XSKETCH synthetic model, using the positioning stability and value distribution (i.e. the real-time) of the overall structure, to make it easy to make complex structures in space
The author claims that the model will include the IFO data model [Codd 70], the entity relationship model [Chen 76], the functional data model [Kerschberg 76] and almost all structured data models information models support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Hammer 81] support the serial data model [Ham
The real data mining/analytic application requires a framework that properly supports the knowledge of the discovery as a multi-stage process, in which one mining operation input can be another production. Previous research, mainly focused on a similar mining task of fast calculation, ignored this key problem. Motivation of this observation, we develop a unified model supporting all major mining and analytic tasks. Our model is made by three different worlds, the corresponding intensity and extension of size, and data set. The concept of size is the model's central part. Equipped with a level, the size integrates a clearly visible mining and analytic operation of production in a clean way. We propose an algorithm called the size, the algorithm, the algorithm (operation) as the size, our model includes three different worlds,
The concept of regularity is a standard of accuracy traditionally accepted in database systems. however, in multi-database systems (MDBS), ensuring global regularity is a difficult task. difficulty is due to the abnormality of the competitiveness control protocol used by the local database management system (DBMS), as well as the desire to maintain the autonomy of the local DBMS.
Privacy - control of personal data - and security - unauthorized others attempt to access the data - are two key issues for e-commerce consumers and websites. anyway, consumers will not access or shop on a website, nor can the site work effectively, without taking into account both. This chapter reviews the current state and relevance of privacy and security corresponding. We are from a social psychology, organization, technology, regulatory and economic perspective of privacy. We then from a technical, social and organizational and economic perspective of security.
For the purpose of addressing the needs of applications that require a wealth of temporary data and queries, we have established a temporary expansion of the Information Base system based on DataBlade technology. Our TIP DataBlade expands Information with a wealth of data types and habits to make temporary models and queries easy.
There are recent interests in the use of the relative database system storage and query XML files. In this context, each technology proposed is through (a) creating charts for storing XML files (also known as the relative charts generated), (b) storing XML files by dividing them into the series of the creation charts, and (c) converting on XML files queries to the creation of charts SQL queries. Because the relative charts generated is a physical database design problem - depending on the nature of the data, query work load and the availability of charts - there are already many technologies proposed for this purpose.
We consider the line tracking problem in a storage environment: for a data project in a material storage view, we want to identify the source data project that produces a view project. We formally defined the line tracking problem, developed a line tracking algorithm that integrates a relative view and proposed a line tracking mechanism in a multi-resource storage environment. Our results can form the basis of a tool that allows analysts to browse the storage data, select a view map, and then "through" check the exact source map that produces a view map.
Learning enhanced relevance feedback is one of the most active fields of research in recent years, based on content image receipt. However, several methods of using relevance feedback are currently available for handling relatively complex image receipt methods in the big image database. In the case of complex image queries, character space and user perception distance function are usually different from the system. This difference leads to a query representation with multiple collections (i.e. areas) in character space. Therefore, it is necessary to deal with distinctive queries in character space. In this article, we propose a new content-based image receipt method using adaptative classification and collection to find complex image queries multiple collections.
Transparent computing introduced data management requirements must be processed in more and more light-level computing devices. the chip on the personal folder, sensor network and data hosted by an independent mobile computer are different images, requiring an assessment of the problem restrictions on the hardware limitation computing equipment. RAM is the most restrictive factor in this context. the paper provides a thorough analysis of the RAM consumption problem and makes the following contribution. First, it proposed a survey of the implementation model, achieving a lower limitation in the sense of RAM consumption. Second, it proposed a new form of optimization, called iteration filter, which significantly reduces the prohibited cost by the previous model, and does not damage the lower RAM limitation.
Brum Filter is a spatial effect of random data structure, allowing members to query sets with some permissible errors. It is widely used in many applications, taking advantage of its ability to micro represent a set, and effectively filter any element does not belong to a set, with the possibility of small errors. The file introduces the spectrum Brum Filter (SBF), an extended original Brum Filter to multiple sets, allowing the filtering of elements, its diversity is less than a limit in the query time.
The digital library brings integrated, managed and communicated multimedia data in a distributed environment. The digital library system is currently viewing users as static when they access information. But in the near future, tens of millions of users will get access to the digital library through wireless access. The user provides digital library services, its location is constantly changing, its network connection is through wireless media, its computing capacity is low requiring modifications to the existing digital library system. In this article, we identify problems that occur when the user moves, classify the specific user queries, and introduce a main feature of the architecture that supports flexible and transparent access to the digital library.
The paper discusses issues related to the integration of space operators into the new generation of query languages similar to SQL. Starting from the space data model, the current space expansion of query language has been briefly reviewed and emphasized the research direction.
Mobile requires system adaptability, one method is to make adaptations transparent, allow them to remain unchanged, an alternative method will be considered as the partnership between the applications and the systems, this paper is our state report on both sides, we report our significant experience in application transparency adaptation, in the Code file system, we also describe our ongoing application awareness adaptation work in Audi.
Life scientists need to analyze their data, using new or background-sensitive methods, which can be published in recent magazines and publications, or based on their own assumptions and assumptions. Genetics Research Network Architecture (gRNA) is a highly programmable, modular environment, specifically designed to enhance the development of genetics research tools. Genetics Research provides a development environment where new applications can be rapidly written and deployed in a environment where they can systematically use computing resources and integrate information from distributed bio-data sources.
An important step in integrating the database is to match the same properties: determine which fields in the two databases refer to the same data. The meaning of information can be embedded in a database model, a concept program, an application or data content. Integration involves extracting the same data, expressing them as data, and matching the same data elements. We introduce a classifier to classify properties according to their field specifications and data values, and then train a neuron network to identify the same properties. In our technology, the same data elements knowledge is from "discovered" rather than "pre-programmed".
For this reason, because comparative control is a precise problem, it has developed a huge literary body, researching the performance of comparative control algorithms. Most literature uses analytical models or random numbers referring to simulation, and clearly or implicitly make certain assumptions about the transaction behavior and relative arguments, they set and not set the arguments. Because of the difficulty of collecting appropriate measurements, only a few studies, using tracking referring to simulation, and not yet researching the specificities of comparative control behavior. In this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we in this paper, we
This chapter explores the basic functions of the quantum data model. It takes into account examples of the quantum model from biology and quantum physics measured. the data model in both cases is similar, but commonly used operations are quite different. in the second case, for example, a person is often interested in the behavior conditions of the local operator, such as zero flow surface, while in the biological case, a person has a mix of value conditions and geological state. In these two areas of application, this chapter emphasizes the universality and flexibility of the model. the system is based on a commercial database, adding a special function to manipulate the quantum data model.
The optimization capacity of RDBMS makes them attractive for data conversion. However, although many useful data conversions can be expressed as a relative query, an important data conversion category, producing multiple output double single input cannot be expressed in this way. To overcome this limit, we recommend expanding the relationship algorithm to a new operator called data map. In this article, we formally data map operators and study some of its characteristics.
A cross-disciplinary research community needs to solve the challenging issues that are raised in the information systems through the application of workflow management technologies.This conclusion is the result of the NSF Workflow and Information Systems Process Automation Seminar held in the Botanical Garden of Georgia on 8-10 May 1996.The seminar gathers active researchers and practitioners from several communities, from database and distribution systems, software processes and software engineering, as well as computer-supported collaboration.
The OASIS prototype is being developed at the University of Dublin, Ireland, and we describe a multi-database architecture, using the ODMG model as a channel model, and describe the extension of building virtual charts in multi-database systems.
The database system involves structured data, unfortunately, the data remains often in an unstructured way (e.g., in the file), even if it has a strong internal structure (e.g., electronic files or programs).
We describe the design and implementation of real-time data management services that combine technology developed in the background of real-time distribution of object management, object DBMS and timetable, which simplifies many services and produces greater results than the total of their parts as it can be used to improve the mobility and flexibility of real-time applications.
Online analytics processing (OLAP) and data storage are a decision-making support technology that aims to gain a competitive advantage by using the increasing amount of data collected and stored in the company’s databases and files to make better and faster decisions.
Distributed Information Search Component (DISCO) is a prototype of unusually distributed database that can be accessed by basic data sources.DISCO prototype is currently focused on three core research issues in the background of these systems.First, because the capacity of each data source is different, it is difficult to convert the query to the data source query.We call this problem a data source problem.Secondly, because each data source operates in a generally unique way, the cost of operation may vary from one converter to another way.We call this problem a basic cost issue.
As well as known, the background plays an important role in the sense of the artwork, this paper discusses the dynamic background of the collection of multimedia documents linked to which the web pages are perfect examples. background document grammatics appear by identifying the browsing routes of different users, despite this multimedia collection. In this paper, we introduce the technology of using multimedia information as part of this definition. Some of the consequences of our method is that the author of the web page cannot fully define the document's grammatics and grammatics appear by use.
Despite the decades of study of the AQP (approximate query processing), our understanding of the sample-based combination remains limited, to a certain extent, even on the surface. The common belief of the community is that adding random samples is useless. This belief is mainly based on a early result, indicating that adding two single samples is not an independent sample's original combination, and it leads to a quarter of poor production. Unfortunately, this early result is rarely applicable to the critical issues faced by practitioners. For example, successful measurements are often ultimately close to accuracy rather than output cardinality. In addition, there are many non-united samples strategies that can be used.
He built a cost-based optimizer, Ingres was still familiar with only small modifications, his design survived until today. his Angres colleagues remembered his strong determination to provide the best technology possible and his desire to do anything so that he did well on this goal, although he took a lot of trouble, but for those who arrived early at the office, he was unlikely to sleep under his table when he was in the middle of this project.
The Problem With Software: Why Smart Engineers Write Bad Code, by Adam Barr.
Since the performance of the database application depends on the use of the memory layer, the intelligent data configuration plays a key role in increasing the location and improving the memory use.The existing technology, however, does not optimize access to all levels of the memory layer and all different work loads, since each storage layer uses different technologies (smart layer, memory layer, disk) and each application access to data using different modes.Cloto is a new bubble pool and storage management architecture, putting it in the memory layer configuration from the data organization to the non-volatility storage device, so that it can be designed independent data configuration on each level of the storage layer.Cloto can be through transparently using the appropriate data configuration and non-volatility storage layer (smart layer, storage layer, storage
In this article, we introduce (1) the general superliterary framework of interaction with the table, (2) a framework of specialization to present the query results expressed in the visual language in the superliterary format.
The “e-government” period of planning, as well as the need to promote the effectiveness of the tax signal equipment between citizens and public employees, so that in the electronic identity card to find the most important way to access the network services in a safe way to the interactivity of the national network services. There are five inspirational principles: from the production of physical support to the safety of the full life cycle of the card used as a service card. Security must be dealt with the views of both the police and public employees’ personal signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and signals and sign
Previous studies have shown that cache behavior is critical for the main memory index structures. cache awareness index structures such as cache sensitive search trees (CSS-Trees) perform searches faster than binary searches and T-Trees. However, CSS-Trees are used to support the work load with relatively static data. Although B+-Trees are more cache awareness than binary searches and T-Trees, they use cache lines that are low as half space is used for storing children's indicators.
The Teradata Multimedia Object Manager is a general use of content analysis multimedia server, designed for the same multi-processing and large-scale parallel processing environment. Multimedia Object Manager defines and manipulates user definitions (UDFs) and is called in parallel analysis or manipulation of multimedia objects content. Multi-calculating intensive application, this technology uses large-scale permanent data sets, including fingerprint matching, signature checking, face recognition, and language recognition/translation.
In large federal and shared databases, resources can show widespread volatility characteristics, the assumptions made when submitting queries are rarely continued during the processing of queries, and therefore, traditional static queries optimization and implementation techniques are ineffective in these environments.
The online analytical processing (OLAP) system provides good performance and easy to use when obtaining summary information from a very large amount of data. However, the complex structures and relationships contained in the relevant non-summary data are not well processed by the OLAP system. On the contrary, the object database system is built to deal with this complexity, but does not support the summary query. This paper introduces the OLAP++, a flexible federal system that allows the OLAP users to simultaneously use the OLAP and the object database system. In a previous paper, we have defined a comprehensive framework to process the OLAP and the object database federal, including the SUMQL++ language, allowing the OLAP system to naturally obtain data from the object database and obtain data.
Multi-dimensional data is produced in a growing pace in almost all modern applications. Developing technologies and tools to extract useful information from these data is one of the key challenges of the 21st century response. Visualization is a popular technology to effective data detection by using the visual perception capacity of the field experts. Visualization involves data and information's graphic presentation to communicate results, verify assumptions and quality detection. In this presentation, we present solutions to the special challenges we face in this field, in the background of our XMDVtool project, which is funded by NSF for many years of efforts. These include multi-dimensional data visualization to be possible through visual detection, through visual detection, through visual detection, through visual detection, through visual detection, through visual detection, through visual detection, through visual detection, through visual
Placing electronic business on a clear basis – model theoretically and technically – must be seen as the core challenges of research and business development.This paper focuses on the discovery and negotiation stage of contractual-based agreements.We introduce a method from the discovery stage of multiple relationships to a single relationship in the contractual negotiation stage.
Inquiry execution engine in Microsoft SQL Server uses hash-based algorithms, internal and external connections, semi-connection, setting operations (such as cross), combination and repeated deletion. Implementation combines many technologies that are presented separately in the research literature, but never combined in one implementation, whether in the product or the research prototype. One of the paper contribution is a design, cleanly integrated with most existing technologies. However, a technology, we call the hash team, previously only in false terms, did not implement in previous research or product work. It achieves many benefits in the hash-based query processing, interesting orders in the type-based query processing. More, we describe how in complex query and query-based query-based query-based query-based query-
At our Board Meeting in Raleigh, NC, we select the following new officials for the EMC Society: President Elect - Bob Scully Vice President, Communication - Flavio Canavero Vice President, Membership Service - Bob Davis Vice President, Standard - Don Heirman Vice President, Technical Service - Colin Brench Vice President, Meeting - Bruce Archambeault Treasurer - John LaSalle
We introduce a multi-dimensional database model that we think can be a concept model for the application based on OLAP (online analysis processing). In addition to providing the features required for the application based on OLAP, the main characteristics of the model we propose are the clear separation between the structural aspects and the content. This separation concerns enables us to define the data operating language in a reasonable, simple and transparent way. In particular, we show that the database operator can easily express.
We identify a emerging database system category that is not widely processed in literature, we call it ARCS (Active, rapidly changing data system) database. These systems present some unique requirements for the monitoring and control of the database. These requirements are so that traditional data and transaction management models seem not enough. We introduce the analysis of the data and transaction characteristics in the ARCS system and identify the relevant research issues.
Database systems must become more open to keep their relevance as a technology of choice and need. Openness means that the database is not only exporting their data, but also exporting their services. This is in the classical application field and non-classic (GIS, Multimedia, Design, etc.) This paper solves the issue of export storage management services, indexing, copying and basic query processing. We describe an abstract object storage model, providing the basic mechanism, "friendly", through these services is uniformly applied to internal storage, internal definition of data, as well as external storage, external definition of data.
In this paper, we introduce the continued study of data analysis, based on our previous work on reduction methods. reduction[19] is a new data pre-treatment technology that optimizes the internal structure of data inspired by Newton's universal gravity law[16] in the real world. it can be applied to many data mining fields. in this method, data moves along the density of the gradient direction, thus making the internal structure of the data more outstanding. it is carried out on a series of networks of different cell sizes.
Oracle8i visual information access provides this facility based on technical license Virage, Inc. This product is built on the top of Oracle8i interMedia, allowing storage, access and management of images, audio and video. Image is using properties matching, such as color, structure and structure and effective content access is provided through image index type index. Index type design is based on multi-level filter algorithms. Filter reduces search space so that expensive comparative algorithms run on a group of data. Index image is used to evaluate image, image, image, image, image, image, image, image, image, image, image.
Information integration provides a competitive advantage for the business and is crucial in the computing of demand. This is the strategic field of software companies investing today, its objective is to provide a unified view of data, regardless of the differences in data format, data location and access interface, dynamic data management positioning to meet the availability, monetary and performance requirements, and to provide independent functions to reduce the burden of IT personnel managing complex data architecture. This paper describes the motivation for information integration to demand computing, explains its requirements, and describes its value through the use of scenes. As shown in the paper, there is still a large number of research, engineering and development work that needs to make information integration vision a reality, and the software company will continue to invest in information integration vision.
Unfortunately, this will be my last influential paper series, I have edited about five years (how time is flying!) and achieved huge achievements, and I always found it is worth looking back to why we do research, and this paper has made a huge contribution to the self-examination process.
Most new databases are no longer built from fragments, but reuse existing data from multiple independent databases.To promote application development, reused data is best redefined as a virtual database, providing the logical unification of the basic data set.
In this article, we take into account two popular types of algorithm inequality, (XopY) and (X op C), where X and Y are properties, C are common cases of domain or X, and op ∈{, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤, ≤,
A range-sum query is very popular and becomes important in finding trends and in discovering relationships between attributes in various database applications. It sums over the selected cells of an OLAP data cube where the target cells are determined by the specified query ranges. Direct method access data cube itself forces too many cells to be accessed, so it occurred a serious transition. Reaction time is very critical for the OLAP application needs to interact with the user. In recent dynamic business environment, data elements in the cube are often changing.
Trusted request response interaction, the server never performed a request more than once, used to support business and security key operations in a variety of sectors, such as banking, e-commerce, or healthcare. This form of interaction can be quite difficult to implement because customers, servers, or communication channels may fail, potentially require a variety and complex recovery program, which may lead to a copy of the message processed on the server. In this article, we solved the following questions: can we provide a meaningful tax rate and reliable request response protocol? We produce an effective sequence of clients and servers operations, organizing a sequence produced to a prescribed tree, and according to their reliability sequence and memory requirements classify the trees of the household protocols that match the reality and complex recovery processes on the service.
In this article, many different algorithms computing agreed solutions to optimize adding orders are studied because traditional dynamic programming techniques are not suitable for complex problems. Two possible solutions space, space deep and Bush trees, are evaluated from a statistical perspective. The result is that the common limitations of deep trees are applied only to some type of added chart. Basically, from three categories of optimizers are analyzed: dynamic, random and genetic algorithms each is widely reviewed, complying with its work principles and its required fitness applications.
Object-based database (OODB) users bring them a lot of heredity data (megabytes even gigabin in addition, the scientific OODB users are constantly producing new data. all of these data must be loaded on the OODB. each relative database system has a load use, but most OODBs do not. the data loaded to a OODB process is by a reference between the objects, or the relationship, complex in the data. These relationships are expressed in the OODB as the object detector, these relationships are unknown moments of load data; they may include cycles; and may be suggested that the system keeps the reversal relationship, and must also be stored.
The intrusion detection system is traditionally based on the characteristics of the attack and the tracking of the system activity to determine whether it meets this character. Recently, the new intrusion detection system based on data mining appeared on the site. This paper describes the design and experience of the ADAM (audit data analysis and mining) system that we use as a test bed to study how data mining technology is useful in intrusion detection.
This paper introduces the Sybase Adaptive Server Enterprise (ASE) Abstract Program (AP) language, a new technology that combines a set of verified technologies to solve the optimizer's wrong decisions. AP language is based on the physical level of the relationship algorithm of the 2 user optimizer communication mechanisms.
The National Tsing Hua University (NTttU) was founded in 1911 and is located in a suburbs of Taiwan's Hsinehu city, about 50 miles from the capital Taipei.
The algorithm requires two channels, linear time and space 1/θ. The first channel is an online algorithm, generalizing a well-known algorithm to find a majority of elements to identify a set of up to 1/θ elements, which may include, in other, all frequencies above θ elements.
Most RDBMS keeps a set of historical records to estimate the selectivity of specific queries. These selectivities are usually used for cost-based queries optimization. Although the problem of setting up a precise historical record on a specific attribute or attribute has been well studied, the problem of building and adjusting a historical record set is rarely noticed, in a multi-dimensional queries self-managed way only based on queries feedback. In this article, we introduce SASH, a self-adjusted historical record set to solve the problem of building andining a set of historical records. SASH uses a new two-stage method to automatically build and maintain itself, using only queries information. In the online queries, the current historical record set for the way of queries feedback, only on the basis of queries feedback in
Similarity search has recently attracted a lot of research interest. This is a difficult problem because of the data’s common high size. The most promising solutions involve implementing size reduction of data and then the multi-dimensional index structure index reduction of data. Many size reduction technologies have been proposed, including the unit value decomposition (SVD), the same differentiated conversion (DFT), and differentiated conversion (DWT). In this article, we introduce a new size reduction technology, which we call adaptive component fixed proximity (APCA). Although the previous technology (e.g., SVD, DFT and DWT) chooses a common representation, even in the search database, APCA does not build the same database, APCA can support differentiated conversion.
We live in an exciting moment in the field of clinical research and the changes that occur will change the way health care is provided and the work of clinical research professionals will guide these changes.
Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly Recording Arms Assembly
